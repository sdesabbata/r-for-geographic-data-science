[{"path":"index.html","id":"welcome","chapter":"Welcome","heading":"Welcome","text":"book introduction geographic data science using R,1 designed companion module GY7702 R Data Science MSc Geographic Information Science School Geography, Geology, Environment University Leicester. , much work progress currently revision 2023/24 module.","code":""},{"path":"index.html","id":"aims-and-learning-objectives","chapter":"Welcome","heading":"Aims and learning objectives","text":"materials included book designed module focusing programming language R effective tool data science geographers. R one widely used programming languages. provides access vast repository programming libraries, covering aspects data science, data wrangling statistical analysis, machine learning data visualisation. includes various libraries processing spatial data, performing geographic information analysis, creating maps. , R highly versatile, free open-source tool geographic information science, combines capabilities traditional GIS software advantages scripting language interface vast array algorithms.materials aim cover necessary skills basic programming, data wrangling reproducible research tackle sophisticated non-spatial data analyses. first part module focus core programming techniques, data wrangling practices reproducible research. second part module focus non-spatial data analysis approaches, including statistical analysis machine learning.book lecture slides use #FFF0E2 background colour avoid using pure white background, can make reading difficult slower people dyslexia. colours also checked readability using Colour Contrast Analyser.","code":""},{"path":"index.html","id":"license-and-acknowledgements","chapter":"Welcome","heading":"License and acknowledgements","text":"work licensed GNU General Public License v3.0 except specified. text licensed Creative Commons Attribution-ShareAlike 4.0 International (CC -SA 4.0). See src/images folder information regarding images used materials. data used materials derived data released Office National Statistics, Chris Gale (Office National Statistics), ESRC Consumer Data Research Centre, Ministry Housing, Communities & Local Government UK Data Service, licensed Open Government Licence v.3.0; contains National Statistics data Crown copyright database right 2015; contains Ordnance Survey data Crown copyright database right 2015 2016. See data folder details.repository includes teaching materials created (Dr Stefano De Sabbata) modules GY7702 R Data Science GY7708 Geospatial Artificial Intelligence working School Geography, Geology, Environment University Leicester. project direct continuation previous granolarr project, different presentation format. also like acknowledge contributions made parts materials Prof Chris Brunsdon Prof Lex Comber (see also Introduction R Spatial Analysis Mapping, Sage, 2015), Dr Marc Padilla, Dr Nick Tate, convened previous versions module University Leicester, thank Dr Jörg Kaduk contribution citing code chapter reproducibility.Last least, like acknowledge myriad small contributions users many platforms, including Stack Exchange Network (e.g., Stack Overflow, provided Creative Commons Attribution-Share Alike 2.5 Generic License), asked answered many questions coding book previous scripts fed book. impossible trace back contributors pages, scripts years, . learning materials created using R, RStudio, RMarkdown Bookdown (many thanks Yihui Xie fantastic tools related documentation), GitHub.","code":""},{"path":"index.html","id":"about-me","chapter":"Welcome","heading":"About me","text":"Hi , name Stef. Associate Professor Geographical Information Science School Geography, Geology Environment research theme lead Cultural Informatics Institute Digital Culture University Leicester, Research Associate Oxford Internet Institute University Oxford.geographic data scientist working intersection human geography, artificial intelligence internet studies. research focuses three intertwined research streams: development machine learning approaches geographic data analysis, study geographies content created internet platforms, application quantitative urban geography. teach data science programming R, information visualisation, geospatial databases information retrieval, digital geographies geographic information science. Chair Geographic Information Science Research Group Royal Geographical Society IBG. also part steering committee GIScience Research UK (GISRUK), chair GISRUK 2018 conference. member Commission Location-Based Services International Cartographic Association.Earlier career, worked Researcher Oxford Internet Institute University Oxford (2013-2015), Junior Research Fellow Wolfson College University Oxford (2014-2015). awarded PhD Department Geography University Zurich 2013 BSc MSc computer science Department Mathematics Computer Science University Udine.can find Twitter @maps4thought GitHub sdesabbata.","code":""},{"path":"index.html","id":"reproducibility","chapter":"Welcome","heading":"Reproducibility","text":"","code":""},{"path":"index.html","id":"instructor","chapter":"Welcome","heading":"Instructor","text":"can now reproduce R Geographic Data Science using Docker. First, install Docker system, install Git already installed, clone repository GitHub. can either build sdesabbata/r--geographic-data-science image running Docker_Build.sh script src folder repository pull latest sdesabbata/r--geographic-data-science image Docker Hub.now code computational environment reproduce materials, can done running script src/Docker_Make.sh (src/Docker_Make_WinPowerShell.sh Windows using PowerShell). script instantiate Docker container sdesabbata/r--geographic-data-science image, bind mount repository folder container execute src/Make.R container, clearing re-making materials. data used materials can re-created original open data using scripts src/utils, described data/README.md.instance, Unix-based system like Linux Mac OS, can reproduce R Geographic Data Science using following four commands:approach allow simply use materials easily edit create version computational environment. develop materials, modify code repository run src/Docker_Make.sh repository folder obtain updated materials.RMarkdown2 code used create materials book lecture slides can found src/book src/slides folders, respectively. files used generate Bookdown3 book xaringan slides. src/slides folder contains xaringan templates styles used RMarkdown code.can edit materials r--geographic-data-science repository folder using RStudio another editor computer compile new materials using Docker. Alternatively, can follow learner instructions start RStudio Server using Docker develop materials environment compiled.first option might quicker minor edits, whereas latter might preferable substantial modifications, especially need test code.","code":"docker pull sdesabbata/r-for-geographic-data-science:latest\ngit clone https://github.com/sdesabbata/r-for-geographic-data-science.git\ncd r-for-geographic-data-science\n\n# follow the instructions in data/README.md before continuing\n\n./src/Docker_Make.sh.\n├── data\n├── docs\n    └── slides\n└── src\n    ├── book\n    ├── images\n    ├── practicals\n    ├── slides\n    └── utils"},{"path":"index.html","id":"learner","chapter":"Welcome","heading":"Learner","text":"learner, can use Docker follow practical sessions instructions complete exercises. First, install Docker system, install Git already installed, clone repository GitHub.can either build sdesabbata/r--geographic-data-science image running Docker_Build.sh script src folder repository pull latest sdesabbata/r--geographic-data-science image Docker Hub. now code computational environment reproduce materials, can done running script src/Docker_RStudio_Start.sh (src/Docker_RStudio_Start_WinPowerShell.sh Windows using PowerShell) repository folder.instance, Unix-based system like Linux Mac OS, can set start r--geographic-data-science container using following four commands:src/Docker_RStudio_Start.sh script first create my_r--geographic-data-science folder parent directory root directory repository (doesn’t exist). script instantiate Docker container sdesabbata/r--geographic-data-science image, bind mount my_r--geographic-data-science folder r--geographic-data-science repository folder container start RStudio Server.script outpur include line starting password set followed temporary password need access RStudio Server. Copy temporary password. Using browser, can access RStudio Server running Docker container typing 127.0.0.1:28787 address bar using rstudio username temporary password copied password. my_r--geographic-data-science folder bound, everything save my_r--geographic-data-science folder home directory RStudio Server saved computer. Everything else lost Docker container stopped.stop Docker container, running script src/Docker_RStudio_Stop.sh (Windows using PowerShell) repository folder.","code":"docker pull sdesabbata/r-for-geographic-data-science:latest\ngit clone https://github.com/sdesabbata/r-for-geographic-data-science.git\ncd r-for-geographic-data-science\n\n# follow the instructions in data/README.md before continuing\n\n./src/Docker_RStudio_Start.sh"},{"path":"index.html","id":"session-info","chapter":"Welcome","heading":"Session info","text":"Stefano De Sabbata – text licensed CC -SA 4.0, contains public sector information licensed Open Government Licence v3.0, code licensed GNU GPL v3.0.","code":"\nsessionInfo()## R version 4.3.1 (2023-06-16)\n## Platform: x86_64-pc-linux-gnu (64-bit)\n## Running under: Ubuntu 22.04.3 LTS\n## \n## Matrix products: default\n## BLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \n## LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.20.so;  LAPACK version 3.10.0\n## \n## locale:\n##  [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              \n##  [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    \n##  [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   \n##  [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 \n##  [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n## [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       \n## \n## time zone: Etc/UTC\n## tzcode source: system (glibc)\n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base     \n## \n## loaded via a namespace (and not attached):\n##  [1] digest_0.6.33   R6_2.5.1        bookdown_0.35   fastmap_1.1.1  \n##  [5] xfun_0.40       cachem_1.0.8    knitr_1.44      memoise_2.0.1  \n##  [9] htmltools_0.5.6 rmarkdown_2.25  xml2_1.3.5      cli_3.6.1      \n## [13] downlit_0.4.3   sass_0.4.7      withr_2.5.0     jquerylib_0.1.4\n## [17] compiler_4.3.1  tools_4.3.1     evaluate_0.21   bslib_0.5.1    \n## [21] yaml_2.3.7      fs_1.6.3        rlang_1.1.1     jsonlite_1.8.7"},{"path":"introduction-to-r.html","id":"introduction-to-r","chapter":"1 Introduction to R","heading":"1 Introduction to R","text":"Print chapterWe start chapter brief introduction R, programming language focus module, tool use data science.R one widely used programming languages nowadays, along Python, especially geographic satellite data science. don’t personally strong preference either, use fairly regularly combination. time, using one matter habit availability particular functionality makes easier complete task set . instance, Python great libraries programming deep neural networks. However, find R effective powerful data manipulation, statistical analysis, visualisation mapping. key reason book focuses R. time, beyond mere details syntax, languages different becoming easier integrate. principles approaches covered book can applied using Python, just using different syntax.","code":""},{"path":"introduction-to-r.html","id":"the-r-programming-language","chapter":"1 Introduction to R","heading":"1.1 The R programming language","text":"R4 created 1992 Ross Ihaka Robert Gentleman University Auckland, New Zealand. R free, open-source implementation S statistical programming language initially created Bell Labs. core, R functional programming language (main functionalities revolve around defining executing functions). However, now supports commonly used imperative (focused instructions variables programming control structures) object-oriented (involving complex object structures) programming language.simple terms, programming R mainly focuses devising series instructions execute task – commonly, loading analysing dataset., R can used program creating sequences instructions involving variables – named entities can store values, . main topic practical session. Instructions can include control flow structures, decision points (/else) loops, topic next practical session. Instructions can also grouped functions, see detail next chapter.R interpreted, compiled. means R interpreter receives instruction write interprets executes . programming languages require code compiled executable file executed computer.","code":""},{"path":"introduction-to-r.html","id":"rstudio","chapter":"1 Introduction to R","heading":"1.1.1 RStudio","text":"RStudio probably popular Integrated Development Environment (IDE) R. using RStudio, R interpreter hidden backend, RStudio frontend application allows interact interpreter. open RStudio RStudio Server, interface divided two main sections. left side, find Console – R script editor, script edited. Console input/output window R interpreter, can type instructions see resulting output.instance, type Consolethe R interpreter understands instruction sum 1 1 returns following result output.materials created RMarkdown, output computation always preceded ##. Note output value 2 preceded [1], indicates output constituted one element. output constituted one element, list numbers , row output preceded index first element output.right side, find two groups panels. top-right, main element Environment panel, represents current state interpreter’s memory, showing information available computation. instance, able see datasets loaded analysis. bottom-right, find Files panel, shows file system (file folders computer server), well Help panel, shows help pages required. discuss panels later practical sessions.","code":"\n1 + 1## [1] 2##  [1]   1   4   9  16  25  36  49  64  81 100 121 144 169 196 225 256 289 324 361\n## [20] 400"},{"path":"introduction-to-r.html","id":"coding-style","chapter":"1 Introduction to R","heading":"1.1.2 Coding style","text":"coding style set rules guidelines write programming code designed ensure code easy read, understand, consistent time. Following good coding style essential writing code others read – instance, work team, publish code submit code piece coursework – ensures understand code months. Following good coding style also essential step towards reproducibility, see later chapter.book, follow Tidyverse Style Guide (style.tidyverse.org). Study Tidyverse Style Guide use consistently.","code":""},{"path":"introduction-to-r.html","id":"core-concepts","chapter":"1 Introduction to R","heading":"1.2 Core concepts","text":"","code":""},{"path":"introduction-to-r.html","id":"values","chapter":"1 Introduction to R","heading":"1.2.1 Values","text":"value typed Console, interpreter returns value. examples , 2 simple numeric value, \"String value\" textual value, R referred character value programming also commonly referred string (short string characters).Numeric example:Character example:Note character values need start end single double quote (' \"), part information . Tidyverse Style Guide suggests always using double quote (\"), use module.Anything follows # symbol considered comment, interpreter ignores .mentioned , interpreter understands simple operations numeric logic values, well text objects (including single characters strings character, text objects longer one character, commonly referred simply strings computer science), discussed detail Appendix 1.","code":"\n2## [1] 2\n\"String value\"## [1] \"String value\"\n# hi, I am a comment, please ignore me\n# Sum 1 and 2\n1 + 2## [1] 3\n# Logical AND operation between\n# the value TRUE and FALSE\nTRUE & FALSE## [1] FALSE\n# Check whether the character a\n# is equal to the character b\n\"a\" == \"b\"## [1] FALSE"},{"path":"introduction-to-r.html","id":"variables","chapter":"1 Introduction to R","heading":"1.2.2 Variables","text":"computer programming, variable can thought storage location (bit memory) associated name (also referred identifier) value can vary – hence name variable. programming, can define variable naming identifier providing value stored variable. , can retrieve value stored variable specifying chosen identifier. Variables essential tool programming, allow saving result piece computation retrieve later analysis.variable can defined R using identifier (e.g., a_variable) left assignment operator <-, followed object linked identifier, value (e.g., 1) assigned right. value variable can invoked simply specifying identifier.type a_variable <- 1 Console RStudio, new element appears Environment panel, representing new variable memory. left part entry contains identifier a_variable, right part contains value assigned variable a_variable, 1. example , another variable named another_variable created summed a_variable, saving result sum_of_two_variables.","code":"\na_variable <- 1\na_variable## [1] 1\nanother_variable <- 4\nanother_variable## [1] 4\nsum_of_two_variables <- a_variable + another_variable\nsum_of_two_variables## [1] 5"},{"path":"introduction-to-r.html","id":"data-frames","chapter":"1 Introduction to R","heading":"1.2.3 Data frames","text":"examples illustrate R allows working simple data types. However, data science, rarely engage directly simple pieces information. Rather, commonly work datasets composed tables. Data frames complex data types encode concept table R combining arranging together series simple objects. following chapters, explore different complex types handle tables detail.R includes many simple example data frames, iris dataset. loading R, pre-defined variable named iris available environment. specifying identifier iris console (shown ), can invoke variable see contents. one 150 rows table (first five rows shown ) reports information iris flower. Four numeric columns (Sepal.Length, Sepal.Width, Petal.Length Petal.Width) used describe size sepal petal, string column (Species) used catalogue species iris.","code":"\niris"},{"path":"introduction-to-r.html","id":"algorithms","chapter":"1 Introduction to R","heading":"1.2.4 Algorithms","text":"operation can executed using computer called algorithm. precise, Nigel Cutland5 defined algorithm “mechanical rule, automatic method, program performing mathematical operation” (Cutland, 1980, p. 76).instructions get mount Ikea furniture can thought algorithm, effective procedure perform operation mounting furniture. playing part computer executing algorithm.program set instructions implementing abstract algorithm specific language – let R, Python, language. definition, algorithms (thus programs implement ) can use variables functions. case R, programs interpreted rather compiled also referred scripts.","code":""},{"path":"introduction-to-r.html","id":"functions","chapter":"1 Introduction to R","heading":"1.2.5 Functions","text":"can think function processing unit , received values input, performs specific task can return value output. simple algorithms can coded programs made one function performs whole task. complex algorithms might require multiple functions, designed complete sub-task, combined perform entire task.can invoke function specifying function name along arguments (input values) simple brackets. argument corresponds parameter (.e., internal variable used within function run operation, see detail later book). Programming languages provide pre-defined functions implement common algorithms (e.g., finding square root number calculating linear regression).instance, sqrt pre-defined function R computes square root number. instruction sqrt(2) tells R interpreter run function calculates square root using 2 input value. function return 1.414214, square root 2, output.Another example function round, returns value rounded specified number digits dot. instance, round(1.414214, digits = 2) returns 1.41. case, specify second argument refers number digits kept, function also arguments can specified.Functions can also used right side assignment operator <- case output value function stored memory slot identifier. Variables can used arguments. instance, saving result square root two variable sqrt_of_two, can use variable first argument function round.rest book illustrate, R provides wide range functions allow conducting step data analysis: data input/ouput data manipulation, statistics machine learning visualisation, even creating web-based book like one currently reading (indeed created using R). instance, can use function hist plot histogram petal lengths flowers iris. Note , code , first parameter iris iris$Petal.Length $ used extract Petal.Length iris (coming chapters).Functions can also used arguments functions. Instead first calculating square root two, saving value variable, using variable first argument round, can directly add function sqrt argument first argument function round.subsequent chapter, see can create functions .introduce variables functions, functions using variables functions, complexity code increases quite rapidly. fact, using function argument another function usually discouraged makes code difficult read. Instead, best always aim code easy read understand possible. essential step ensuring follow coding style guidelines closely.","code":"\nsqrt(2)## [1] 1.414214\nround(1.414214, digits = 2)## [1] 1.41\nsqrt_of_two <- sqrt(2)\nsqrt_of_two## [1] 1.414214\nround(sqrt_of_two, digits = 2)## [1] 1.41\nhist(iris$Petal.Length, main = \"Petal lengths\")\nround(sqrt(2), digits = 2)## [1] 1.41"},{"path":"introduction-to-r.html","id":"libraries","chapter":"1 Introduction to R","heading":"1.2.6 Libraries","text":"Functions can collected stored libraries (sometimes referred packages), containing related functions sometimes datasets. instance, base library R includes sqrt function , rgdal library, contains implementations GDAL (Geospatial Data Abstraction Library) functionalities R.Libraries can installed R using function install.packages using Tool > Install Packages... RStudio.","code":""},{"path":"introduction-to-r.html","id":"tidyverse","chapter":"1 Introduction to R","heading":"1.3 Tidyverse","text":"meta-library Tidyverse7 contains following libraries:ggplot28 system declaratively creating graphics based Grammar Graphics. provide data, tell ggplot2 map variables aesthetics, graphical primitives use, takes care details.dplyr provides grammar data manipulation, providing consistent set verbs solve common data manipulation challenges.tidyr provides set functions help get tidy data. Tidy data data consistent form: brief, every variable goes column, every column variable.readr provides fast friendly way read rectangular data (like csv, tsv, fwf). designed flexibly parse many types data found wild, still cleanly failing data unexpectedly changes.purrr enhances R’s functional programming (FP) toolkit providing complete consistent set tools working functions vectors. master basic concepts, purrr allows replace many loops code easier write expressive.tibble modern re-imagining data frame, keeping time proven effective, throwing . Tibbles data.frames lazy surly: less complain forcing confront problems earlier, typically leading cleaner, expressive code.stringr provides cohesive set functions designed make working strings easy possible. built top stringi, uses ICU C library provide fast, correct implementations common string manipulations.forcats provides suite useful tools solve common problems factors. R uses factors handle categorical variables, variables fixed known set possible values.library can loaded using function library, shown (note name library quoted). library installed computer, don’t need install , every script needs load libraries uses. library loaded, functions can used.Important: always necessary load tidyverse meta-library want use stringr functions pipe operator %>%.","code":"\nlibrary(tidyverse)"},{"path":"introduction-to-r.html","id":"the-pipe-operator","chapter":"1 Introduction to R","heading":"1.3.1 The pipe operator","text":"pipe operator useful outline complex operations step step (see also R Data Science, Chapter 18). pipe operator %>%takes result one functionand passes next functionas first argumentthat doesn’t need included code anymoreThe code shows simple example. number 2 taken input first pipe passes first argument function sqrt. output value 1.41 taken input second pipe, passes first argument function trunc. final output 1 finally returned.image graphically illustrates pipe operator works, compared procedure executed using two temporary variables used store temporary values.Similarly, can use pipe recreate histogram petal lengths iris dataset shown , using function pull extract column Petal.Length table, instead operator $. However, Tidyverse includes ggplot2 library, provides support sophisticated plotting, see subsequent chapters.first step sequence pipes can value, variable, function, including arguments. code shows series examples different ways achieving result. examples use function round, also allows second argument: digits = 2. Note , using pipe operator, nominally second argument provided function round – round(digits = 2)complex operation created use %>% can used right side <-, assign outcome operation variable.","code":"\n2 %>%\n  sqrt() %>%\n  trunc()## [1] 1\niris %>% \n  pull(Petal.Length) %>% \n  hist(main = \"Petal lengths\")\n# No pipe, using variables\ntmp_variable_A <- 2\ntmp_variable_B <- sqrt(tmp_variable_A)\nround(tmp_variable_B, digits = 2)\n\n# No pipe, using functions only\nround(sqrt(2), digits = 2)\n\n# Pipe starting from a value\n2 %>%\n  sqrt() %>%\n  round(digits = 2)\n\n# Pipe starting from a variable\nthe_value_two <- 2\nthe_value_two %>%\n  sqrt() %>%\n  round(digits = 2)\n\n# Pipe starting from a function\nsqrt(2) %>%\n round(digits = 2)\nsqrt_of_two <- \n  2 %>%\n  sqrt() %>%\n  round(digits = 2)"},{"path":"introduction-to-r.html","id":"exercise-101.1","chapter":"1 Introduction to R","heading":"1.4 Exercise 101.1","text":"Question 101.1.1: Write piece code using pipe operator takes input number 1632, calculates logarithm base 10, takes highest integer number lower calculated value (lower round), verifies whether integer.Question 101.1.2: Write piece code using pipe operator takes input number 1632, calculates square root, takes lowest integer number higher calculated value (higher round), verifies whether integer.Question 101.1.3: Write piece code using pipe operator takes input string \"1632\", transforms number, checks whether result Number.Question 101.1.4: Write piece code using pipe operator takes input string \"-16.32\", transforms number, takes absolute value truncates , finally checks whether result Available.Question 101.1.5: Rewrite piece code substituting last line function mean(). kind result obtain? represent?Question 101.1.6: edit code created Question 101.1.6 substituting Petal.Length Petal.Width first Species ? kind results obtain? mean?Stefano De Sabbata – text licensed CC -SA 4.0, contains public sector information licensed Open Government Licence v3.0, code licensed GNU GPL v3.0.","code":"\niris %>% \n  pull(Petal.Length) %>% \n  hist(main = \"Petal lengths\")"},{"path":"reproducible-data-science.html","id":"reproducible-data-science","chapter":"2 Reproducible data science","heading":"2 Reproducible data science","text":"Print chapter","code":""},{"path":"reproducible-data-science.html","id":"data-science","chapter":"2 Reproducible data science","heading":"2.1 Data science","text":"Singleton Arribas-Bel define “geographic data science” space “effectively combines long-standing tradition epistemologies Geographic Information Science Geography many recent advances given Data Science relevance emerging ‘datafied’ world” (Singleton Arribas-Bel, 2021, p679). particular, argue “data science” emerged term indicate use statistical visual analytics tool study world digitalisation everyday life resulted “data deluge” commonly referred “big data”10. academic debate historical epistemological background term “data science” quite complex, term now acquired wide-spread usage., “data science” commonly used refer set tools approaches analysing data, including statistical analysis, visualisation (used referred ) data mining. Data science also overlaps field machine learning, part artificial intelligence includes methods normally umbrella statistics. , point, confused, don’t worry, ’s quite normal. definitions frequently debated frequently prone become buzzwords.book focuses introduction data science using R focus geographic themes, although necessarily using spatial analysis (.e., spatial relationships part computation), covered another module wonderful books Introduction R Spatial Analysis Mapping Chris Brunsdon Lex Comber Geocomputation R Robin Lovelace, Jakub Nowosad Jannes Muenchow.","code":""},{"path":"reproducible-data-science.html","id":"reproducibility-1","chapter":"2 Reproducible data science","heading":"2.2 Reproducibility","text":"According Christopher Gandrud11, quantitative analysis project can considered reproducible : “data code used make finding available sufficient independent researcher recreate finding”. Reproducibility practices rooted software engineering, including project design practices (Scrum), software readability principles, testing versioning.GIScience, programming essential interact early GIS software ArcInfo 1980s 1990s, release ArcGIS 8.0 suite 1999, included graphical user interface. past decade seen gradual return programming scripting GIS, especially languages R Python allowed combine GIS capabilities much broader data science machine learning functionalities. Many disciplines seen similar trajectory, programming data science become integral science, reproducibility practices become cornerstone scientific development.Nowadays, many academic journals conferences require level reproducibility submitting paper (e.g., see AGILE Reproducible Paper Guidelines Association Geographic Information Laboratories Europe). Companies keen reproducible analysis, reliable efficient long term. Second, amount data increases, reproducible approaches effectively create reliable analyses can easily verified reproduced different new data. Alex David Singleton, Seth Spielman, Chris Brunsdon12 discussed issue reproducibility GIScience, identifying following best practices:“Data accessible within public domain available researchers”.“Software used open code scrutable”.“Workflows public link data, software, methods analysis presentation discursive narrative”.“peer review process academic publishing require submission workflow model ideally open archiving materials necessary replication”.“full reproducibility possible (commercial software sensitive data) aim adopt aspects attainable within circumstances”.rest chapter discusses three tools can help improve reproducibility code: Markdown, RMarkdown Git.","code":""},{"path":"reproducible-data-science.html","id":"r-projects","chapter":"2 Reproducible data science","heading":"2.3 R Projects","text":"RStudio provides extremely useful functionality organise code data, R Projects. specialised files RStudio can use store information specific project working – Environment, History, working directory, much , see coming weeks. Working well-organised project crucial reproducible data science.RStudio Server, Files tab bottom-left panel, click Home make sure home folder – working computer, create folder practicals wherever convenient. Click New Folder enter Practicals prompt dialogue create folder named Practicals.Select File > New Project … main menu, prompt menu, New Directory, New Project. Insert GY7702-practical-102 directory name, select Practicals folder field Create project subdirectory . Finally, click Create Project.RStudio now created project, activated . case, Files tab bottom-right panel GY7702-practical-102 folder, contains GY7702-practical-102.Rproj file. GY7702-practical-102.Rproj stores Environment information current project, project files (e.g., R scripts, data, output files) stored within GY7702-practical-102 folder. Moreover, GY7702-practical-102 now working directory, means can refer file folder using name save file, default directory save .top-right corner RStudio, see blue icon representing R cube next name project (GY7702-practical-102). also indicates within GY7702-practical-102 project. Click GY7702-practical-102 select Close Project close project. Next R cube icon, now see Project: (None). Click Project: (None) select GY7702-practical-102 list reactivate GY7702-practical-102 project. future, thus able close reactivate project necessary, depending working . Projects can also activated clicking related .Rproj file Files tab bottom-right panel Open Project… option file menu.GY7702-practical-102 project activated, select top menu File > New File > RMarkdown create new RMarkdown document – can use default options creation menu, just example. Save knit RMarkdown document (shown previous chapter). can see, RMarkdown file knitted saved within GY7702-practical-102 folder.","code":""},{"path":"reproducible-data-science.html","id":"r-scripts","chapter":"2 Reproducible data science","heading":"2.4 R Scripts","text":"RStudio Console handy interacting R interpreter obtain results operations commands. However, moving simple instructions actual program script conduct data analysis, Console usually sufficient anymore. fact, Console comfortable way providing long complex instructions interpreter. instance, doesn’t easily allow overwrite past instructions want change something procedure. better option create programs data analysis scripts significant size use RStudio integrated editor create R script.create R script, select top menu File > New File > R Script. opens embedded RStudio editor new empty R script folder. Copy two lines file. first loads tidyverse library, whereas second simply calculates square root two.can see, comment precedes line, describing subsequent command . Adequately commenting code fundamental practice programming. learning resource, comments examples explain “” subsequent lines code . However, comments generally focus “” procedure (.e., algorithm) implemented set instructions (.e., section script) crucially “” procedure implemented specific way. see complex examples rest book.top menu, select File > Save, type -first-script.R (make sure include underscore .R extension) File name, click Save. Finally, click Source button top-right editor.Congratulations, executed first R script! 😊👍can edit script adding (instance) new lines code shown , saving file, executing script’s new version.\nRStudio also allows select one lines click Run execute selected lines line cursor currently .Self-test questions:happens select print(current_time) click Run run just line?happens click Source button thus execute new version script?happens click Source third time?three differ ()?","code":"\n# Load the Tidyverse\nlibrary(tidyverse)\n\n# Calculate the square root of two\n2 %>% sqrt()## [1] 1.414214\n# First variable in a script:\n# the line below uses the Sys.time of the base library \n# to obtain the current time as a character string\ncurrent_time <- Sys.time()\nprint(current_time)"},{"path":"reproducible-data-science.html","id":"reproducible-workflows","chapter":"2 Reproducible data science","heading":"2.4.1 Reproducible workflows","text":"readr library (also part Tidyverse) provides series functions can used load save data different file formats. read_csv function reads Comma Separated Values (CSV) file path provided first argument.code loads 2011_OAC_Raw_uVariables_Leicester.csv containing 2011 Output Area Classification (2011 OAC) Leicester. 2011 OAC geodemographic classification census Output Areas (OA) UK, created Gale et al. (2016) starting initial set 167 prospective variables United Kingdom Census 2011: 86 removed, 41 retained , 40 combined, leading final set 60 variables. Gale et al. (2016) finally used k-means clustering approach create 8 clusters supergroups (see map datashine.org.uk), well 26 groups 76 subgroups. dataset file 2011_OAC_Raw_uVariables_Leicester.csv contains original 167 variables, well resulting groups, city Leicester. full variable names can found file 2011_OAC_Raw_uVariables_Lookup.csv.read_csv instruction throws warning shows assumptions data types used loading data. illustrated output last line code, data loaded tibble 969 x 190, 969 rows – one OA – 190 columns, 167 represent input variables used create 2011 OAC. function write_csv can similarly used save dataset csv file, illustrated exercise .read_csv write_csv require specify file path, can specified two different ways:Absolute file path: full file path root folder computer file.\nabsolute file path file can obtained using file.choose() instruction R Console, open interactive window allow select file computer. absolute path file printed console.\nAbsolute file paths provide direct link specific file ensure loading exact file.\nHowever, absolute file paths can problematic file moved script run different system, file path invalid\nabsolute file path file can obtained using file.choose() instruction R Console, open interactive window allow select file computer. absolute path file printed console.Absolute file paths provide direct link specific file ensure loading exact file.However, absolute file paths can problematic file moved script run different system, file path invalidRelative file path: partial path current working folder file.\ncurrent working directory (current folder) part environment R session can identified using getwd() instruction `R Console*.\nnew R session started, current working directory usually computer user’s home folder.\nworking within R project, current working directory project directory.\ncurrent working can manually set specific directory using function setwd.\n\nUsing relative path working within R project option provides best overall consistency, assuming (data) files read scripts project also contained project folder (subfolder).\ncurrent working directory (current folder) part environment R session can identified using getwd() instruction `R Console*.\nnew R session started, current working directory usually computer user’s home folder.\nworking within R project, current working directory project directory.\ncurrent working can manually set specific directory using function setwd.\nnew R session started, current working directory usually computer user’s home folder.working within R project, current working directory project directory.current working can manually set specific directory using function setwd.Using relative path working within R project option provides best overall consistency, assuming (data) files read scripts project also contained project folder (subfolder).","code":"\nlibrary(tidyverse)\nlibrary(knitr)\n\n# Read the Leicester 2011 OAC dataset from the csv file\nleicester_2011OAC <- \n  read_csv(\"2011_OAC_Raw_uVariables_Leicester.csv\")\n# Absolute file path\n# Note: the first / indicates the root folder\nread_csv(\"/home/username/GY7702/data/2011_OAC_Raw_uVariables_Leicester.csv\")\n\n# Relative file path\n# assuming the working directory is the user's home folder\n# /home/username\n# Note: no initial / for relative file paths\nread_csv(\"GY7702/data/2011_OAC_Raw_uVariables_Leicester.csv\")\n\n\n# Relative file path\n# assuming you are working within an R project created in the folder\n# /home/username/GY7702\n# Note: no initial / for relative file paths\nread_csv(\"data/2011_OAC_Raw_uVariables_Leicester.csv\")"},{"path":"reproducible-data-science.html","id":"exercise-102.1","chapter":"2 Reproducible data science","heading":"2.4.2 Exercise 102.1","text":"Create new subfolder named data within R project GY7702-practical-102 upload 2011_OAC_Raw_uVariables_Leicester.csv file data folder13.Create new R script named students-around-campus.R including reproducible workflow defined code , uses tidyverse functions read_csv write_csv, well select filter functions – discussed detail next chapter, don’t worry much now 😊 – execute following steps:read 2011 OAC file 2011_OAC_Raw_uVariables_Leicester.csv directly file, without storing variable;select OA codes variable, variables representing code name supergroup group assigned OA 2011 OAC (supgrpcode supgrpname, well grpcode grpname);filter OAs classified part Students Around Campus group 2011 OAC;write results file named 2011_OAC_Leicester_StudentsAroundCampus.csv, version 2011_OAC_supgrp_Leicester.csv.Save run script. new dataset written csv file can loaded software, GIS, analyse visualise data. time, R offers wide range tools analyse visualise, see modules. can download inspect file verify contains seven columns mentioned code, rows \"Students Around Campus\" value column grpname.","code":"\nread_csv(\"data/2011_OAC_Raw_uVariables_Leicester.csv\") %>%\n  select(\n    OA11CD, LSOA11CD, \n    supgrpcode, supgrpname,\n    grpcode, grpname,\n    Total_Population\n  ) %>%\n  filter(grpname == \"Students Around Campus\") %>%\n  write_csv(\"data/2011_OAC_Leicester_StudentsAroundCampus.csv\")"},{"path":"reproducible-data-science.html","id":"rmarkdown","chapter":"2 Reproducible data science","heading":"2.5 RMarkdown","text":"essential tool used creating book RMarkdown, R library allows create scripts mix Markdown mark-language R, create dynamic documents. RMarkdown script can compiled, point Markdown notation interpreted create output files, R code executed output incorporated document.instance, following markdown codeis rendered asThis link University Leicester bold.core Markdown notation used chapter presented . full RMarkdown cheatsheet available .R code can embedded RMarkdown documents using code snippets, example . results code chunk displayed within document (echo=TRUE specified), followed output execution code.create RMarkdown document RStudio, select File > New File > R Markdown … – might prompt RStudio update packages. RMarkdown document creation menu appears, specify “Practical 102” title name author, select PDF output format. new document contain core document information, plus additional content simply explains RMarkdown works.Read document , inspecting R Markdown code. Note information provided previous step encoded first five lines compose heading. code snippet, option echo=TRUE tells RStudio include code output document, along output computation. echo=FALSE specified, code omitted. option message=FALSE warning=FALSE added, messages warnings R displayed output document.Save document (File > Save) -first-rmarkdown.Rmd click Knit button visible file panel RStudio compile source file PDF file – browser blocks opening new file window, please allow pop-open top-right browser. Read compiled document compare source document assess compiled document derived source.","code":"[This is a link to the University of Leicester](http://le.ac.uk) and **this is in bold**.# Header 1\n## Header 2\n### Header 3\n#### Header 4\n##### Header 5\n\n**bold**\n*italics*\n\n[This is a link to the University of Leicester](http://le.ac.uk)\n\n- Example list\n    - Main folder\n        - Analysis\n        - Data\n        - Utils\n    - Other bullet point\n- And so on\n    - and so forth\n\n1. These are\n    1. Numeric bullet points\n    2. Number two\n2. Another number two\n3. This is number three```{r, echo=TRUE}\na_number <- 0\na_number <- a_number + 1\na_number <- a_number + 1\na_number <- a_number + 1\na_number\n```\na_number <- 0\na_number <- a_number + 1\na_number <- a_number + 1\na_number <- a_number + 1\na_number## [1] 3"},{"path":"reproducible-data-science.html","id":"exercise-102.2","chapter":"2 Reproducible data science","heading":"2.5.1 Exercise 102.2","text":"Create new RMarkdown document using Students around campus Leicester title PDF output file type. Delete example code add code , includes markdown second-heading section named Libraries chunk loading tidyverse knitr libraries. Save file name students-around-campus-map.Rmd Leicester_population project.Upload Leicester_2011_OAs.geojson data folder, add new markdown second-heading section named Map new R chunk. new R chunk, copy-paste code uses range functions see coming weeks, alongside sf mapsf libraries, create map showing location OAs classified part Students Around Campus group 2011 OAC., mentioned , don’t worry much details code right now. get coming weeks. Focus understanding embracing reproducible data science workflow. 😊","code":"---\ntitle: \"Students around campus in Leicester\"\noutput: pdf_document\ndate: \"2022-10-20\"\n---\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE)\n```\n\n## Libraries\n\n```{r libraries, warning=FALSE, message=FALSE}\nlibrary(tidyverse)\nlibrary(knitr)\n```\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(mapsf)\n\nleicester_2011OAC_students <- \n  read_csv(\"data/2011_OAC_Raw_uVariables_Leicester.csv\") %>%\n      filter(grpname == \"Students Around Campus\")\n\nst_read(\"data/Leicester_2011_OAs.geojson\") %>%\n  left_join(leicester_2011OAC_students) %>% \n  mf_map(\n    var = \"grpname\", \n    type = \"typo\",\n    pal = \"Dark 3\",\n    leg_title = \"2011 OAC groups\",\n    leg_no_data = \"Other groups\"\n    )\nmf_credits(txt = \"Source: Office for National Statistics, Census 2021. Contains National Statistics data Crown copyright and database right 2022; Contains Ordnance Survey data Crown copyright and database right 2022.\")"},{"path":"reproducible-data-science.html","id":"how-to-cite","chapter":"2 Reproducible data science","heading":"2.6 How to cite","text":"","code":""},{"path":"reproducible-data-science.html","id":"references","chapter":"2 Reproducible data science","heading":"2.6.1 References","text":"Academic references can added RMarkdown illustrated R Markdown Cookbook.14 Bibtex references can added separate .bib file linked heading RMarkdown document. References can cited using @ symbol followed reference id.instance, documents links references.bib bibtex file, contains academic references, packages.bib bibtex files, contains additional references R packages (see also next section), adding following line heading.references.bib contains following reference R Markdown Cookbook book.allows writing first sentence section follows.Bibtex references can obtained journals clicking Cite link paper Google Scholar selecting Bibtex.","code":"bibliography: [references.bib, packages.bib]@book{xie2020r,\n  title={R markdown cookbook},\n  author={Xie, Yihui and Dervieux, Christophe and Riederer, Emily},\n  year={2020},\n  publisher={Chapman and Hall/CRC},\n  url = {https://bookdown.org/yihui/rmarkdown-cookbook/}\n}Academic references can be added to RMarkdown [as illustrated in the R Markdown Cookbook](https://bookdown.org/yihui/rmarkdown-cookbook/bibliography.html) [@xie2020r]."},{"path":"reproducible-data-science.html","id":"code","chapter":"2 Reproducible data science","heading":"2.6.2 Code","text":"UK’s Software Sustainability Institute provides clear guidance cite software written others. outlined guidance, always cite credit work. However, using academic-style citations always straightforward working libraries, linked academic paper provide DOI. cases, least include link authors’ website repository script final report using library. instance, can add link Tidyverse’s website, repository CRAN page using library. However, Wickham et al.15 also wrote paper work Tidyverse Journal Open Source Software, can also cite paper using Bibtex RMarkdown.Appropriate citations even important directly copying adapting code others’ work. Plagiarism principles apply code much text. Massachusetts Institute Technology (MIT)’s Academic Integrity MIT: Handbook Students includes section writing code provides good guidance cite code include projects adapt code properly.\nalso applies re-using code, written . important refer previous work fully acknowledge previous work used project others can find everything used project.common practice follow particular referencing style -text quotations, references bibliography, Harvard style (see, e.g., Harvard Format Citation Guide available Mendeley’s help pages).\nFollowing guidelines ensure others can easily use reproduce work also demonstrate academic honesty integrity.Stefano De Sabbata – text licensed CC -SA 4.0, contains public sector information licensed Open Government Licence v3.0, code licensed GNU GPL v3.0.","code":""},{"path":"data-manipulation.html","id":"data-manipulation","chapter":"3 Data manipulation","heading":"3 Data manipulation","text":"Print chapterProgramming languages commonly provide simple data types, seen first chapter, complex objects capable storing organising multiple values, tables briefly seen action second chapter. chapter bridge gap discussing series complex data types manipulate information store. , discuss dplyr library (also part Tidyverse), offers grammar data manipulation.continuing, create new R project named GY7702-practical-103 create new R script named complex-data-types.R. Follow along examples copy-pasting code script running one line time.","code":""},{"path":"data-manipulation.html","id":"complex-data-types","chapter":"3 Data manipulation","heading":"3.1 Complex data types","text":"","code":""},{"path":"data-manipulation.html","id":"vectors","chapter":"3 Data manipulation","heading":"3.1.1 Vectors","text":"simplest complex objects usually allow storing multiple values type ordered list. objects take different names different languages. R, referred vectors16.Vectors can defined R using function c, takes parameters items stored vector. items stored order provided.vector created assigned identifier, elements within vector can retrieved specifying identifier, followed square brackets index (indices see ) elements retrieved. Indices start 1, index first element 1, index second element 2, forth17.Alternatively, first, last, nth functions dplyr library within Tidyverse can used extract single values vectors.retrieve subset vector (.e., one element), can specify integer vector containing indices (rather single integer value) items interest square brackets.Functions operators can applied vectors way applied simple values. instance, built-numerical functions R can used vector variable directly. , vector specified input, selected function applied element vector.Similarly, string functions can applied vectors containing character values. instance, code uses str_length obtain vector numeric values representing lengths city names included vector character values east_midlands_cities.seen previous chapter, condition entered Console evaluated provided input, logical value (TRUE FALSE) provided output. Similarly, provided input vector, condition evaluated element vector, vector logical values returned – contains respective results conditions element.subset elements vector can also selected providing vector logical values brackets identifier. new vector returned, containing values TRUE value specified correspondingly.result evaluating condition vector vector logical values, can used filter vectors based conditions. condition provided square brackets (vector identifier instead index), new vector returned, contains elements condition true.factor data type similar vector. However, values contained factor can selected set levels. code illustrates difference vectors factors.R also provides data type named ordered, similar data type factor levels considered ordered categories respond operators like > (greater ) filtered. Appending 1 provides details vectors eager lear – get stuck code feel like knowing work vectors might help. 😊","code":"\neast_midlands_cities <- c(\"Derby\", \"Leicester\", \"Lincoln\", \"Nottingham\")\nlength(east_midlands_cities)## [1] 4\n# or using the pipe operator\nlibrary(tidyverse)\neast_midlands_cities %>% length()## [1] 4\n# Retrieve the third city\neast_midlands_cities[3]## [1] \"Lincoln\"\n# Retrieve the first city\neast_midlands_cities %>% first()## [1] \"Derby\"\n# Retrieve the first city\neast_midlands_cities %>% nth(1)## [1] \"Derby\"\n# Retrieve the third city\neast_midlands_cities %>% nth(3)## [1] \"Lincoln\"\n# Retrieve first and third city\neast_midlands_cities[c(1, 3)]## [1] \"Derby\"   \"Lincoln\"\none_to_ten <- 1:10\none_to_ten##  [1]  1  2  3  4  5  6  7  8  9 10\none_to_ten + 1##  [1]  2  3  4  5  6  7  8  9 10 11\nsqrt(one_to_ten)##  [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427\n##  [9] 3.000000 3.162278\nstr_length(east_midlands_cities)## [1]  5  9  7 10\n-3 > 0## [1] FALSE\nminus_two_to_two <- c(-2, -1, 0, 1, 2) \nminus_two_to_two > 0## [1] FALSE FALSE FALSE  TRUE  TRUE\nminus_two_to_two[c(TRUE, TRUE, FALSE, FALSE, TRUE)]## [1] -2 -1  2\nminus_two_to_two > 0## [1] FALSE FALSE FALSE  TRUE  TRUE\nminus_two_to_two[minus_two_to_two > 0]## [1] 1 2\nhouses_vector <- c(\"Bungalow\", \"Flat\", \"Flat\",\n  \"Detached\", \"Flat\", \"Terrace\", \"Terrace\")\nhouses_vector## [1] \"Bungalow\" \"Flat\"     \"Flat\"     \"Detached\" \"Flat\"     \"Terrace\"  \"Terrace\"\nhouses_factor <- factor(c(\"Bungalow\", \"Flat\", \"Flat\",\n  \"Detached\", \"Flat\", \"Terrace\", \"Terrace\"))\nhouses_factor## [1] Bungalow Flat     Flat     Detached Flat     Terrace  Terrace \n## Levels: Bungalow Detached Flat Terrace"},{"path":"data-manipulation.html","id":"lists","chapter":"3 Data manipulation","heading":"3.1.2 Lists","text":"Variables type list can contain elements different types (including vectors matrices), whereas elements vectors type.named lists element name, elements can selected using name symbol $.","code":"\nemployee <- list(\"Stef\", 2015)\nemployee## [[1]]\n## [1] \"Stef\"\n## \n## [[2]]\n## [1] 2015\nemployee[[1]] # Note the double square brackets for selection## [1] \"Stef\"\nemployee <- list(employee_name = \"Stef\", start_year = 2015)\nemployee## $employee_name\n## [1] \"Stef\"\n## \n## $start_year\n## [1] 2015\nemployee$employee_name## [1] \"Stef\""},{"path":"data-manipulation.html","id":"data-frames-and-tibbles","chapter":"3 Data manipulation","heading":"3.2 Data frames and tibbles","text":"mentioned first chapter, data frames complex data types encode concept table R combining arranging together series simple objects. Data frames similar named lists, element list vector representing column vectors length, thus representing number rows.Tidyverse, tibble defined : “modern re-imagining data frame, keeping time proven effective, throwing . Tibbles data.frames lazy surly: less complain forcing confront problems earlier, typically leading cleaner, expressive code”.continuing, create data subfolder copy 2011_OAC_Raw_uVariables_Leicester.csv file data subfolder. , create new RMarkdown documemnt compiled PDF file, using Data manipulation title data-manipulation.Rmd file name. Follow along examples copy-pasting code new R chunks, running one time compiling whole document.","code":""},{"path":"data-manipulation.html","id":"selecting-and-filtering-tables","chapter":"3 Data manipulation","heading":"3.2.1 Selecting and filtering tables","text":"approaches seen selecting filtering data vectors can applied data frames tibbles. difference tables bi-dimensional (rather one-dimensional), thus, two pieces information necessary. first index specifies rows select filter, second index specifies columns select filter. information provided either first second index, rows columns provided. However, can see examples , complex selection filtering query become, longer less readable code becomes.Fortunately, rather working base R instructions, can use dplyr library, part Tidyverse offers grammar data manipulation. function select can used select columns output. instance, code , function select used select columns OA11CD, supgrpcode, supgrpname, combination function slice_head, can used include first n rows (5 example ) output.function filter can instead used filter rows based specified condition. example , output filter step includes rows value grpname \"Students Around Campus\" (.e., OAs classified part Students Around Campus group). Note grpname needs included select step order able used subsequent filter operation. functions dplyr library can combined order makes logical sense. However, select step didn’t include grpname, column couldn’t used filter step.","code":"\nlibrary(tidyverse)\nlibrary(knitr)\n\n# Read the Leicester 2011 OAC dataset from the csv file\nleicester_2011OAC <- \n  read_csv(\"2011_OAC_Raw_uVariables_Leicester.csv\")\n# Select the 5th row\nleicester_2011OAC[5, ]\n\n# Select the 9th column (OAC supergroup name)\nleicester_2011OAC[, 9]\n\n# Select the 5th row and 9th column\nleicester_2011OAC[5, 9]\n\n# Select the OAC supergroup name column (9th column) of the 5th row\nleicester_2011OAC[5, \"supgrpname\"]\n\n# Select the OA code, OAC supergroup code name columns of the 5th row\nleicester_2011OAC[5, c(\"OA11CD\", \"supgrpcode\", \"supgrpname\")]\n\n# Select the OA code, OAC supergroup code name columns\n# of the 5th to 10th rows \nleicester_2011OAC[5:10, c(\"OA11CD\", \"supgrpcode\", \"supgrpname\")]\n\n# Select the OA code, OAC supergroup code name columns\n# for all OAs with more than 600 inhabitants\nleicester_2011OAC[leicester_2011OAC$Total_Population > 600, c(\"OA11CD\", \"supgrpcode\", \"supgrpname\")]\nleicester_2011OAC %>%\n  select(OA11CD, supgrpcode, supgrpname) %>%\n  slice_head(n = 5) %>%\n  kable()\nleicester_2011OAC %>%\n  select(OA11CD, supgrpname, grpname, Total_Population) %>%\n  filter(grpname == \"Students Around Campus\") %>%\n  slice_head(n = 5) %>%\n  kable()"},{"path":"data-manipulation.html","id":"summarise","chapter":"3 Data manipulation","heading":"3.2.2 Summarise","text":"function count dplyr library can used count number rows data frame. code provides leicester_2011OAC dataframe (read input section ) input function count pipe operator, thus creating new tibble one row one column, containing number rows dataframe – , number OAs Leicester.discussed previous lecture, tibble data type similar data frames, used Tidyverse libraries. Tidyverse functions output tibble rather data.frame objects representing table. However, data.frame object can provided input, automatically converted Tidyverse functions proceeding processing steps.example already shows pipe operator can used effectively multi-step operation. tibble outputted count function , column n provides count. function kable library knitr used produce well-formatted table.Note code goes new line every %>%, space added beginning new lines. common R programming (especially functions many parameters) makes code readable.verb count special case (shorthand) general verb summarise (summarize using American English spelling also available functionality), allows generating tables presenting aggregate values input data.instance, summarise can used create table containing:column presenting total population Leicester;column presenting total number OAs Leicester;column presenting average population per OAs Leicester;column presenting logical value stating whether OA Leicester contains 600 people.can achieved applying different aggregate functions column (Total_Population) leicester_2011OAC illustrated .sum calculate total population Leicester;n count number rows (mentioned count shorthand combination summarise n);mean calculate average population per OAs Leicester;conditional statement comparing Total_Population value 600 using numeric operator > (greater ) assess whether OA Leicester contains 600 people.set aggregate functions frequently used summarise available dplyr page summarise.carry complex aggregations, function summarise can used combination function group_by summarise values data frame based groups. Rows value column specified verb group_by (example , 2011 OAC supergroup, supgrpname) grouped together, values aggregated based functions specified verb summarise (using one columns calculation).shorthand function count also option specify column used internal grouping. example , column name supgrpname provided argument function count. result, output shows number rows grouped 2011 OAC supergroup.","code":"\nleicester_2011OAC %>%\n  count() %>%\n  kable()\nleicester_2011OAC %>%\n  summarise(\n    # Total population in Leicester\n    tot_pop = sum(Total_Population),\n    # Number of OAs, no input column needed\n    num_of_OAs = n(),\n    # Average population\n    avg_pop = mean(Total_Population), \n    # Is there any OA with over 200 people?\n    pop_over_600 = any(Total_Population > 600)\n  ) %>% \n  kable()\nleicester_2011OAC %>%\n  group_by(supgrpname) %>% \n  summarise(\n    # Total population in Leicester\n    tot_pop = sum(Total_Population),\n    # Number of OAs, no input column needed\n    num_of_OAs = n(),\n    # Average population\n    avg_pop = mean(Total_Population), \n    # Is there any OA with over 200 people?\n    pop_over_600 = any(Total_Population > 600)\n  ) %>% \n  kable()\nleicester_2011OAC %>%\n  count(supgrpname) %>%\n  kable()"},{"path":"data-manipulation.html","id":"mutate","chapter":"3 Data manipulation","heading":"3.2.3 Mutate","text":"function mutate can used create new column conducting operations current columns. instance, example , summarise first used calculate total number people number OAs per 2011 OAC supergroup. verb mutate used calculate average population per OA per 2011 OAC supergroup, recreating avg_pop column different process.second example, u005 column (represents area OA hectares, see 2011_OAC_Raw_uVariables_Lookup.csv) used calculate population density OA.","code":"\nleicester_2011OAC %>%\n  group_by(supgrpname) %>% \n  summarise(\n    # Total population in Leicester\n    tot_pop = sum(Total_Population),\n    # Number of OAs, no input column needed\n    num_of_OAs = n(),\n    # Is there any OA with over 200 people?\n    pop_over_600 = any(Total_Population > 600)\n  ) %>% \n  mutate(\n    # Average population\n    avg_pop = tot_pop / num_of_OAs, \n  ) %>% \n  kable(digits = c(0, 0, 0, 0, 2))\nleicester_2011OAC %>%\n  mutate(\n    # Population density\n    pop_density = Total_Population / u005, \n  ) %>% \n  select(OA11CD, pop_density) %>% \n  slice_head(n = 10) %>% \n  kable(digits = c(0, 2))"},{"path":"data-manipulation.html","id":"arrange","chapter":"3 Data manipulation","heading":"3.2.4 Arrange","text":"function arrange can used sort tibble ascending order values specified column. operator - specified column name, descending order used. code produce table showing ten OAs largest population Leicester.example , used slice_head present first n (example 10) rows table based existing order. dplyr library also provides functions slice_max slice_min incorporate sorting functionality (see slice reference page)., following code uses slice_max produce table including 10 OAs highest population.following code, instead, uses slice_min, thus producing table including 10 OAs lowest population..cases, table contains ties, rows containing value present among maximum minimum selected values presented, case rows containing value 21 example .","code":"\nleicester_2011OAC %>%\n  select(\n    OA11CD, supgrpname, \n    Total_Population\n  ) %>% \n  arrange(\n    # Descending delay\n    -Total_Population\n  ) %>% \n  slice_head(n = 10) %>% \n  kable()\nleicester_2011OAC %>%\n  select(\n    OA11CD, supgrpname, \n    Total_Population\n  ) %>% \n  slice_max(\n    Total_Population, \n    n = 10\n  ) %>%\n  kable()\nleicester_2011OAC %>%\n  select(\n    OA11CD, supgrpname, \n    Total_Population\n  ) %>% \n  slice_min(\n    Total_Population, \n    n = 10\n  ) %>%\n  kable()"},{"path":"data-manipulation.html","id":"data-manipulation-workflow","chapter":"3 Data manipulation","heading":"3.2.5 Data manipulation workflow","text":"Finally, code illustrates complex, multi-step operation using functions discussed . full example short analysis using one series pipes read, process write data using R almost tidyverse verbs seen far.input data read csv file part data selected filtered. data grouped, aggregated arranged order. percentage people living OAs assigned 2011 OAC supergroup calculated. Tee pipe (magrittr library) used write resulting table file, also passing input subsequent kable function display data.","code":"\nlibrary(magrittr)\n\n# Let's start from the filename\n\"2011_OAC_supgrp_Leicester.csv\" %>% \n  # as input to the read_csv function\n  read_csv(col_types = \"cccci\") %>% \n  # Select only the necessary columns\n  select(supgrpname,    Total_Population) %>% \n  # Let's say we are not interested in\n  # the Suburbanites supergroup\n  filter(supgrpname != \"Suburbanites\") %>% \n  # Group by supergroup\n  group_by(supgrpname) %>% \n  # Aggregate population\n  summarise(tot_pop = sum(Total_Population)) %>% \n  # Ungroup\n  ungroup() %>% \n  # Arrange by descending total population\n  arrange(-tot_pop) %>% \n  # Calculate percentage\n  mutate(perc_pop = (tot_pop / 329839) * 100) %T>%\n  # Then use the Tee pipe %T>% in the line above\n  # to write the calculated values to file\n  # and pass the same input values to the kable function\n  write_csv(\n    \"2011_Leicester_pop_per_OAC_supgrp_excl_suburb.csv\"\n  ) %>% \n  # Print to screen nicely\n  kable()"},{"path":"data-manipulation.html","id":"componentization","chapter":"3 Data manipulation","heading":"3.3 Componentization","text":"important note example , information stored local environment. input read directly computer storage, whole process conducted R internal memory, output saved back computer storage. whole purpose pipe operator: avoid creating unnecessary temporary “mid-products” computation (.e., variables whose purpose store data next step).However, frequently useful split processing different steps, allow data inspected mid-way analysis data used twice two subsequent steps. Finding good balance long short pipe sequences many “mid-products” programmer’s task – something can learnt experience observing good examples code.following example showcases process can subdivided five meaningful steps.Read data. frequently done separate step, datasets might require significant time read computer storage, frequently best read .Calculate total population per 2011 OAC supergroup, excluding suburban population. significant transformation original data, might worth saving inspecting.Update new table using mutate assignment pipe operator %<>% (magrittr library).Print result.Save result computer storage.","code":"\nleicester_2011OAC <-\n  # Let's start from the filename\n  \"2011_OAC_supgrp_Leicester.csv\" %>% \n  # as input to the read_csv function\n  read_csv(col_types = \"cccci\") \nleicester_nonsuburb_pop <-\n  leicester_2011OAC %>% \n  # Select only the necessary columns\n  select(supgrpname,    Total_Population) %>% \n  # Let's say we are not interested in\n  # the Suburbanites supergroup\n  filter(supgrpname != \"Suburbanites\") %>% \n  # Group by supergroup\n  group_by(supgrpname) %>% \n  # Aggregate population\n  summarise(tot_pop = sum(Total_Population)) %>% \n  # Ungroup\n  ungroup() %>% \n  # Arrange by descending total population\n  arrange(-tot_pop)\n\nleicester_nonsuburb_pop %<>%\n  # Calculate percentage\n  mutate(perc_pop = (tot_pop / 329839) * 100)\nleicester_nonsuburb_pop %>% \n  # Print to screen nicely\n  kable(digits = c(0, 0, 2))\nleicester_nonsuburb_pop %>% \n  # Write the calculated values to file\n  write_csv(\n    \"2011_Leicester_pop_per_OAC_supgrp_excl_suburb.csv\"\n  ) "},{"path":"data-manipulation.html","id":"exercises-103.1","chapter":"3 Data manipulation","heading":"3.4 Exercises 103.1","text":"Create RMarkdown document RStudio, using Exercise 103 title PDF output. Delete contents except first five lines compose heading. Save document practical-103_exercises.Rmd. Add libraries code necessary read data 2011_OAC_Raw_uVariables_Leicester.csv file. Create first section document (e.g., adding second heading Exercise 103.1) add answers questions .order answer questions , inspect look-table 2011_OAC_Raw_uVariables_Lookup.csv (e.g., using Microsoft Excel) identify columns necessary complete task.Question 103.1.1: Identify five variables part variable subdomain Housing Type write code necessary compute total number household spaces Leicester housing type.Question 103.1.2: Write code necessary compute total number household spaces Leicester housing type grouped 2011 OAC supergroup.Question 103.1.3: Write code necessary compute percentage household spaces (.e., total number household spaces) Leicester housing type grouped 2011 OAC supergroup.Question 103.1.4: Modify code written Question 103.1.3, using verb rename change column names columns containing percentages names resemble related housing type (e.g., perc_of_detached).","code":""},{"path":"data-manipulation.html","id":"exercises-103.2","chapter":"3 Data manipulation","heading":"3.5 Exercises 103.2","text":"Question 103.2.1: Explore look-table 2011_OAC_Raw_uVariables_Lookup.csv identify another set variables think might relate type housing 2011 OAC supergroups. Create new section document (e.g., adding second heading) include short text (200 words) RMarkdown document describing justifying set variables choose.Question 103.2.2: Write code necessary conduct analysis conducted Question 103.1 using variables identified Question 103.2.1.Question 103.2.3: Inspect table obtained answer Question 103.1.3 table obtained answer Question 103.2.2, compare results. Write short text (300 words) RMarkdown document reporting discussing results tell socio-demographic structure Leicester.Stefano De Sabbata – text licensed CC -SA 4.0, contains public sector information licensed Open Government Licence v3.0, code licensed GNU GPL v3.0.","code":""},{"path":"table-operations.html","id":"table-operations","chapter":"4 Table operations","heading":"4 Table operations","text":"Print chapterThe first two sections illustrate pivot join functionalities Tidyverse libraries using simple examples. last section instead presents complex example, loading wrangling data related 2011 Output Area Classification Indexes Multiple Deprivation 2015.","code":"\nlibrary(tidyverse)\nlibrary(knitr)"},{"path":"table-operations.html","id":"long-and-wide-table-formats","chapter":"4 Table operations","heading":"4.1 Long and wide table formats","text":"Tabular data usually presented two different formats.Wide: common approach, real-world entity (e.g. city) represented one single row attributes represented different columns (e.g., column representing total population area, another column representing size area, etc.).Long: probably less common approach, still necessary many cases, real-world entity (e.g. city) represented multiple rows, one reporting one attributes. case, one column used indicate attribute row represent, another column used report value.tidyr library provides two functions allow transforming wide-formatted data long format, vice-versa. Please take time understand example check tidyr help pages continuing.","code":"\ncity_info_wide <- data.frame(\n    city = c(\"Leicester\", \"Nottingham\"),\n    population = c(329839, 321500),\n    area = c(73.3, 74.6),\n    density = c(4500, 4412)\n  ) %>%\n  as_tibble()\n\ncity_info_wide %>%\n kable()\ncity_info_long <- city_info_wide %>%\n  pivot_longer(\n    # exclude IDs (city names) from the pivoted columns\n    cols = -city,\n    # name for the new column containing\n    # the names of the old columns\n    names_to = \"attribute\",\n    # name for the new column containing\n    # the values included under the old columns\n    values_to = \"value\"\n  )\n\ncity_info_long %>%\n  kable()\ncity_info_back_to_wide <- city_info_long %>%\n  pivot_wider(\n    # column containing the attribute names\n    names_from = attribute,\n    # column containing the values\n    values_from = value\n  )\n\ncity_info_back_to_wide %>%\n  kable()"},{"path":"table-operations.html","id":"joining-tables","chapter":"4 Table operations","heading":"4.2 Joining tables","text":"join operation combines two tables one matching rows values specified column. operation usually executed columns containing identifiers, matched different tables containing different data real-world entities. instance, table presents telephone prefixes two cities. information can combined data present wide-formatted table join operation columns containing city names. two tables contain cities, full join operation executed, cells values assigned.discussed lecture, dplyr library offers different types join operations, correspond different SQL joins illustrated image . use implications different types joins discussed detail GY7708 module next semester.Please take time understand example check related dplyr help pages continuing. first four examples execute exact full join operation using three different syntaxes: without using pipe operator, specifying argument . Note approaches writing join valid produce result. choice approach use depend code writing. particular, might find useful use syntax uses pipe operator join operation one stem series data manipulation steps. Using argument usually advisable unless certain aim join two tables exactly column names two table.Note result join operations saved variable. function kable added join operation pipe %>% display resulting table nice format.","code":"\ncity_telephone_prexix <- data.frame(\n    city = c(\"Leicester\", \"Birmingham\"),\n    telephon_prefix = c(\"0116\", \"0121\")\n  ) %>%\n  as_tibble()\n\ncity_telephone_prexix %>%\n  kable()\n# Option 1: without using the pipe operator\n\n# full join verb\nfull_join(\n    # left table\n    city_info_wide,\n    # right table\n    city_telephone_prexix,\n    # columns to match\n    by = c(\"city\" = \"city\")\n  ) %>%\n  kable()\n# Option 2: without using the pipe operator\n#   and without using the argument \"by\"\n#   as columns have the same name\n#   in the two tables.\n# Same result as Option 1\n\n# full join verb\nfull_join(\n    # left table\n    city_info_wide,\n    # right table\n    city_telephone_prexix\n  ) %>%\n  kable()\n# Option 3: using the pipe operator\n#   and without using the argument \"by\"\n#   as columns have the same name\n#   in the two tables.\n# Same result as Option 1 and 2\n\n# left table\ncity_info_wide %>%\n  # full join verb\n  full_join(\n    # right table\n    city_telephone_prexix\n  ) %>%\n  kable()\n# Option 4: using the pipe operator\n#   and using the argument \"by\".\n# Same result as Option 1, 2 and 3\n\n# left table\ncity_info_wide %>%\n  # full join verb\n  full_join(\n    # right table\n    city_telephone_prexix,\n    # columns to match\n    by = c(\"city\" = \"city\")\n  ) %>%\n  kable()\n# Left join\n# Using syntax similar to Option 1 above\n\n# left join\nleft_join(\n    # left table\n    city_info_wide, \n    # right table\n    city_telephone_prexix,\n    # columns to match\n    by = c(\"city\" = \"city\")\n  ) %>%\n  kable()\n# Right join\n# Using syntax similar to Option 2 above\n\n# right join verb\nright_join(\n    # left table\n    city_info_wide, \n    # right table\n    city_telephone_prexix\n  ) %>%\n  kable()\n# Inner join\n# Using syntax similar to Option 3 above\n\n# left table\ncity_info_wide %>%\n  # inner join\n  inner_join(\n    # right table\n    city_telephone_prexix\n  ) %>%\n  kable()"},{"path":"table-operations.html","id":"data-indices-of-multiple-deprivation","chapter":"4 Table operations","heading":"4.3 Data: Indices of Multiple Deprivation","text":"Open Leicester_population project created previous chapter. Create new RMarkdown document using “Exploring deprivation indices Leicester” title PDF output file type. Delete example code setup chunk. Add new markdown second-heading section named Libraries chunk loading tidyverse knitr libraries (see ). Save file name exploring_Leicester_deprivation.Rmd Leicester_population project.Download Blackboard (data folder repository) file IndexesMultipleDeprivation2015_Leicester.csv upload file Leicester_population folder clicking Upload button selecting files computer.Indices Multiple Deprivation 2015 (see map cdrc.ac.uk) based series variables across seven distinct domains deprivation combined calculate Index Multiple Deprivation 2015 (IMD 2015). overall measure multiple deprivations experienced people living area. indexes calculated every Lower layer Super Output Area (LSOA), larger geographic units OAs used 2011 OAC. dataset file IndexesMultipleDeprivation2015_Leicester.csv contains main Index Multiple Deprivation, well values seven distinct domains deprivation two additional indexes regarding deprivation affecting children older people. dataset includes scores, ranks (1 indicates deprived area), decile (.e., first decile includes 10% deprived areas England).code loads IMD 2015 dataset.","code":"\n# Load Indexes of Multiple deprivation data\nleicester_IMD2015 <- \n  read_csv(\"IndexesMultipleDeprivation2015_Leicester.csv\")\nleicester_2011OAC <- \n  read_csv(\"2011_OAC_Raw_uVariables_Leicester.csv\")"},{"path":"table-operations.html","id":"working-with-multiple-tables","chapter":"4 Table operations","heading":"4.4 Working with multiple tables","text":"","code":""},{"path":"table-operations.html","id":"from-long-to-wide-table","chapter":"4 Table operations","heading":"4.4.1 From long to wide table","text":"IMD 2015 data long format, means every area represented one row: column Value presents value; column IndicesOfDeprivation indicates index value refers ; column Measurement indicates whether value score, rank, decile. code illustrates data format used IndicesOfDeprivation table, showing rows LSOA including University Leicester (feature code E01013649).following section, analysis aims explore certain census variables vary areas different deprivation levels. Thus, need extract Decile rows IMD 2015 dataset transform data wide format, index represented separate column.purpose, also need change name indexes slightly, exclude spaces punctuation, new column names simpler original text, can used column names. part manipulation performed using mutate functions stringr library.Let’s compare columns original long IMD 2015 dataset wide dataset created , using function colnames.leicester_IMD2015_decile_wide, now one row representing LSOA including University Leicester (feature code E01013649) main Index Multiple Deprivations now represented column IndexofMultipleDeprivationIMD. value reported – 5, means selected LSOA estimated range 40-50% deprived areas England – changed data format.","code":"\nleicester_IMD2015 %>%\n  filter(FeatureCode == \"E01013649\") %>%\n  select(FeatureCode, IndicesOfDeprivation, Measurement, Value) %>%\n  kable()\nleicester_IMD2015_decile_wide <- leicester_IMD2015 %>%\n  # Select only Socres\n  filter(Measurement == \"Decile\") %>%\n  # Trim names of IndicesOfDeprivation\n  mutate(\n    IndicesOfDeprivation = str_replace_all(IndicesOfDeprivation, \"\\\\s\", \"\")\n  ) %>%\n  mutate(\n    IndicesOfDeprivation = str_replace_all(IndicesOfDeprivation, \"[:punct:]\", \"\")\n  ) %>%\n  mutate(\n    IndicesOfDeprivation = str_replace_all(IndicesOfDeprivation, \"\\\\(\", \"\")\n  ) %>%\n  mutate(\n    IndicesOfDeprivation = str_replace_all(IndicesOfDeprivation, \"\\\\)\", \"\")\n  ) %>%\n  # Spread\n  pivot_wider(\n    names_from = IndicesOfDeprivation,\n    values_from = Value\n  ) %>%\n  # Drop columns\n  select(-DateCode, -Measurement, -Units)\nleicester_IMD2015 %>% \n  colnames() %>%\n  # limit width of printing area\n  print(width = 70)## [1] \"FeatureCode\"          \"DateCode\"            \n## [3] \"Measurement\"          \"Units\"               \n## [5] \"Value\"                \"IndicesOfDeprivation\"\nleicester_IMD2015_decile_wide %>% \n  colnames() %>%\n  # limit width of printing area\n  print(width = 70)##  [1] \"FeatureCode\"                                     \n##  [2] \"HealthDeprivationandDisabilityDomain\"            \n##  [3] \"IncomeDeprivationAffectingOlderPeopleIndexIDAOPI\"\n##  [4] \"BarrierstoHousingandServicesDomain\"              \n##  [5] \"EmploymentDeprivationDomain\"                     \n##  [6] \"EducationSkillsandTrainingDomain\"                \n##  [7] \"LivingEnvironmentDeprivationDomain\"              \n##  [8] \"IncomeDeprivationAffectingChildrenIndexIDACI\"    \n##  [9] \"CrimeDomain\"                                     \n## [10] \"IndexofMultipleDeprivationIMD\"                   \n## [11] \"IncomeDeprivationDomain\"\n# Original long IMD 2015 dataset\nleicester_IMD2015 %>%\n  filter(\n    FeatureCode == \"E01013649\",\n    IndicesOfDeprivation == \"Index of Multiple Deprivation (IMD)\",\n    Measurement == \"Decile\"\n  ) %>%\n  select(FeatureCode, IndicesOfDeprivation, Measurement, Value) %>%\n  kable()\n# New wide IMD 2015 dataset\nleicester_IMD2015_decile_wide %>%\n  filter(FeatureCode == \"E01013649\") %>%\n  select(FeatureCode, IndexofMultipleDeprivationIMD) %>%\n  kable()"},{"path":"table-operations.html","id":"joining-tables-1","chapter":"4 Table operations","heading":"4.4.2 Joining tables","text":"discussed , two tables can joined using common column identifiers. can thus join 2011 OAC IMD 2015 datasets single table. LSOA code included 2011 OAC table used match information corresponding row IMD 2015. resulting table provides information 2011 OAC OA, plus Index Multiple Deprivations decile LSOA containing OA.operation can carried using function inner_join, specifying common column (columns, one used identifier) argument . Note using inner_join result dropping row doesn’t match table, either way. case, happen, OAs part LSOA, LSOA contains least one OA.LSOA contains multiple OAs, row leicester_IMD2015_decile_wide table matched multiple rows leicester_2011OAC table. instance, shown table , information IMD 2015 dataset LSOA encompassing University Leicester (feature code E01013649) joined multiple rows 2011 OAC dataset, including OA encompassing University Leicester (feature code E00068890) well neighbouring OAs.result stored variable leicester_2011OAC_IMD2015, analysis can carried . instance, count can used count many OAs fall 2011 OAC supergroup decile Index Multiple Deprivations.another example, code can used group OAs based decile calculate percentage adults employment using u074 (adults employment household: dependent children) u075 (adults employment household: dependent children) variables 2011 OAC dataset.","code":"\nleicester_2011OAC_IMD2015 <- \n  leicester_2011OAC %>%\n  inner_join(\n    leicester_IMD2015_decile_wide, \n    by = c(\"LSOA11CD\" = \"FeatureCode\")\n  )\nleicester_2011OAC_IMD2015 %>%\n  # Note that the LSOA11CD column needs to be used\n  # as the previous join as combined \n  # LSOA11CD and FeatureCode\n  # into one, name LSOA11CD\n  filter(LSOA11CD == \"E01013649\") %>%\n  select(OA11CD, LSOA11CD, supgrpname, IndexofMultipleDeprivationIMD) %>%\n  kable()\nleicester_2011OAC_IMD2015 %>%\n  count(supgrpname, IndexofMultipleDeprivationIMD) %>%\n  kable()\nleicester_2011OAC_IMD2015 %>%\n  group_by(IndexofMultipleDeprivationIMD) %>%\n  summarise(\n    adults_not_empl_perc = (sum(u074 + u075) / sum(Total_Population)) * 100\n  ) %>%\n  kable()"},{"path":"table-operations.html","id":"exercise-104.1","chapter":"4 Table operations","heading":"4.5 Exercise 104.1","text":"Extend “Exploring deprivation indices Leicester” document include code necessary solve questions . Use full list variable names 2011 UK Census used generate 2011 OAC can found file 2011_OAC_Raw_uVariables_Lookup.csv indetify columns use complete tasks.Question 104.1.1: Write piece code using pipe operator dplyr library generate table showing percentage EU citizens total population, calculated grouping OAs related decile Index Multiple Deprivations, accounting areas classified Cosmopolitans Ethnicity Central Multicultural Metropolitans.Question 104.1.2: Write piece code using pipe operator dplyr library generate table showing percentage EU citizens total population, calculated grouping OAs related supergroup 2011 OAC, accounting areas top 5 deciles Index Multiple Deprivations.Question 104.1.3: Write piece code using pipe operator dplyr library generate table showing percentage people aged 65 , calculated grouping OAs related supergroup 2011 OAC decile Index Multiple Deprivations, ordering table calculated value descending order.Question 104.1.4: Write piece code using pipe operator dplyr tidyr libraries generate long format leicester_2011OAC_IMD2015 table including values (census variables) used Question 104.1.3.Question 104.1.5: Write piece code using pipe operator dplyr tidyr libraries generate table similar one generated Question 104.1.4, showing values percentages total population.Stefano De Sabbata – text licensed CC -SA 4.0, contains public sector information licensed Open Government Licence v3.0, code licensed GNU GPL v3.0.","code":""},{"path":"exploratory-visualisation.html","id":"exploratory-visualisation","chapter":"5 Exploratory visualisation","heading":"5 Exploratory visualisation","text":"Print chapterThis chapter showcases exploratory analysis distribution people aged 20 24 Leicester, focusing u011 variable 2011 Output Area Classification (2011OAC) dataset introduced previous chapters.continuing, create new R project RStudio, upload 2011_OAC_Raw_uVariables_Leicester.csv file project folder. Create new RMarkdown document within project replicate analysis document. document set , start adding R code snipped including code , loads 2011OAC dataset libraries used chapter.","code":"\nlibrary(tidyverse)\nlibrary(knitr)\nleicester_2011OAC <- read_csv(\"2011_OAC_Raw_uVariables_Leicester.csv\")"},{"path":"exploratory-visualisation.html","id":"gglot2-recap","chapter":"5 Exploratory visualisation","heading":"5.1 GGlot2 recap","text":"seen introductory chapter, ggplot2 library part Tidyverse, offers series functions creating graphics declaratively, based concepts outlined Grammar Graphics. dplyr library offers functionalities cover data manipulation variable transformations, ggplot2 library offers functionalities allow specify elements, define guides, apply scale coordinate system transformations.Marks can specified ggplot2 using geom_ functions.mapping variables (table columns) visual variables can specified ggplot2 using aes element.Furthermore, ggplot2 library:\nautomatically adds necessary guides using default table column names, additional functions can used overwrite defaults;\nprovides wide range scale_ functions can used control scales visual variables;\nprovides series coord_ fucntions allow transforming coordinate system.\nautomatically adds necessary guides using default table column names, additional functions can used overwrite defaults;provides wide range scale_ functions can used control scales visual variables;provides series coord_ fucntions allow transforming coordinate system.Check ggplot2 reference details functions options discussed . R Graph Gallery also provides good overview many charts can created using R. Healy’s Data Visualization: Practical Introduction Kabacoff’s Data Visualization R provide detailed introduction data visualisation using R ggplot2.","code":""},{"path":"exploratory-visualisation.html","id":"visualising-amounts","chapter":"5 Exploratory visualisation","heading":"5.2 Visualising Amounts","text":"start analysis visualising number people living Leicester age bracket available dataset.first step summarise total amounts one columns u007 (Age 0 4) u019 (Age 90 ). code uses dplyr verb across apply summarise function sum across columns u007 u019. table pivoted longer age bracket entity table, number people age bracket attribute visualised – procedure unclear, try execute step--step (summarise first, pivot) see different stages.values can represented visualisation using bar chart, combines visual variables size (bar) position (x-axis, end bar) encode numeric value. function theme_bw used use simple black--white theme.","code":"\nleicester_2011OAC %>% \n  summarise(\n    across(\n      u007:u019,\n      sum\n    )\n  ) %>% \n  pivot_longer(\n    cols = everything(),\n    names_to = \"age_bracket\",\n    values_to = \"num_of_people\"\n  ) %>% \n  ggplot(\n    aes(\n      x = num_of_people,\n      y = age_bracket\n    )\n  ) +\n  geom_col() +\n  theme_bw()"},{"path":"exploratory-visualisation.html","id":"visualising-proportions","chapter":"5 Exploratory visualisation","heading":"5.3 Visualising Proportions","text":"better observe values compare proportions overall total, visualisation can transformed stacked bar chart, compounding area marks one single bar. number people can moved x y axis. age bracket can moved y axis fill visual variable.13 categories, can’t use ColorBrewer schemes case (palettes count 12 colours). Thus, can use scale_fill_tableau function ggthemes library select \"Tableau 20\" colour palette, offers 20 clearly distinct colours designed Tableau, complete definition colour hue visual variable. factor(1) value can used create single bar x axis.polar coordinate system transformation coord_polar(theta = \"y\") can used transform stacked bar chart pie chart.can use theme_minimal theme set theme elements element_blank clean-chart. can also rename age brackets columns ahead pivot change order slices using direction = -1 make chart readable.categories case clear order, visual variable colour value used instead colour hue, thus using series shared hue. However, 13 categories, can’t use ColorBrewer schemes. Thus, example uses one viridis colour palettes.can see, creating pie chart ggplot2 provides great insight nature visualisations possibilities offered library, can also quite tricky – see, instance, Kabacoff’s section pie chatrs, ggplot2 Piechart page R Graph Gallery ggplot2 pie chart guide Statistical tools high-throughput data analysis (STHDA) examples.Self-test question: one two pie charts think readable ?","code":"\nlibrary(ggthemes)\n\nleicester_2011OAC %>% \n  summarise(\n    across(\n      u007:u019,\n      sum\n    )\n  ) %>% \n  pivot_longer(\n    cols = everything(),\n    names_to = \"age_bracket\",\n    values_to = \"num_of_people\"\n  ) %>% \n  ggplot(\n    aes(\n      x = factor(1),\n      y = num_of_people,\n      fill = age_bracket\n    )\n  ) +\n  geom_col() +\n  scale_fill_tableau(\"Tableau 20\") +\n  theme_bw()\nleicester_2011OAC %>% \n  summarise(\n    across(\n      u007:u019,\n      sum\n    )\n  ) %>% \n  pivot_longer(\n    cols = everything(),\n    names_to = \"age_bracket\",\n    values_to = \"num_of_people\"\n  ) %>% \n  ggplot(\n    aes(\n      x = factor(1),\n      y = num_of_people,\n      fill = age_bracket\n    )\n  ) +\n  geom_col() +\n  scale_fill_tableau(\"Tableau 20\") +\n  coord_polar(theta = \"y\") +\n  theme_bw()\nleicester_2011OAC %>% \n  summarise(\n    across(\n      u007:u019,\n      sum\n    )\n  ) %>% \n  rename(\n    `0 to 4` = u007,\n    `5 to 9` = u008,\n    `10 to 14` = u009,\n    `15 to 19` = u010,\n    `20 to 24` = u011,\n    `25 to 29` = u012,\n    `30 to 44` = u013,\n    `45 to 59` = u014,\n    `60 to 64` = u015,\n    `65 to 74` = u016,\n    `75 to 84` = u017,\n    `85 to 89` = u018,\n    `90 and over` = u019\n  ) %>% \n  pivot_longer(\n    cols = everything(),\n    names_to = \"age_bracket\",\n    values_to = \"num_of_people\"\n  ) %>% \n  ggplot(\n    aes(\n      x = factor(1),\n      y = num_of_people,\n      fill = age_bracket\n    )\n  ) +\n  geom_col() +\n  scale_fill_tableau(\"Tableau 20\") +\n  coord_polar(theta = \"y\", direction = -1) +\n  labs(\n    fill = \"Age bracket (years old)\"\n  ) +\n  theme_minimal() +\n  theme(\n    panel.border = element_blank(),\n    panel.grid = element_blank(),\n    axis.ticks = element_blank(),\n    axis.title.x = element_blank(),\n    axis.text.x = element_blank(),\n    axis.title.y = element_blank(),\n    axis.text.y = element_blank()\n  )\nleicester_2011OAC %>% \n  summarise(\n    across(\n      u007:u019,\n      sum\n    )\n  ) %>% \n  rename(\n    `0 to 4` = u007,\n    `5 to 9` = u008,\n    `10 to 14` = u009,\n    `15 to 19` = u010,\n    `20 to 24` = u011,\n    `25 to 29` = u012,\n    `30 to 44` = u013,\n    `45 to 59` = u014,\n    `60 to 64` = u015,\n    `65 to 74` = u016,\n    `75 to 84` = u017,\n    `85 to 89` = u018,\n    `90 and over` = u019\n  ) %>% \n  pivot_longer(\n    cols = everything(),\n    names_to = \"age_bracket\",\n    values_to = \"num_of_people\"\n  ) %>% \n  ggplot(\n    aes(\n      x = factor(1),\n      y = num_of_people,\n      fill = age_bracket\n    )\n  ) +\n  geom_col() +\n  scale_fill_viridis_d() +\n  coord_polar(theta = \"y\", direction = -1) +\n  labs(\n    fill = \"Age bracket (years old)\"\n  ) +\n  theme_minimal() +\n  theme(\n    panel.border = element_blank(),\n    panel.grid = element_blank(),\n    axis.ticks = element_blank(),\n    axis.title.x = element_blank(),\n    axis.text.x = element_blank(),\n    axis.title.y = element_blank(),\n    axis.text.y = element_blank()\n  )"},{"path":"exploratory-visualisation.html","id":"visualising-distributions","chapter":"5 Exploratory visualisation","heading":"5.4 Visualising Distributions","text":"histogram can used explore distribution variable u011.aim explore portion population distributed among different supergroups 2011OAC, number charts allow us visualise relationship.instance, bar chart can enhanced use visual variable colour fill option. code creates stacked bar chart sections bar filled colour associated 2011OAC supergroup.However, graphic extremely clear. boxplot violin plot created data shown . cases, parameter axis.text.x function theme set element_text(angle = 90, hjust = 1) order orientate labels x-axis vertically, supergroup names rather long, overlap one-another set horizontally x-axis. cases, option fig.height R snippet RMarkdown set higher value (e.g., 5) allow sufficient room supergroup names.","code":"\nleicester_2011OAC %>%\n  ggplot(\n    aes(\n      x = u011\n    )\n  ) +\n  geom_histogram(binwidth = 5) +\n  theme_bw()\nleicester_2011OAC %>%\n  ggplot(\n    aes(\n      x = u011,\n      fill = fct_reorder(supgrpname, supgrpcode)\n    )\n  ) +\n  geom_histogram(binwidth = 5) +\n  ggtitle(\"Leicester's young adults\") +\n  labs(\n    fill = \"2011 Output Area\\nClassification\\n(supergroups)\"\n  ) +\n  xlab(\"Residents aged 20 to 24\") +\n  ylab(\"Count\") +\n  scale_fill_manual(\n    values = c(\"#e41a1c\", \"#f781bf\", \"#ff7f00\", \"#a65628\", \"#984ea3\", \"#377eb8\", \"#ffff33\")\n  ) +\n  theme_bw()\nleicester_2011OAC %>%\n  ggplot(\n    aes(\n      x = fct_reorder(supgrpname, supgrpcode),\n      y = u011,\n      fill = fct_reorder(supgrpname, supgrpcode)\n    )\n  ) +\n  geom_boxplot() +\n  ggtitle(\"Leicester's young adults\") +\n  labs(\n    fill = \"2011 Output Area\\nClassification\\n(supergroups)\"\n  ) +\n  xlab(\"2011 Output Area Classification (supergroups)\") +\n  ylab(\"Residents aged 20 to 24\") +\n  scale_fill_manual(\n    values = c(\"#e41a1c\", \"#f781bf\", \"#ff7f00\", \"#a65628\", \"#984ea3\", \"#377eb8\", \"#ffff33\")\n  ) +\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\nleicester_2011OAC %>%\n  ggplot(\n    aes(\n      x = fct_reorder(supgrpname, supgrpcode),\n      y = u011,\n      fill = fct_reorder(supgrpname, supgrpcode)\n    )\n  ) +\n  geom_violin() +\n  ggtitle(\"Leicester's young adults\") +\n  labs(\n    fill = \"2011 Output Area\\nClassification\\n(supergroups)\"\n  ) +\n  xlab(\"2011 Output Area Classification (supergroups)\") +\n  ylab(\"Residents aged 20 to 24\") +\n  scale_fill_manual(\n    values = c(\"#e41a1c\", \"#f781bf\", \"#ff7f00\", \"#a65628\", \"#984ea3\", \"#377eb8\", \"#ffff33\")\n  ) +\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))"},{"path":"exploratory-visualisation.html","id":"visualising-relationships","chapter":"5 Exploratory visualisation","heading":"5.5 Visualising Relationships","text":"first bar chart seems illustrate distribution might skewed towards left, values seemingly 50. However, tells part story people aged 20 24 distributed Leicester. fact, Output Area (OA) different total population. , higher number people aged 20 24 living OA might simply due OA populous others. Thus, next step compare u011 Total_Population, instance, scatterplot one .","code":"\nleicester_2011OAC %>%\n  ggplot(\n    aes(\n      x = Total_Population,\n      y = u011,\n      colour = fct_reorder(supgrpname, supgrpcode)\n    )\n  ) +\n  geom_point(size = 0.5) +\n  ggtitle(\"Leicester's young adults\") +\n  labs(\n    colour = \"2011 Output Area\\nClassification\\n(supergroups)\"\n  ) +\n  xlab(\"Total number of residents\") +\n  ylab(\"Residents aged 20 to 24\") +\n  scale_y_log10() +\n  scale_colour_brewer(palette = \"Set1\") +\n  scale_colour_manual(\n    values = c(\"#e41a1c\", \"#f781bf\", \"#ff7f00\", \"#a65628\", \"#984ea3\", \"#377eb8\", \"#ffff33\")\n  ) +\n  theme_bw()"},{"path":"exploratory-visualisation.html","id":"exercise-201.1","chapter":"5 Exploratory visualisation","heading":"5.6 Exercise 201.1","text":"Open Leicester_population project used previous chapters extend “Exploring deprivation indices Leicester” document include code necessary solve questions . Use full list variable names 2011 UK Census used generate 2011 OAC can found file 2011_OAC_Raw_uVariables_Lookup.csv indetify columns use complete tasks.Question 201.1.1: Write piece code create chart showing percentage EU citizens total population decile Index Multiple DeprivationsQuestion 201.1.2: Write piece code create chart showing relationship percentage EU citizens total population related score Index Multiple Deprivations, illustrating also 2011 OAC class OA.Question 201.1.3: Write piece code create chart showing relationship percentage people aged 65 related score Income Deprivation, illustrating also 2011 OAC class OA.Question 201.1.4: graph produced Question 201.1.3 mean? Write 100 words explaining conclusions can drawn graph – remember “larger score, deprived area”.Question 201.1.5: Identify index multiple deprivation closely relate percentage people per OA whose “day--day activities limited lot little” based “Standardised Illness Ratio”.Stefano De Sabbata – text licensed CC -SA 4.0, contains public sector information licensed Open Government Licence v3.0, code licensed GNU GPL v3.0.","code":""},{"path":"exploratory-statistics.html","id":"exploratory-statistics","chapter":"6 Exploratory statistics","heading":"6 Exploratory statistics","text":"Print chapterThis chapter showcases exploratory analysis distribution people aged 20 24 Leicester, using u011 variable 2011 Output Area Classification (2011OAC) already used previous chapters. continuing, open project created previous chapter. Create new RMarkdown document within project replicate analysis document. document set , start adding R code snipped including code , loads 2011OAC dataset libraries used chapter.graphics provide preliminary evidence distribution people aged 20 24 might, fact, different different 2011 supergroups. remainder chapter, going explore hypothesis . First, load necessary statistical libraries.code calculates percentage people aged 20 24 (.e., u011) total population per OA, also recodes (see recode) names 2011OAC supergroups shorter 2-letter version, useful tables presented . Note code uses library name prefix dplyr:: front function name recode (thus, dplyr::recode) make sure function recode package car (thus, car::recode) used instead.OA code, recoded 2011OAC supergroup name, newly created perc_age_20_to_24 retained new table leic_2011OAC_20to24. step sometimes useful stepping stone analysis can make code easier read line. Sometimes also necessary step interacting certain libraries, fully compatible Tidyverse libraries, leveneTest (function ).","code":"\nlibrary(tidyverse)\nlibrary(knitr)\n\nleicester_2011OAC <- read_csv(\"2011_OAC_Raw_uVariables_Leicester.csv\")\nlibrary(pastecs)\nlibrary(car)\nleic_2011OAC_20to24 <- leicester_2011OAC %>%\n  mutate(\n    perc_age_20_to_24 = (u011 / Total_Population) * 100,\n    supgrpname = \n      dplyr::recode(\n        supgrpname, \n          `Suburbanites` = \"SU\",\n          `Cosmopolitans` = \"CP\",\n          `Multicultural Metropolitans` = \"MM\",\n          `Ethnicity Central` = \"EC\",\n          `Constrained City Dwellers` = \"CD\",\n          `Hard-Pressed Living` = \"HP\",\n          `Urbanites` = \"UR\"\n    )\n  ) %>%\n  select(OA11CD, supgrpname, perc_age_20_to_24)\n\nleic_2011OAC_20to24 %>%\n  slice_head(n = 5) %>%\n  kable()"},{"path":"exploratory-statistics.html","id":"descriptive-statistics","chapter":"6 Exploratory statistics","heading":"6.1 Descriptive statistics","text":"first step statistical analysis modelling explore “shape” data involved, looking descriptive statistics variables involved. function stat.desc pastecs library provides three series descriptive statistics.base:\nnbr.val: overall number values dataset;\nnbr.null: number NULL values – NULL often returned expressions functions whose values undefined;\nnbr.na: number NAs – missing value indicator;\nnbr.val: overall number values dataset;nbr.null: number NULL values – NULL often returned expressions functions whose values undefined;nbr.na: number NAs – missing value indicator;desc:\nmin (see also min function): minimum value dataset;\nmax (see also max function): minimum value dataset;\nrange: difference min max (different range());\nsum (see also sum function): sum values dataset;\nmedian (see also median function): median, value separating higher half lower half values\nmean (see also mean function): arithmetic mean, sum number values NA;\nSE.mean: standard error mean – estimation variability mean calculated different samples data (see also central limit theorem);\nCI.mean.0.95: 95% confidence interval mean – indicates 95% probability actual mean within distance sample mean;\nvar: variance (\\(\\sigma^2\\)), quantifies amount variation average squared distances mean;\nstd.dev: standard deviation (\\(\\sigma\\)), quantifies amount variation square root variance;\ncoef.var: variation coefficient quantifies amount variation standard deviation divided mean;\nmin (see also min function): minimum value dataset;max (see also max function): minimum value dataset;range: difference min max (different range());sum (see also sum function): sum values dataset;median (see also median function): median, value separating higher half lower half valuesmean (see also mean function): arithmetic mean, sum number values NA;SE.mean: standard error mean – estimation variability mean calculated different samples data (see also central limit theorem);CI.mean.0.95: 95% confidence interval mean – indicates 95% probability actual mean within distance sample mean;var: variance (\\(\\sigma^2\\)), quantifies amount variation average squared distances mean;std.dev: standard deviation (\\(\\sigma\\)), quantifies amount variation square root variance;coef.var: variation coefficient quantifies amount variation standard deviation divided mean;norm (default FALSE, use norm = TRUE include output):\nskewness: skewness value indicates\npositive: distribution skewed towards left;\nnegative: distribution skewed towards right;\n\nkurtosis: kurtosis value indicates:\npositive: heavy-tailed distribution;\nnegative: flat distribution;\n\nskew.2SE kurt.2SE: skewness kurtosis divided 2 standard errors. greater 1, respective statistics significant (p < .05);\nnormtest.W: test statistics Shapiro–Wilk test normality;\nnormtest.p: significance Shapiro–Wilk test normality.\nskewness: skewness value indicates\npositive: distribution skewed towards left;\nnegative: distribution skewed towards right;\npositive: distribution skewed towards left;negative: distribution skewed towards right;kurtosis: kurtosis value indicates:\npositive: heavy-tailed distribution;\nnegative: flat distribution;\npositive: heavy-tailed distribution;negative: flat distribution;skew.2SE kurt.2SE: skewness kurtosis divided 2 standard errors. greater 1, respective statistics significant (p < .05);normtest.W: test statistics Shapiro–Wilk test normality;normtest.p: significance Shapiro–Wilk test normality.Shapiro–Wilk test compares distribution variable normal distribution mean standard deviation. null hypothesis Shapiro–Wilk test sample normally distributed, thus normtest.p lower 0.01 (.e., p < .01), test indicates distribution probably normal. threshold accept reject hypothesis arbitrary based conventions, p < .01 commonly accepted threshold, p < .05 relatively small data sample (e.g., 30 cases).next step thus apply stat.desc variable currently exploring (.e., perc_age_20_to_24), including norm section.table tells us 969 OA Leicester valid value variable perc_age_20_to_24, NULL NA value found. values vary 1% almost 61%, average value 11% population OA aged 20 24.short paragraph reporting values table, taking advantage two features RMarkdown. First, output stat.desc function snippet stored variable leic_2011OAC_20to24_stat_desc, valid variable rest document. Second, RMarkdown allows -line R snippets, can also refer variables defined snippet text. , source paragraph reads , -line R snipped opened single grave accent (.e., `) followed lowercase r closed another single grave accent.included code RMarkdown document, copy text verbatim RMarkdown document make sure understand code -line R snippets works.data described statistics presented table random sample population, 95% confidence interval CI.mean.0.95 indicate can 95% confident actual mean distribution somewhere 10.566 - 0.596 = 9.97% 10.566 + 0.596 = 11.162%.However, sample. Thus statistical interpretation valid, way sum values doesn’t make sense, sum series percentages.skew.2SE kurt.2SE greater 1, indicate skewness kurtosis values significant (p < .05). skewness positive, indicates distribution skewed towards left (low values). kurtosis positive, indicates distribution heavy-tailed.function skim library skimr can also used generate quick summary content dataset.Table 6.1: Data summaryVariable type: characterVariable type: numeric","code":"\nleic_2011OAC_20to24_stat_desc <- leic_2011OAC_20to24 %>%\n  select(perc_age_20_to_24) %>%\n  stat.desc(norm = TRUE)\n  \nleic_2011OAC_20to24_stat_desc %>%\n  kable(digits = 3)The table above tells us that all `r leic_2011OAC_20to24_stat_desc[\"nbr.val\",\n\"perc_age_20_to_24\"] %>% round(digits = 0)` OA in Leicester have a valid \nvalue for the variable `perc_age_20_to_24`, as no `r NULL` nor `r \nNA` value have been found.The values vary from about `r \nleic_2011OAC_20to24_stat_desc[\"min\", \"perc_age_20_to_24\"] %>% round(digits = \n0)`% to almost `r leic_2011OAC_20to24_stat_desc[\"max\", \n\"perc_age_20_to_24\"] %>% round(digits = 0)`%, with an average value of \n`r leic_2011OAC_20to24_stat_desc[\"mean\", \"perc_age_20_to_24\"] %>% \nround(digits = 0)`% of the population in an OA aged between 20 and 24. \nlibrary(skimr)\n\nleic_2011OAC_20to24 %>% \n  skim()"},{"path":"exploratory-statistics.html","id":"significance","chapter":"6 Exploratory statistics","heading":"6.2 Significance","text":"statistical tests based idea hypothesis testing:null hypothesis set;data fit statistical model;model assessed test statistic;significance probability obtaining test statistic value chance.threshold accept reject hypothesis arbitrary based conventions. 0.05 threshold (p < .05) quite common relatively small samples (e.g., dozens cases), strict 0.01 threshold (p < .01) commonly advised large samples (e.g., hundreds cases). However, Mingfeng Lin, Henry C. Lucas, Galit Shmueli18 advise thresholds might suitable working big data (e.g., tens thousands cases).","code":""},{"path":"exploratory-statistics.html","id":"shapirowilk-test","chapter":"6 Exploratory statistics","heading":"6.3 Shapiro–Wilk test","text":"perc_age_20_to_24 heavy-tailed distribution, skewed towards low values, surprising normtest.p value indicates Shapiro–Wilk test significant, indicates distribution normal.code present output shapiro.test function, present outcome Shapiro–Wilk test values provided input. output values values reported norm section stat.desc. Note shapiro.test function require argument numeric vector. Thus pull function must used extract perc_age_20_to_24 column leic_2011OAC_20to24 vector, whereas using select single column name argument produce output table single column.null hypothesis Shapiro–Wilk test sample normally distributed, p value lower 0.01 threshold (p < .01) indicates probability true low. , flipper length penguins Palmer Station dataset normally distributed.two code snippets can used visualise density-based histogram including shape normal distribution mean standard deviation, Q-Q plot, visually confirm fact perc_age_20_to_24 normally distributed.Q-Q plot R can created using variety functions. example , plot created using stat_qq stat_qq_line functions ggplot2 library. Note perc_age_20_to_24 variable mapped particular option aes sample.perc_age_20_to_24 normally distributed, dots Q-Q plot distributed straight line included plot.","code":"\nleic_2011OAC_20to24 %>%\n  pull(perc_age_20_to_24) %>%\n  shapiro.test()## \n##  Shapiro-Wilk normality test\n## \n## data:  .\n## W = 0.64491, p-value < 2.2e-16\nleic_2011OAC_20to24 %>%\n  ggplot(\n    aes(\n      x = perc_age_20_to_24\n    )\n  ) +\n  geom_histogram(\n    aes(\n      y =..density..\n    ),\n    binwidth = 5\n  ) + \n  stat_function(\n    fun = dnorm, \n    args = list(\n      mean = leic_2011OAC_20to24 %>% pull(perc_age_20_to_24) %>% mean(),\n      sd = leic_2011OAC_20to24 %>% pull(perc_age_20_to_24) %>% sd()\n    ),\n    colour = \"red\", size = 1\n  )\nleic_2011OAC_20to24 %>%\n  ggplot(\n    aes(\n      sample = perc_age_20_to_24\n    )\n  ) +\n  stat_qq() +\n  stat_qq_line()"},{"path":"exploratory-statistics.html","id":"exercise-202.1","chapter":"6 Exploratory statistics","heading":"6.4 Exercise 202.1","text":"Create new RMarkdown document, add code necessary recreate table leic_2011OAC_20to24 used example . Use code re-shape table leic_2011OAC_20to24 pivoting perc_age_20_to_24 column wider multiple columns using supgrpname new column names.manipulation creates one column per supergroup, containing perc_age_20_to_24 OA part supergroup, NA value OA part supergroup. transformation illustrated two tables . first shows extract original leic_2011OAC_20to24 dataset, followed wide version leic_2011OAC_20to24_supgrp.Question 202.1.1: code uses newly created leic_2011OAC_20to24_supgrp table calculate descriptive statistics calculated variable leic_2011OAC_20to24 supergroup. leic_2011OAC_20to24 normally distributed subgroups? yes, supergroups based values justify claim? (Write 200 words)Question 202.1.2: Write code necessary test normality leic_2011OAC_20to24 supergroups analysis conducted Question 202.1.1 indicated normal, using function shapiro.test, draw respective Q-Q plot.Question 202.1.3: Observe output Levene’s test executed . result tell variance perc_age_20_to_24 supergroups?Note leveneTest designed work Tidyverse approach. , code uses . argument placeholder specify input table leic_2011OAC_20to24 coming pipe used argument data parameter.Stefano De Sabbata – text licensed CC -SA 4.0, contains public sector information licensed Open Government Licence v3.0, code licensed GNU GPL v3.0.","code":"\nleic_2011OAC_20to24_supgrp <- leic_2011OAC_20to24 %>%\n  pivot_wider(\n    names_from = supgrpname,\n    values_from = perc_age_20_to_24\n  )\nleic_2011OAC_20to24 %>%\n  slice_min(OA11CD, n = 10) %>%\n  kable(digits = 3)\nleic_2011OAC_20to24_supgrp %>%\n  slice_min(OA11CD, n = 10) %>%\n  kable(digits = 3)\nleic_2011OAC_20to24_supgrp %>%\n  select(-OA11CD) %>%\n  stat.desc(norm = TRUE) %>%\n  kable(digits = 3)\nleic_2011OAC_20to24 %>% \n  leveneTest(\n    perc_age_20_to_24 ~ supgrpname, \n    data = .\n  )## Levene's Test for Homogeneity of Variance (center = median)\n##        Df F value    Pr(>F)    \n## group   6  62.011 < 2.2e-16 ***\n##       962                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"comparing-data.html","id":"comparing-data","chapter":"7 Comparing data","heading":"7 Comparing data","text":"Print chapterThe first part chapter guides ANOVA (analysis variance) using iris dataset, second part showcases correlation analysis using two variables dataset used create 2011 Output Area Classification (2011OAC). Create new R project chapter create new RMarkdown document replicate analysis document separate RMarkdown document work exercises.many functions used analyses part oldest libraries developed R, developed easily compatible Tidyverse %>% operator. Fortunately, magrittr library (loaded ) define %>% operator seen far, also exposition pipe operator %$%, exposes columns data.frame left operator expression right operator. , %$% allows refer column data.frame directly subsequent expression. , lines expose column Petal.Length data.frame iris pass mean function using different approaches, equivalent outcome.","code":"\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(knitr)\n# Classic R approach\nmean(iris$Petal.Length) ## [1] 3.758\n# Using %>% pipe\niris$Petal.Length %>% \n  mean()  ## [1] 3.758\n# Using %>% pipe and %$% exposition pipe\niris %$% \n  Petal.Length %>% \n  mean() ## [1] 3.758"},{"path":"comparing-data.html","id":"independent-t-test","chapter":"7 Comparing data","heading":"7.1 Independent T-test","text":"Independent T-test simple statistical test can used compare two independent groups based attribute. instance, might want compare versicolor virginica species iris based petal length, assess different. boxplot indicates two species seem rather different petal length, difference statistically significant? type question can answer using Independent T-test.Independent T-test can seen special version general linear model. general linear model, observation \\(\\) can predicted \\(model\\) (predictors) allow amount error.\\[outcome_i = (model) + error_i \\]Independent T-test, predictor group average outcome, equal outcome allow amount error.\\[outcome_i = (group\\ mean) + error_i \\]Calculating average per group can seen extremely simplified version machine learning. observed values used “learn” model, case average. “model” simple table average group. , can “learn” model petal length versicolor virginica iris, shown . want “predict” petal length new flower, can simply look species return related average species. instance, “prediction” new versicolor iris 4.26.simplify, Independent Test crates model estimates averages compare standard deviations two groups able say whether two groups different. However, analysis meaningful, means standard deviations must representative distributions. , values two groups normally distributed. can test two Shapiro–Wilk tests. shown , tests significant, indicates groups normally distributed values.can thus run Independent test. 50 flowers species safer set significance threshold 0.01. result object contains series values can interest understand model. case, focus p-value, case estimated p-value < 2.2e-16. p-value lower 0.01, test significant. Thus, group means different.can report result Independent test :t(95.57) = -12.6, p < 0.01The Independent T-test result object t_test__ver_and_vir also includes standard error, used measure quality (.e., fitness) model compared data created (.e., fitted “learn”).calculations standard error bit complicated, relate another extremely common measure, residual (also known , error), difference observed value model. Throughout statistics, machine learning artificial intelligence, common use Sum Squares Residuals (SSR) Mean Squares Error (MSE) measures model quality.simple model one used Independent T-test, can quickly calculate two values follows. Join table table containing averages calculated . Calculate residuals (also known , errors) difference observed value average, well squares values. Finally, calculate sum average aggregation.","code":"\nversicolor_and_virginica <-\n  iris %>% \n  filter(\n    Species == \"versicolor\" | Species == \"virginica\"\n  ) %>% \n  select(Species, Petal.Length)\n\nversicolor_and_virginica %>%\n  ggplot(\n    aes(\n      x = Species, \n      y = Petal.Length\n    )\n  ) +\n  geom_boxplot() +\n  theme_bw()\navg_petal_length_ver_and_vir <-\n  versicolor_and_virginica %>% \n  group_by(Species) %>% \n  summarise(\n    avg_petal_length = mean(Petal.Length)\n  )\n\navg_petal_length_ver_and_vir %>% \n  kable()\nversicolor_and_virginica %>% \n  filter(Species == \"versicolor\") %>% \n  pull(Petal.Length) %>% \n  shapiro.test()## \n##  Shapiro-Wilk normality test\n## \n## data:  .\n## W = 0.966, p-value = 0.1585\nversicolor_and_virginica %>% \n  filter(Species == \"virginica\") %>% \n  pull(Petal.Length) %>% \n  shapiro.test()## \n##  Shapiro-Wilk normality test\n## \n## data:  .\n## W = 0.96219, p-value = 0.1098\nt_test__ver_and_vir <-\n  versicolor_and_virginica %$%\n  t.test(Petal.Length ~ Species)\nt_test__ver_and_vir## \n##  Welch Two Sample t-test\n## \n## data:  Petal.Length by Species\n## t = -12.604, df = 95.57, p-value < 2.2e-16\n## alternative hypothesis: true difference in means between group versicolor and group virginica is not equal to 0\n## 95 percent confidence interval:\n##  -1.49549 -1.08851\n## sample estimates:\n## mean in group versicolor  mean in group virginica \n##                    4.260                    5.552\nt_test__ver_and_vir %$%\n  stderr## [1] 0.1025089\n# Join and calculate the residuals\nresiduals_ver_and_vir <-\n  versicolor_and_virginica %>% \n  left_join(avg_petal_length_ver_and_vir) %>% \n  mutate(\n    residual = Petal.Length - avg_petal_length\n  ) %>% \n  mutate(\n    sq_residual = residual^2\n  )## Joining with `by = join_by(Species)`\n# A quick look at the table created above\nresiduals_ver_and_vir %>% \n  group_by(Species) %>% \n  slice_head(n=3) %>% \n  kable()\nresiduals_ver_and_vir %>% \n  summarise(\n    ssr = sum(sq_residual),\n    mse = mean(sq_residual)\n  ) %>% \n  kable()"},{"path":"comparing-data.html","id":"anova","chapter":"7 Comparing data","heading":"7.2 ANOVA","text":"ANOVA (analysis variance) tests whether values variable (e.g., length petal) average different multiple different groups (e.g., different species iris). ANOVA developed generalised version Independent T-test, objective allows test two groups.ANOVA test following assumptions:normally distributed values groups\nespecially groups different sizes\nespecially groups different sizeshomogeneity variance values groups\ngroups different sizes\ngroups different sizesindependence groups","code":""},{"path":"comparing-data.html","id":"anova-example","chapter":"7 Comparing data","heading":"7.2.1 ANOVA example","text":"example seen lecture illustrates ANOVA can used verify three different species iris iris dataset different petal length.groups size, need test homogeneity variance. Furthermore, groups come different species flowers, need test independence values. assumption needs testing whether values three groups normally distributed. 50 flowers per species, can set significance threshold 0.01.already run two Shapiro–Wilk tests Independent T-test . Shapiro–Wilk test iris species setosa also significant, indicates three groups normally distributed values.can thus conduct ANOVA test using function aov, function summary obtain summary results test.difference significant F(2, 147) = 1180.16, p < .01. Also, note output contains row Residuals related value column Sum Sq, SSR value simple model used calculate ANOVA statistics.image highlights important values output: significance value Pr(>F); F-statistic value F value; two degrees freedom values F-statistic Df column.","code":"\niris %>%\n  ggplot(\n    aes(\n      x = Species, \n      y = Petal.Length\n    )\n  ) +\n  geom_boxplot() +\n  theme_bw()\niris %>% \n  filter(Species == \"setosa\") %>% \n  pull(Petal.Length) %>% \n  shapiro.test()## \n##  Shapiro-Wilk normality test\n## \n## data:  .\n## W = 0.95498, p-value = 0.05481\n# Classic R coding approach (not using %$%)\n# iris_anova <- aov(Petal.Length ~ Species, data = iris)\n# summary(iris_anova)\n\niris %$%\n  aov(Petal.Length ~ Species) %>%\n  summary()##              Df Sum Sq Mean Sq F value Pr(>F)    \n## Species       2  437.1  218.55    1180 <2e-16 ***\n## Residuals   147   27.2    0.19                   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"comparing-data.html","id":"correlation","chapter":"7 Comparing data","heading":"7.3 Correlation","text":"term correlation used refer series standardised measures covariance, can used statistically assess whether two variables related .Furthermore, two variables related, measures can identify whether :positively related:\nentities high values one tend high values ;\nentities low values one tend low values ;\nentities high values one tend high values ;entities low values one tend low values ;negatively:\nentities high values one tend low values ;\nentities low values one tend high values .\nentities high values one tend low values ;entities low values one tend high values .Correlation can calculated many ways, three approaches far common. start null hypothesis relationship variables. Thus, p-value pre-defined significance threshold, null hypothesis rejected, conclusion relationship two variables.test significant case:positive correlation value indicates positive relationship;negative correlation value indicates negative relationship;square correlation value can taken indication percentage shared variance two variables.However, one different assumptions variables’ distribution thus implements general ideal measure different way:two variables normally distributed:\nPearson’s r;\nPearson’s r;two variables normally distributed:\nties among values:\nSpearman’s rho;\n\nties among values:\nKendall’s tau.\n\nties among values:\nSpearman’s rho;\nSpearman’s rho;ties among values:\nKendall’s tau.\nKendall’s tau.","code":""},{"path":"comparing-data.html","id":"correlation-analysis-example","chapter":"7 Comparing data","heading":"7.3.1 Correlation analysis example","text":"studying people live cities, number questions might arise live move around city. instance, looking map Leicester, clear (many English cities) seems high concentration flats city centre. time, seems almost flats suburbs. might lead us ask: “households living flats (thus mostly city centre) amount cars households living city centre?”due many reasons. suburbs England largely residential, whereas working places located city centre. , people living flats might likely walk cycle work commute using public transportation within city cities. City centres usually afford fewer spaces parking. Many flats rented students, might less likely car. list continue, still hypotheses based certain (probably biased) view city. Can use data analysis explore whether ground hypothesis?map demonstrative. next semester’s module GY7707 Geospatial Data Analytics cover spatial data handling, analysis mapping R detail. want replicate map , need download \nCensus Residential Data Pack 2011 Leicester Consumer Data Research Centre (requires set account) use code unzipped downloaded file.dataset used create 2011 Output Area Classification (2011OAC) contains two variables might help explore issue. data current anymore, values might collect conduct fresh survey specific study. However, can still provide insight.u089: count flats per Output Area (OA). statistical unit variable Household_Spaces. OAs vary size composition, can use Total_Household_Spaces calculate percentage flats per OA, stable measure.\nperc_flats = (u089 / Total_Household_Spaces) * 100\nperc_flats = (u089 / Total_Household_Spaces) * 100u118: 2 cars vans household. statistical unit variable Household. OAs vary size composition, can use Total_Households calculate percentage households per OA 2 cars vans, stable measure.\nperc_2ormore_cars = (u118 / Total_Households) * 100\nperc_2ormore_cars = (u118 / Total_Households) * 100The process transforming variables within certain range (percentage, thus using [0..100] range, [0..1] range) commonly referred normalisation. process transforming variable mean zero standard deviation one (z-scores) commonly referred standardisation. However, note terms sometime used interchangeably.Plotting two variables together scatterplot reveals pattern. Indeed, low percentage households living flats two cars. However, proportion households owning two cars live suburbs seem span almost throughout whole range, zero 80%. seems indicate level negative relationship, picture clearly far less clear-cut might initially assumed. initial assumption car ownership households living flats seems hold, probably didn’t consider situation suburbs sufficient care.first step establishing whether relationship two variables assess whether normally distributed, thus, correlation test use analysis. scatterplot already seem suggests variables rather skewed.969 OAs Leicester, can set significance threshold 0.01. results shapiro.test functions show neither two variables normally distributed. Transforming variables using inverse hyperbolic sine still result normally distributed variables. Thus, discard Pearson’s r option explore correlation two variables.next step assess whether ties among values two variables. code fist counts number cases per value. counts number values number cases greater one.variable perc_flats 127 values ties perc_2ormore_cars 115 values ties. , using Spearman’s rho advisable Kendall’s tau used. , can set significance threshold 0.01.Finally, can run cor.test function assess relationship two variables. code saves results test variable. allows two subsequent actions. First, can show full results simply invoking name variable (term used programming-related meaning ) final line code. Second, can extract square estimated value RMarkdwon following paragraph, show percentage shared variance.percentage flats percentage households owning 2 cars vans per OA city Leicester negatively related, relationship significant (p-value < 0.01) correlation value negative (tau = -0.41). two variables share 16.8% variance. can thus conclude significant weak relationship two variables.","code":"\n# Read the Leicester 2011 OAC dataset from the csv file\nleicester_2011OAC <- \n  read_csv(\"2011_OAC_Raw_uVariables_Leicester.csv\")\n\n# Load the shapefile data\n# using the library sf\n# https://r-spatial.github.io/sf/\nlibrary(sf)\nleic_2011OAC_shp <- \n  read_sf(\"data/Census_Residential_Data_Pack_2011/Local_Authority_Districts/E06000016/shapefiles/E06000016.shp\")\n\n# Create the map \n# using the library tmap\n# https://r-tmap.github.io/tmap/\nlibrary(tmap)\nleic_2011OAC_shp %>%\n  # Join shapefile and 2011 OAC data\n  left_join(\n    leicester_2011OAC,\n    by = c(\"oa_code\" = \"OA11CD\")\n  ) %>%\n  # Calculate percentages\n  mutate(perc_flats = (u089/Total_Dwellings)*100) %>%\n  # Create the map\n  tm_shape() +\n  # Define the choropleth aesthetic\n  tm_polygons(\n    \"perc_flats\",\n    title = \"Percentage\\nof flats\",\n    palette = \"viridis\",\n    legend.show = TRUE,\n    border.alpha = 0\n  ) +\n  # Define the layout\n  tm_layout(\n    frame = FALSE,\n    legend.title.size=1,\n    legend.text.size = 0.5,\n    legend.position = c(\"left\",\"bottom\")\n  ) +\n  # Don't forget the appropriate attribution\n  tm_credits(\n    \"Source: CDRC 2011 OAC Geodata Pack by the ESRC Consumer\\nDataResearch Centre; Contains National Statistics data Crown\\ncopyright and database right 2015; Contains Ordnance Survey\\ndata Crown copyright and database right 2015\",\n    size = 0.3,\n    position = c(\"right\", \"bottom\")\n  )\nflats_and_cars <-\n  leicester_2011OAC %>%\n  mutate(\n    perc_flats = (u089 / Total_Household_Spaces) * 100,\n    perc_2ormore_cars = (u118 / Total_Households) * 100\n  ) %>%\n  select(\n    OA11CD, supgrpname, supgrpcode,\n    perc_flats, perc_2ormore_cars\n  )\nlibrary(pastecs)\n\nflats_and_cars %>%\n  select(perc_flats, perc_2ormore_cars) %>%\n  mutate(\n    ihs_perc_flats = asinh(perc_flats),\n    ihs_perc_2omcars = asinh(perc_2ormore_cars)\n  ) %>%\n  stat.desc(basic = FALSE, desc = FALSE, norm = TRUE) %>%\n  kable()\nties_perc_flats <-\n  flats_and_cars %>%\n  count(perc_flats) %>%\n  filter(n > 1) %>% \n  # Specify wt = n() to count rows\n  # otherwise n is taken as weight\n  count(wt = n()) %>%\n  pull(n)\n\nties_perc_2ormore_cars <-\n  flats_and_cars %>%\n  count(perc_2ormore_cars) %>%\n  filter(n > 1) %>% \n  # Specify wt = n() to count rows\n  # otherwise n is taken as weight\n  count(wt = n()) %>%\n  pull(n)\nflats_and_cars_corKendall <-\n  flats_and_cars %$%\n  cor.test(\n    perc_flats, perc_2ormore_cars, \n    method = \"kendall\"\n  )\n\nflats_and_cars_corKendall## \n##  Kendall's rank correlation tau\n## \n## data:  perc_flats and perc_2ormore_cars\n## z = -19.026, p-value < 2.2e-16\n## alternative hypothesis: true tau is not equal to 0\n## sample estimates:\n##        tau \n## -0.4094335The percentage of flats and the percentage of households \nowning 2 or more cars or vans per OA in the city of Leicester \nare negatively related, as the relationship is significant \n(`p-value < 0.01`) and the correlation value is negative \n(`tau =` -0.41). The two variables share 16.8% of variance. We can thus conclude \nthat there is a significant but very weak relationship \nbetween the two variables."},{"path":"comparing-data.html","id":"exercise-203.2","chapter":"7 Comparing data","heading":"7.4 Exercise 203.2","text":"See group work exercise, Introduction Part 1.Stefano De Sabbata – text licensed CC -SA 4.0, contains public sector information licensed Open Government Licence v3.0, code licensed GNU GPL v3.0.","code":""},{"path":"regression-analysis.html","id":"regression-analysis","chapter":"8 Regression analysis","heading":"8 Regression analysis","text":"Print chapterThis chapter focuses regression analysis, starting simple regression multiple regression analysis. sections discuss run regression analysis (importantly) check analysis meets test assumptions report results analysis document.","code":"\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(knitr)"},{"path":"regression-analysis.html","id":"simple-regression","chapter":"8 Regression analysis","heading":"8.1 Simple regression","text":"simple regression analysis supervised machine learning approach creating model able predict value one outcome variable \\(Y\\) based one predictor variable \\(X_1\\), estimating intercept \\(b_0\\) coefficient (slope) \\(b_1\\), accounting reasonable amount error \\(\\epsilon\\).\\[Y_i = (b_0 + b_1 * X_{i1}) + \\epsilon_i \\]Least squares commonly used approach generate regression model. model fits line minimise squared values residuals (errors), calculated squared difference observed values values predicted model.\\[redidual = \\sum(observed - model)^2\\]model considered robust residuals show particular trends, indicate “something” interfering model. particular, assumption regression model :linearity: relationship actually linear;normality residuals: standard residuals normally distributed mean 0;homoscedasticity residuals: level predictor variable(s) variance standard residuals (homo-scedasticity) rather different (hetero-scedasticity);independence residuals: adjacent standard residuals correlated.","code":""},{"path":"regression-analysis.html","id":"simple-regression-example","chapter":"8 Regression analysis","heading":"8.1.1 Simple regression example","text":"example seen lecture illustrated simple regression can used create model predict arrival delay based departure delay flight, based data available nycflights13 dataset flight November 20th, 2013.\\[arr\\_delay_i = (Intercept + Coefficient_{dep\\_delay} * dep\\_delay_{i1}) + \\epsilon_i \\]last line arranges table arrival delay. advisable give meaningful order data (e.g., based outcome) creating model, order facilitate robust execution Durbin-Watson test (see ).scatterplot seems indicate relationship indeed linear.code generates model using function lm, function summary obtain summary results test. model summary saved variables delay_model delay_model_summary, respectively, use . variable delay_model_summary can called directly visualise result test.image highlights important values output: adjusted \\(R^2\\) value; model significance value p-value related F-statistic information F-statistic; intercept dep_delay coefficient estimates Estimate column related significance values column Pr(>|t|).output indicates:p-value: < 2.2e-16: \\(p<.001\\) model significant;\nderived comparing calulated F-statistic value F distribution 3396.74 specified degrees freedom (1, 972);\nReport : \\(F(1, 972) = 3396.74\\)\nderived comparing calulated F-statistic value F distribution 3396.74 specified degrees freedom (1, 972);Report : \\(F(1, 972) = 3396.74\\)Adjusted R-squared: 0.7773: departure delay can account 77.73% arrival delay;Coefficients:\nIntercept estimate -4.9672 significant;\ndep_delay coefficient (slope) estimate 1.0423 significant.\nIntercept estimate -4.9672 significant;dep_delay coefficient (slope) estimate 1.0423 significant.","code":"\n# Load the library\nlibrary(nycflights13)\n\n# November 20th, 2013\nflights_nov_20 <- flights %>%\n  filter(\n    !is.na(dep_delay) &\n    !is.na(arr_delay) &\n    month == 11 &\n    day ==20\n  ) %>% \n  arrange(dep_time)\n# Classic R coding version\n# delay_model <- lm(arr_delay ~ dep_delay, data = flights_nov_20)\n# delay_model_summary <- summary(delay_model)\n\n# Load magrittr library to use %$%\nlibrary(magrittr)\n\ndelay_model <- flights_nov_20 %$%\n  lm(arr_delay ~ dep_delay) \n\ndelay_model_summary <- delay_model %>%\n  summary()\n\ndelay_model_summary## \n## Call:\n## lm(formula = arr_delay ~ dep_delay)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -43.906  -9.022  -1.758   8.678  57.052 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -4.96717    0.43748  -11.35   <2e-16 ***\n## dep_delay    1.04229    0.01788   58.28   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 13.62 on 972 degrees of freedom\n## Multiple R-squared:  0.7775, Adjusted R-squared:  0.7773 \n## F-statistic:  3397 on 1 and 972 DF,  p-value: < 2.2e-16\nflights_nov_20 %>%\n  ggplot(aes(x = dep_delay, y = arr_delay)) +\n  geom_point() + coord_fixed(ratio = 1) +\n  geom_abline(intercept = 4.0943, slope = 1.04229, color=\"red\")"},{"path":"regression-analysis.html","id":"checking-regression-assumptions","chapter":"8 Regression analysis","heading":"8.1.2 Checking regression assumptions","text":"","code":""},{"path":"regression-analysis.html","id":"normality","chapter":"8 Regression analysis","heading":"8.1.2.1 Normality","text":"Shapiro-Wilk test can used check normality standard residuals. test significant robust models. example , standard residuals normally distributed. However, plot show distribution residuals far away normal distribution.","code":"\ndelay_model %>% \n  rstandard() %>% \n  shapiro.test()## \n##  Shapiro-Wilk normality test\n## \n## data:  .\n## W = 0.98231, p-value = 1.73e-09"},{"path":"regression-analysis.html","id":"homoscedasticity","chapter":"8 Regression analysis","heading":"8.1.2.2 Homoscedasticity","text":"Breusch-Pagan test can used check homoscedasticity standard residuals. test significant robust models. example , standard residuals homoscedastic.","code":"\nlibrary(lmtest)\n\ndelay_model %>% \n  bptest()## \n##  studentized Breusch-Pagan test\n## \n## data:  .\n## BP = 0.017316, df = 1, p-value = 0.8953"},{"path":"regression-analysis.html","id":"independence","chapter":"8 Regression analysis","heading":"8.1.2.3 Independence","text":"Durbin-Watson test can used check independence residuals. test statistic close 2 (1 3) significant robust models. example , standard residuals might completely independent. Note, however, result depends order data.idea autocorrelation residuals tested Durbin-Watson test can illustrated using lag plot , standard residual case compared standard residual previous case table. clear patter plot indicate residuals independent. plot , clear pattern visible, thus reinforcing results test .","code":"\n# Also part of the library lmtest\ndelay_model %>%\n  dwtest()## \n##  Durbin-Watson test\n## \n## data:  .\n## DW = 1.8731, p-value = 0.02358\n## alternative hypothesis: true autocorrelation is greater than 0\ndelay_model %>%\n  rstandard() %>% \n  lag.plot()"},{"path":"regression-analysis.html","id":"plots","chapter":"8 Regression analysis","heading":"8.1.2.4 Plots","text":"plot.lm function can used explore residuals visuallly. Usage illustrated . Residuals vs Fitted Scale-Location plot provide insight homoscedasticity residuals, Normal Q-Q plot provides illustration normality residuals, Residuals vs Leverage can useful identify exceptional cases (e.g., Cook’s distance greater 1).","code":"\ndelay_model %>%\n  plot(which = c(1))\ndelay_model %>%\n  plot(which = c(2))\ndelay_model %>%\n  plot(which = c(3))\ndelay_model %>%\n  plot(which = c(5))"},{"path":"regression-analysis.html","id":"how-to-report-a-simple-regression","chapter":"8 Regression analysis","heading":"8.1.3 How to report a simple regression","text":"Overall, can say delay model computed fit (\\(F(1, 972) = 3396.74\\), \\(p < .001\\)), indicating departure delay might account 77.73% arrival delay. However model partially robust. residuals satisfy homoscedasticity assumption (Breusch-Pagan test, \\(BP = 0.02\\), \\(p =0.9\\)), independence assumption (Durbin-Watson test, \\(DW = 1.87\\), \\(p =0.02\\)), normally distributed (Shapiro-Wilk test, \\(W = 0.98\\), \\(p < .001\\)).stargazer function stargazer library can applied model delay_model generate nicer output RMarkdown PDF documents including results = \"asis\" R snippet option.","code":"\n# Install stargazer if not yet installed\n# install.packages(\"stargazer\")\n\nlibrary(stargazer)\n\n# Not rendered in bookdown\nstargazer(delay_model, header = FALSE)"},{"path":"regression-analysis.html","id":"exercise-205","chapter":"8 Regression analysis","heading":"8.2 Exercise 205","text":"See group work exercise, Part 2.Stefano De Sabbata – text licensed CC -SA 4.0, contains public sector information licensed Open Government Licence v3.0, code licensed GNU GPL v3.0.","code":""},{"path":"multiple-regression.html","id":"multiple-regression","chapter":"9 Multiple regression","heading":"9 Multiple regression","text":"Print chapterThis chapter focuses regression analysis, starting simple regression multiple regression analysis. sections discuss run regression analysis (importantly) check analysis meets test assumptions report results analysis document.multiple regression analysis supervised machine learning approach creating model able predict value one outcome variable \\(Y\\) based two predictor variables \\(X_1 \\dots X_M\\), estimating intercept \\(b_0\\) coefficients (slopes) \\(b_1 \\dots b_M\\), accounting reasonable amount error \\(\\epsilon\\).\\[Y_i = (b_0 + b_1 * X_{i1} + b_2 * X_{i2} + \\dots + b_M * X_{iM}) + \\epsilon_i \\]assumptions simple regression, plus assumption multicollinearity: two predictor variables used model, pair variables correlated. assumption can tested checking variance inflation factor (VIF). largest VIF value greater 10 average VIF substantially greater 1, might issue multicollinearity.","code":"\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(knitr)\nlibrary(stargazer)\nlibrary(lmtest)"},{"path":"multiple-regression.html","id":"multiple-regression-example","chapter":"9 Multiple regression","heading":"9.0.1 Multiple regression example","text":"example explores whether regression model can created estimate number people Leicester commuting work using private transport (u121 2011 Output Area Classification dataset seen previous chapters) Leicester, using number people different industry sectors predictors.instance, occupations electricity, gas, steam air conditioning supply (u144) require travel distances equipment, thus related variable u144 included model, whereas people working information communication might likely work home commute public transport.Let’s observe variable relate one another using pairs plot.Based plot understanding variables, can try create model able relate estimate dependent (output) variable perc_u120 (Method Travel Work, Private Transport) independent (predictor) variables:perc_u142: Industry Sector, Mining quarryingperc_u144: Industry Sector, Electricity, gas, steam air conditioning …perc_u146: Industry Sector, Constructionperc_u149: Industry Sector, Accommodation food service activitiesA multiple regression model can specified similar way simple regression model, using lm function, adding additional predictor variables using + operator.output suggests model fit (\\(F(4, 964) = 150.62\\), \\(p < .001\\)), indicating model based presence people working four selected industry sectors can account 38.21% number people using private transportation commute work. However model partially robust. residuals normally distributed (Shapiro-Wilk test, \\(W = 1\\), \\(p =0.83\\)) seems multicollinearity average VIF \\(1.02\\), residuals don’t satisfy homoscedasticity assumption (Breusch-Pagan test, \\(BP = 28.4\\), \\(p < .001\\)), independence assumption (Durbin-Watson test, \\(DW = 0.73\\), \\(p < .01\\)).coefficient values calculated lm functions important create model, provide useful information. instance, coefficient variable perc_u144 1.169, indicates presence people working electricity, gas, steam air conditioning supply increases one percentage point, number people using private transportation commute work increases 1.169 percentage points, according model. coefficients also indicate presence people working accommodation food service activities actually negative impact (context variables selected model) number people using private transportation commute work.example, variables use unit similar type, makes interpreting model relatively simple. case, can useful look standardized \\(\\beta\\), provide information measured terms standard deviation, make comparisons variables different types easier draw. instance, values calculated using function lm.beta library lm.beta indicate presence people working construction highest impact outcome varaible. presence people working construction increases one standard deviation, number people using private transportation commute work increases 0.29 standard deviations, according model.","code":"\nleicester_2011OAC <- read_csv(\"2011_OAC_Raw_uVariables_Leicester.csv\")\n# Select and \n# normalise variables\nleicester_2011OAC_transp <-\n  leicester_2011OAC %>%\n  select(\n    OA11CD, \n    Total_Pop_No_NI_Students_16_to_74, Total_Employment_16_to_74, \n    u121, u141:u158\n  ) %>%\n  # percentage method of travel\n  mutate(\n    u121 = (u121 / Total_Pop_No_NI_Students_16_to_74) * 100\n  ) %>%\n  # percentage across industry sector columns\n  mutate(\n    across( \n      u141:u158,\n      function(x){ (x / Total_Employment_16_to_74) * 100 }\n    )\n  ) %>%\n  # rename columns\n  rename_with(\n    function(x){ paste0(\"perc_\", x) },\n    c(u121, u141:u158)\n  ) %>% \n  arrange(perc_u121)\nlibrary(GGally)\n\nleicester_2011OAC_transp %>%\n  select(perc_u121, perc_u141:perc_u158) %>%\n  ggpairs(\n    upper = list(continuous = \n        wrap(ggally_cor, method = \"kendall\")),\n    lower = list(continuous = \n        wrap(\"points\", alpha = 0.3, size=0.1))\n  ) +\n  theme_bw()\n# Create model\ncommuting_model1 <- \n  leicester_2011OAC_transp %$%\n  lm(\n    perc_u121 ~ \n      perc_u142 + perc_u144 + perc_u146 + perc_u149\n  )\n\n# Print summary\ncommuting_model1 %>%\n  summary()## \n## Call:\n## lm(formula = perc_u121 ~ perc_u142 + perc_u144 + perc_u146 + \n##     perc_u149)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -35.315  -6.598  -0.244   6.439  31.472 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 37.12690    0.94148  39.434  < 2e-16 ***\n## perc_u142    3.74768    1.21255   3.091  0.00205 ** \n## perc_u144    1.16865    0.25328   4.614 4.48e-06 ***\n## perc_u146    1.05408    0.09335  11.291  < 2e-16 ***\n## perc_u149   -1.56948    0.08435 -18.606  < 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 9.481 on 964 degrees of freedom\n## Multiple R-squared:  0.3846, Adjusted R-squared:  0.3821 \n## F-statistic: 150.6 on 4 and 964 DF,  p-value: < 2.2e-16\n# Not rendered in bookdown\nstargazer(commuting_model1, header=FALSE)\ncommuting_model1 %>%\n  rstandard() %>% \n  shapiro.test()## \n##  Shapiro-Wilk normality test\n## \n## data:  .\n## W = 0.99889, p-value = 0.8307\ncommuting_model1 %>% \n  bptest()## \n##  studentized Breusch-Pagan test\n## \n## data:  .\n## BP = 28.403, df = 4, p-value = 1.033e-05\ncommuting_model1 %>%\n  dwtest()## \n##  Durbin-Watson test\n## \n## data:  .\n## DW = 0.72548, p-value < 2.2e-16\n## alternative hypothesis: true autocorrelation is greater than 0\nlibrary(car)\n\ncommuting_model1 %>%\n  vif()## perc_u142 perc_u144 perc_u146 perc_u149 \n##  1.006906  1.016578  1.037422  1.035663\n# Install lm.beta library if necessary\n# install.packages(\"lm.beta\")\nlibrary(lm.beta)\n\nlm.beta(commuting_model1)## \n## Call:\n## lm(formula = perc_u121 ~ perc_u142 + perc_u144 + perc_u146 + \n##     perc_u149)\n## \n## Standardized Coefficients::\n## (Intercept)   perc_u142   perc_u144   perc_u146   perc_u149 \n##          NA  0.07836017  0.11754058  0.29057993 -0.47841083"},{"path":"multiple-regression.html","id":"exercise-204.2","chapter":"9 Multiple regression","heading":"9.1 Exercise 204.2","text":"See group work exercise, Part 3.Stefano De Sabbata – text licensed CC -SA 4.0, contains public sector information licensed Open Government Licence v3.0, code licensed GNU GPL v3.0.","code":""},{"path":"supervised-machine-learning.html","id":"supervised-machine-learning","chapter":"10 Supervised machine learning","heading":"10 Supervised machine learning","text":"Print chapterThe field machine learning sits intersection computer science statistics, core component data science. According Mitchell (1997), “field machine learning concerned question construct computer programs automatically improve experience.”Machine learning approaches divided two main types19.Supervised:\ntraining “predictive” model data;\none () attribute dataset used “predict” another attribute.\ntraining “predictive” model data;one () attribute dataset used “predict” another attribute.Unsupervised:\ndiscovery descriptive patterns data;\ncommonly used data mining.\ndiscovery descriptive patterns data;commonly used data mining.Classification one classic supervised machine learning tasks, algorithms used learn (.e., model) relationship series input values (.k.. predictors, independent variables) output categorical values labels (.k.. outcome, dependent variable). model trained training dataset can learn relationship input labels, used label new, unlabeled data.chapters explores three approaches supervised machine learning approaches classification:logistic regression;support vector machines;artificial neural networks.","code":""},{"path":"supervised-machine-learning.html","id":"confusion-matrices","chapter":"10 Supervised machine learning","heading":"10.1 Confusion matrices","text":"classification model created, next step validation. latter can involve different approaches procedures, one common simple approaches split data training testing set. model trained training set validated using testing set. sets contain input values (predictors) output values (outcome).model trained using training set can used predict values testing set. outcome prediction can compared actual categories testing dataset. confusion matrix representation correspondence actual values predicted values testing dataset, including:true positive: correctly classified first (positive) class;true negative: correctly classified second (negative) class;false positive: incorrectly classified first (positive) class;false negative: incorrectly classified second (negative) class.number true false positive negatives used calculate number performance measures. simplest measures performance accuracy error rate.\\[\naccuracy = \\frac{true\\ positive + true\\ negative}{total\\ number\\ \\ cases}\n\\]\n\\[\nerror\\ rate = 1 - accuracy\n\\]also number additional measures can provide insight quality prediction, sensitivity (true positive rate) specificity (true negative rate). model created predict binary categorical variable, based definition (first category positive, second category negative), sensitivity measure quality predicting first category, specificity measure quality predicting second category.\\[\nsensitivity = \\frac{true\\ positive}{true\\ positive + false\\ negative} = \\frac{correct\\ 1st}{\\ 1st}\n\\]\\[\nspecificity = \\frac{true\\ negative}{true\\ negative + false\\ positive} = \\frac{correct\\ 2nd}{\\ 2nd}\n\\]Two , similar measures precision recall. model high precision model can trusted make correct prediction identifying observation part first category. formula recall used sensitivity, case, different interpretation, derived computer science literature search engines, model high recall able correctly retrieve items specified category. Note precision recall dependent one two categories defined first.\\[\nprecision = \\frac{true\\ positive}{true\\ positive + false\\ positive} = \\frac{correct\\ 1st}{predicted\\ \\ 1st}\n\\]\\[\nrecall = \\frac{true\\ positive}{true\\ positive + false\\ negative} = \\frac{correct\\ 1st}{\\ 1st}\n\\]\nPrecision recall can also combined single measure performance called F-score (.k.., F-measure F1).\\[\nF-score = \\frac{2 \\times precision \\times recall}{precision + recall}\n\\]\nFinally, kappa statistic (common Cohen’s kappa) additional measure accuracy, measures agreement prediction actual values, also accounting probability correct prediction chance.","code":""},{"path":"supervised-machine-learning.html","id":"urban-and-rural-population-density","chapter":"10 Supervised machine learning","heading":"10.2 Urban and rural population density","text":"two examples explore relation variables United Kingdom 2011 Census included among 167 initial variables used create 2011 Output Area Classification (Gale et al., 2016) Rural Urban Classification (2011) Output Areas England Wales created Office National Statistics. various examples models explore whether possible learn rural-urban distinction using census variables, Local Authority Districts (LADs) Leicestershire (excluding city Leicester .code uses libraries caret, e1071 neuralnet. Please install continuing.examples use data seen previous chapters, 7 LADs Leicestershire outside boundaries city Leicester: Blaby, Charnwood, Harborough, Hinckley Bosworth, Melton, North West Leicestershire, Oadby Wigston. data loaded 2011_OAC_Raw_uVariables_Leicestershire.csv. second part code extracts data Rural Urban Classification (2011) compressed file RUC11_OA11_EW.zip, loads extracted data finally deletes .can join two datasets create simplified, binary rural - urban classification, used examples .","code":"\ninstall.packages(\"caret\")\ninstall.packages(\"e1071\")\ninstall.packages(\"neuralnet\")\n# Libraries\nlibrary(tidyverse)\nlibrary(magrittr)\n\n# 2011 OAC data for Leicestershire (excl. Leicester)\nliec_shire_2011OAC <- readr::read_csv(\"2011_OAC_Raw_uVariables_Leicestershire.csv\")\n\n# Rural Urban Classification (2011)\n# >>> Note that if you upload the file to RStudio Server\n# >>> the file will be automatically unzipped\n# >>> thus the unzip and unlink instrcutions are not necessary\nunzip(\"RUC11_OA11_EW.zip\")\nru_class_2011 <- readr::read_csv(\"RUC11_OA11_EW.csv\")\nunlink(\"RUC11_OA11_EW.csv\")\nliec_shire_2011OAC_RU <-\n  liec_shire_2011OAC %>%\n  dplyr::left_join(ru_class_2011) %>%\n  dplyr::mutate(\n    rural_urban = \n      forcats::fct_recode(\n        RUC11CD,\n        urban = \"C1\",\n        rural = \"D1\",\n        rural = \"E1\",\n        rural = \"F1\"\n      ) %>% \n      forcats::fct_relevel(\n        c(\"rural\", \"urban\")\n      )\n  )"},{"path":"supervised-machine-learning.html","id":"logistic-regression","chapter":"10 Supervised machine learning","heading":"10.3 Logistic regression","text":"Can predict whether Output Area (OA) urban rural, solely based population density?two patters plot seem quite close even plotted using logarithmically transformed x-axis. first step, can extract dataset data need, create logarithmic transformation population density value. able perform simple validatin model, can divide data training (80% dataset) testing set (20% dataset).can compute logit model using stats::glm function specifying binomial() family. summary model highlights model significant, Residual deviance fairly close Null deviance (null model), good sign.per regression models, necessary test assumptions logit model overall distribution residuals. However, model one predictor, confine performance analysis simple validation.can test performance model validation exercise, using testing dataset. Finally, can compare results prediction original data using confusion matrix.","code":"\nliec_shire_2011OAC_RU %>%\n  ggplot2::ggplot(\n    aes(\n      x = u006, \n      y = rural_urban\n    )\n  ) +\n  ggplot2::geom_point(\n    aes(\n      color = rural_urban,\n      shape = rural_urban\n    )\n  ) +\n  ggplot2::scale_color_manual(values = c(\"deepskyblue2\", \"darkgreen\")) +\n  ggplot2::scale_x_log10() +\n  ggplot2::theme_bw()\n# Data for logit model\nru_logit_data <-\n  liec_shire_2011OAC_RU %>%\n  dplyr::select(OA11CD, u006, rural_urban) %>%\n  dplyr::mutate(\n    density_log = log10(u006)\n  )\n\n# Training set\nru_logit_data_trainig <-\n  ru_logit_data %>% \n  slice_sample(prop = 0.8)\n\n# Testing set\nru_logit_data_testing <- \n  ru_logit_data %>% \n  anti_join(ru_logit_data_trainig)\nru_logit_model <- \n  ru_logit_data_trainig %$%\n  stats::glm(\n    rural_urban ~ \n      density_log, \n    family = binomial()\n  )\n\nru_logit_model %>%  \n  summary()## \n## Call:\n## stats::glm(formula = rural_urban ~ density_log, family = binomial())\n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(>|z|)    \n## (Intercept) -1.23275    0.12739  -9.677   <2e-16 ***\n## density_log  1.76297    0.09781  18.025   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 2076.3  on 1667  degrees of freedom\n## Residual deviance: 1628.7  on 1666  degrees of freedom\n## AIC: 1632.7\n## \n## Number of Fisher Scoring iterations: 4\nru_logit_prediction <- \n  ru_logit_model %>%\n  # Use model to predict values\n  stats::predict(\n    ru_logit_data_testing, \n    type = \"response\"\n  ) %>%\n  as.numeric()\n\nru_logit_data_testing <- \n  ru_logit_data_testing %>%\n  tibble::add_column(\n    # Add column with predicted class\n    logit_predicted_ru = \n      # Values below 0.5 indicate first factor level (rural)\n      # Values above 0.5 indicate second factor level (ruban)\n      ifelse(\n        ru_logit_prediction <= 0.5,\n        \"rural\", # first factor level\n        \"urban\"  # second factor level\n      ) %>%\n      forcats::as_factor() %>% \n      forcats::fct_relevel(\n        c(\"rural\", \"urban\")\n      )\n  )\n\n# Load library for confusion matrix\nlibrary(caret)\n\n# Confusion matrix\ncaret::confusionMatrix(\n  ru_logit_data_testing %>% dplyr::pull(logit_predicted_ru),\n  ru_logit_data_testing %>% dplyr::pull(rural_urban),\n  mode = \"everything\"\n)## Confusion Matrix and Statistics\n## \n##           Reference\n## Prediction rural urban\n##      rural    66    21\n##      urban    56   274\n##                                           \n##                Accuracy : 0.8153          \n##                  95% CI : (0.7747, 0.8514)\n##     No Information Rate : 0.7074          \n##     P-Value [Acc > NIR] : 2.897e-07       \n##                                           \n##                   Kappa : 0.5129          \n##                                           \n##  Mcnemar's Test P-Value : 0.0001068       \n##                                           \n##             Sensitivity : 0.5410          \n##             Specificity : 0.9288          \n##          Pos Pred Value : 0.7586          \n##          Neg Pred Value : 0.8303          \n##               Precision : 0.7586          \n##                  Recall : 0.5410          \n##                      F1 : 0.6316          \n##              Prevalence : 0.2926          \n##          Detection Rate : 0.1583          \n##    Detection Prevalence : 0.2086          \n##       Balanced Accuracy : 0.7349          \n##                                           \n##        'Positive' Class : rural           \n## "},{"path":"supervised-machine-learning.html","id":"support-vector-machines","chapter":"10 Supervised machine learning","heading":"10.4 Support vector machines","text":"Support vector machines (SVMs) another common approach supervised classification. SVMs perform classification task partitioning data space regions separated hyperplanes. instance, bi-dimensional space, hyperplane line, algorithm designed find line best separates two groups data. Computationally, process dissimilar linear regression.showcase use SVMs, example expands one building model urban - rural classification uses total population area (logarithmically transformed) two separate input values, rather combined population density. aim SVM find line maximises margin two groups shown plot .plot illustrates two variables skewed (note axes logarithmically transformed) two groups linearly separable. can thus follow procedure similar one seen : extract necessary data; split data training testing validation; build model; predict values testing set interpret confusion matrix.third complex example, can explore build model rural - urban classification using presence five different types dwelling input variables. confusion matrix clearly illustrates simple linear SVM adequate construct model.","code":"\nliec_shire_2011OAC_RU %>%\n  ggplot2::ggplot(\n    aes(\n      x = u006, \n      y = Total_Population\n    )\n  ) +\n  ggplot2::geom_point(\n    aes(\n      color = rural_urban,\n      shape = rural_urban\n    )\n  ) +\n  ggplot2::scale_color_manual(values = c(\"deepskyblue2\", \"darkgreen\")) +\n  ggplot2::scale_x_log10() +\n  ggplot2::scale_y_log10() +\n  ggplot2::theme_bw()\n# Data for SVM model\nru_svm_data <-\n  liec_shire_2011OAC_RU %>%\n  dplyr::select(OA11CD, Total_Population, u006, rural_urban) %>%\n  dplyr::mutate(\n    area_log = log10(u006),\n    population_log = log10(Total_Population)\n  )\n\n# Training set\nru_svm_data_trainig <-\n  ru_svm_data %>% \n  slice_sample(prop = 0.8)\n\n# Testing set\nru_svm_data_testing <- \n  ru_svm_data %>% \n  anti_join(ru_svm_data_trainig)\n\n# Load library for svm function\nlibrary(e1071)\n\n# Build the model\nru_svm_model <- \n  ru_svm_data_trainig %$%\n  e1071::svm(\n    rural_urban ~ \n      area_log + population_log, \n    # Use a simple linear hyperplane\n    kernel = \"linear\", \n    # Scale the data\n    scale = TRUE,\n    # Cost value for observations\n    # crossing the hyperplane\n    cost = 10\n  )\n\n# Predict the values for the testing dataset\nru_svm_prediction <-\n  stats::predict(\n    ru_svm_model,\n    ru_svm_data_testing %>% \n      dplyr::select(area_log, population_log)\n  )\n\n# Add predicted values to the table\nru_svm_data_testing <-\n  ru_svm_data_testing %>%\n  tibble::add_column(\n    svm_predicted_ru = ru_svm_prediction\n  )\n\n# Confusion matrix\ncaret::confusionMatrix(\n  ru_svm_data_testing %>% dplyr::pull(svm_predicted_ru),\n  ru_svm_data_testing %>% dplyr::pull(rural_urban),\n  mode = \"everything\"\n  )## Confusion Matrix and Statistics\n## \n##           Reference\n## Prediction rural urban\n##      rural    66    29\n##      urban    61   261\n##                                           \n##                Accuracy : 0.7842          \n##                  95% CI : (0.7415, 0.8227)\n##     No Information Rate : 0.6954          \n##     P-Value [Acc > NIR] : 3.136e-05       \n##                                           \n##                   Kappa : 0.4517          \n##                                           \n##  Mcnemar's Test P-Value : 0.001084        \n##                                           \n##             Sensitivity : 0.5197          \n##             Specificity : 0.9000          \n##          Pos Pred Value : 0.6947          \n##          Neg Pred Value : 0.8106          \n##               Precision : 0.6947          \n##                  Recall : 0.5197          \n##                      F1 : 0.5946          \n##              Prevalence : 0.3046          \n##          Detection Rate : 0.1583          \n##    Detection Prevalence : 0.2278          \n##       Balanced Accuracy : 0.7098          \n##                                           \n##        'Positive' Class : rural           \n## \n# Data for SVM model\nru_dwellings_data <-\n  liec_shire_2011OAC_RU %>%\n  dplyr::select(\n    OA11CD, rural_urban, Total_Dwellings,\n    u086:u090\n  ) %>%\n  # scale across\n  dplyr::mutate(\n    dplyr::across( \n      u086:u090,\n      scale\n      #function(x){ (x / Total_Dwellings) * 100 }\n    )\n  ) %>%\n  dplyr::rename(\n    scaled_detached = u086,\n    scaled_semidetached = u087,\n    scaled_terraced = u088,\n    scaled_flats = u089,    \n    scaled_carava_tmp = u090\n  ) %>%\n  dplyr::select(-Total_Dwellings)\n\n# Training set\nru_dwellings_data_trainig <-\n  ru_dwellings_data %>% \n  slice_sample(prop = 0.8)\n\n# Testing set\nru_dwellings_data_testing <- \n  ru_dwellings_data %>% \n  anti_join(ru_dwellings_data_trainig)\n\n# Build the model\nru_dwellings_svm_model <- \n  ru_dwellings_data_trainig %$%\n  e1071::svm(\n    rural_urban ~ \n      scaled_detached + scaled_semidetached + scaled_terraced + \n      scaled_flats + scaled_carava_tmp, \n    # Use a simple linear hyperplane\n    kernel = \"linear\",\n    # Cost value for observations\n    # crossing the hyperplane\n    cost = 10\n  )\n\n# Predict the values for the testing dataset\nru_dwellings_svm_prediction <-\n  stats::predict(\n    ru_dwellings_svm_model,\n    ru_dwellings_data_testing %>% \n      dplyr::select(scaled_detached:scaled_carava_tmp)\n  )\n\n# Add predicted values to the table\nru_dwellings_data_testing <-\n  ru_dwellings_data_testing %>%\n  tibble::add_column(\n    dwellings_svm_predicted_ru = ru_dwellings_svm_prediction\n  )\n\n# Confusion matrix\ncaret::confusionMatrix(\n  ru_dwellings_data_testing %>% dplyr::pull(dwellings_svm_predicted_ru),\n  ru_dwellings_data_testing %>% dplyr::pull(rural_urban),\n  mode = \"everything\"\n  )## Confusion Matrix and Statistics\n## \n##           Reference\n## Prediction rural urban\n##      rural     0     0\n##      urban   130   287\n##                                           \n##                Accuracy : 0.6882          \n##                  95% CI : (0.6414, 0.7324)\n##     No Information Rate : 0.6882          \n##     P-Value [Acc > NIR] : 0.5237          \n##                                           \n##                   Kappa : 0               \n##                                           \n##  Mcnemar's Test P-Value : <2e-16          \n##                                           \n##             Sensitivity : 0.0000          \n##             Specificity : 1.0000          \n##          Pos Pred Value :    NaN          \n##          Neg Pred Value : 0.6882          \n##               Precision :     NA          \n##                  Recall : 0.0000          \n##                      F1 :     NA          \n##              Prevalence : 0.3118          \n##          Detection Rate : 0.0000          \n##    Detection Prevalence : 0.0000          \n##       Balanced Accuracy : 0.5000          \n##                                           \n##        'Positive' Class : rural           \n## "},{"path":"supervised-machine-learning.html","id":"kernels","chapter":"10 Supervised machine learning","heading":"10.4.1 Kernels","text":"Instead relying simple linear hyperplanes, can use “kernel trick” project data higher-dimensional space, might allow groups (easily) linearly separable.","code":"\n# Build a second model\n# using a radial kernel\nru_dwellings_svm_radial_model <- \n  ru_dwellings_data_trainig %$%\n  e1071::svm(\n    rural_urban ~ \n      scaled_detached + scaled_semidetached + scaled_terraced + \n      scaled_flats + scaled_carava_tmp, \n    # Use a radial kernel\n    kernel = \"radial\",\n    # Cost value for observations\n    # crossing the hyperplane\n    cost = 10\n  )\n\n# Predict the values for the testing dataset\nru_svm_dwellings_radial_prediction <-\n  stats::predict(\n    ru_dwellings_svm_radial_model,\n    ru_dwellings_data_testing %>% \n      dplyr::select(scaled_detached:scaled_carava_tmp)\n  )\n\n# Add predicted values to the table\nru_dwellings_data_testing <-\n  ru_dwellings_data_testing %>%\n  tibble::add_column(\n    dwellings_radial_predicted_ru = ru_svm_dwellings_radial_prediction\n  )\n\n# Confusion matrix\ncaret::confusionMatrix(\n  ru_dwellings_data_testing %>% dplyr::pull(dwellings_radial_predicted_ru),\n  ru_dwellings_data_testing %>% dplyr::pull(rural_urban),\n  mode = \"everything\"\n  )## Confusion Matrix and Statistics\n## \n##           Reference\n## Prediction rural urban\n##      rural    29    13\n##      urban   101   274\n##                                           \n##                Accuracy : 0.7266          \n##                  95% CI : (0.6811, 0.7689)\n##     No Information Rate : 0.6882          \n##     P-Value [Acc > NIR] : 0.04938         \n##                                           \n##                   Kappa : 0.2182          \n##                                           \n##  Mcnemar's Test P-Value : 3.691e-16       \n##                                           \n##             Sensitivity : 0.22308         \n##             Specificity : 0.95470         \n##          Pos Pred Value : 0.69048         \n##          Neg Pred Value : 0.73067         \n##               Precision : 0.69048         \n##                  Recall : 0.22308         \n##                      F1 : 0.33721         \n##              Prevalence : 0.31175         \n##          Detection Rate : 0.06954         \n##    Detection Prevalence : 0.10072         \n##       Balanced Accuracy : 0.58889         \n##                                           \n##        'Positive' Class : rural           \n## "},{"path":"supervised-machine-learning.html","id":"artificial-neural-networks","chapter":"10 Supervised machine learning","heading":"10.5 Artificial neural networks","text":"Artificial neural networks (ANNs) one studied approaches supervised machine learning, term actually defines large set different approaches. model aims simulate simplistic version brain made artificial neurons. artificial neuron combines series input values one output values, using series weights (one per input value) activation function. aim model learn optimal set weights , combined input values, generates correct output value. latter also influenced activation function, modulates final result.neuron effectively regression model. input values predictors (independent variables), output outcome (dependent variable), weights coefficients (see also previous chapter regression models). selection activation function defines regression model. ANNs commonly used classification, one common activation functions used sigmoid, thus rendering every single neuron logistic regression.instance ANN defined topology (number layers nodes), activation functions algorithm used train network. selection parameters renders construction ANNs complex task, quality result frequently relies experience data scientist.Number layers\nSingle-layer network: one node input variable one node per category output variable, effectively logistic regression.\nMulti-layer network: adds one hidden layer, aims capture hidden “features” data, combinations input values, use final classification.\nDeep neural networks: several hidden layers, aiming capture complex “features” data.\nSingle-layer network: one node input variable one node per category output variable, effectively logistic regression.Multi-layer network: adds one hidden layer, aims capture hidden “features” data, combinations input values, use final classification.Deep neural networks: several hidden layers, aiming capture complex “features” data.Number nodes\nnumber nodes needs selected one hidden layers.\nnumber nodes needs selected one hidden layers.can explore construction ANN input output variables seen previous example, compare one two approaches produces better results. example creates multi-layer ANN, using two hidden layers, five two nodes, respectively.","code":"\n# Load library for ANNs\nlibrary(neuralnet)\n\n# Build a third model\n# using an ANN\nru_dwellings_nnet_model <-\n  neuralnet::neuralnet(\n    rural_urban ~ \n      scaled_detached + scaled_semidetached + scaled_terraced + \n      scaled_flats + scaled_carava_tmp,\n    data = ru_dwellings_data_trainig,\n    # Use 2 hidden layers\n    hidden = c(5, 2),\n    # Max num of steps for training\n    stepmax = 1000000\n  )\nru_dwellings_nnet_model %>%  plot(rep = \"best\")\n# Predict the values for the testing dataset\nru_dwellings_nnet_prediction <-\n  neuralnet::compute(\n    ru_dwellings_nnet_model,\n    ru_dwellings_data_testing %>% \n      dplyr::select(scaled_detached:scaled_carava_tmp)\n  )\n\n# Derive predicted categories\nru_dwellings_nnet_predicted_categories <-\n  # from the prediction object\n  ru_dwellings_nnet_prediction %$%\n  # extract the result\n  # which is a matrix of probabilities\n  # for each object and category\n  net.result %>%\n  # select the column (thus the category)\n  # with higher probability\n  max.col %>%\n  # recode columns values as\n  # rural or urban\n  dplyr::recode(\n    `1` = \"rural\",\n    `2` = \"urban\"\n  ) %>%\n  forcats::as_factor() %>% \n  forcats::fct_relevel(\n    c(\"rural\", \"urban\")\n  )\n\n# Add predicted values to the table\nru_dwellings_data_testing <-\n  ru_dwellings_data_testing %>%\n  tibble::add_column(\n    dwellings_nnet_predicted_ru = \n      ru_dwellings_nnet_predicted_categories\n  )\n\n# Confusion matrix\ncaret::confusionMatrix(\n  ru_dwellings_data_testing %>% dplyr::pull(dwellings_nnet_predicted_ru),\n  ru_dwellings_data_testing %>% dplyr::pull(rural_urban),\n  mode = \"everything\"\n  )## Confusion Matrix and Statistics\n## \n##           Reference\n## Prediction rural urban\n##      rural    56    24\n##      urban    74   263\n##                                           \n##                Accuracy : 0.765           \n##                  95% CI : (0.7213, 0.8049)\n##     No Information Rate : 0.6882          \n##     P-Value [Acc > NIR] : 0.0003257       \n##                                           \n##                   Kappa : 0.388           \n##                                           \n##  Mcnemar's Test P-Value : 7.431e-07       \n##                                           \n##             Sensitivity : 0.4308          \n##             Specificity : 0.9164          \n##          Pos Pred Value : 0.7000          \n##          Neg Pred Value : 0.7804          \n##               Precision : 0.7000          \n##                  Recall : 0.4308          \n##                      F1 : 0.5333          \n##              Prevalence : 0.3118          \n##          Detection Rate : 0.1343          \n##    Detection Prevalence : 0.1918          \n##       Balanced Accuracy : 0.6736          \n##                                           \n##        'Positive' Class : rural           \n## "},{"path":"supervised-machine-learning.html","id":"exercise-404.1","chapter":"10 Supervised machine learning","heading":"10.6 Exercise 404.1","text":"Question 404.1.1: Create SVM model capable classifying areas Leicester Leicestershire rural urban based series variables relate “Economic Activity” among 167 initial variables used create 2011 Output Area Classification (Gale et al., 2016).Question 404.1.2: Create ANN using input output values used Question 404.1.1.Question 404.1.3: Assess one two models preforms better classification.Stefano De Sabbata – text licensed CC -SA 4.0, contains public sector information licensed Open Government Licence v3.0, code licensed GNU GPL v3.0.","code":""},{"path":"unsupervised-machine-learning.html","id":"unsupervised-machine-learning","chapter":"11 Unsupervised machine learning","heading":"11 Unsupervised machine learning","text":"Print chapterAs discussed previous chapter, machine learning approaches divided two main types20.Supervised:\ntraining “predictive” model data;\none () attribute dataset used “predict” another attribute.\ntraining “predictive” model data;one () attribute dataset used “predict” another attribute.Unsupervised:\ndiscovery descriptive patterns data;\ncommonly used data mining.\ndiscovery descriptive patterns data;commonly used data mining.Clustering classic unsupervised machine learning task, aims “automatically divides data clusters , groups similar items”(Lantz, 2019). computer science, wide range approaches developed tackle clustering. Among approaches, two common centroid-based approaches (k-means) hierarchical approaches. approaches include density-based clustering methods (DBSCAN) mixed approaches (bagged clustering), combine different aspects centroid-based hierarchical approaches.","code":""},{"path":"unsupervised-machine-learning.html","id":"k-means","chapter":"11 Unsupervised machine learning","heading":"11.1 K-means","text":"k-mean approach clusters \\(n\\) observations (\\(x\\)) \\(k\\) clusters (\\(c\\)) minimising within-cluster sum squares (WCSS) iterative process. , algorithm calculates distance observation (.e., case, object, row table) centroid cluster. square values distances summed cluster, whole dataset. aim algorithm minimise value.\\[WCSS = \\sum_{c=1}^{k} \\sum_{x \\c} (x - \\overline{x}_c)^2\\]minimise WCSS, trying identify k clusters, k-mean first randomly select k observations initial centroids. , k-means repeats two steps . Every time k-means repeats two steps, new centroids closer two actual centre. process continues centroids don’t change anymore (within certain margin error) reached maximum number iterations set analyst.assignment step: observations assigned closest centroidsupdate step: calculate means cluster, new centroid","code":""},{"path":"unsupervised-machine-learning.html","id":"number-of-clusters","chapter":"11 Unsupervised machine learning","heading":"11.1.1 Number of clusters","text":"key limitation k-mean requires select number clusters identified advance. Unfortunately, analysts always position knowing advance many clusters supposed within data analysing. cases, number heuristics can used select number clusters best fits data.well-known method “elbow method”. approach suggests calculate k-means range values k, calculate WCSS obtained k, select value k minimises WCSS without increasing number clusters beyond point decrease WCSS minimal. approach called “elbow method” (can seen examples ) printing line representing value WCSS values k taken account, suggests select value k “elbow” inflation point line.heuristics exist, suggest using alternative measures cluster quality. instance, cluster library provides simple ways calculate silhouette measure gap statistic. silhouette value indicates well observations fit within clusters, whereas gap statistic measures dispersion within cluster, compared uniform distribution values. higher value gap statistic, away distribution uniform (thus higher quality clustering).three heuristics, best approach calculate values using bootstrapping approach. , calculate statistics multiple times samples dataset, order account random variation. However, clusGap function cluster library allows bootstrapping natively, illustrated .","code":""},{"path":"unsupervised-machine-learning.html","id":"geodemographic-classification","chapter":"11 Unsupervised machine learning","heading":"11.2 Geodemographic Classification","text":"GIScience, clustering approaches commonly used create geodemographic classifications. instance, Gale et al., 2016 created 2011 Output Area Classification (2011 OAC) starting initial set 167 prospective variables UK Census 2011.process creating classification, 86 variables removed initial set, including highly correlated variables don’t bring additional information classification process. Furthermore, 41 variable retained , whereas 40 combined, create final set 60 variables. k-mean approach applied cluster census Output Areas (OAs) 8 supergroups, 26 groups 76 subgroups.paper provides detail report process. particular, interesting see authors applied process variable selection involving repeated clustering excluding one variable, see within cluster sum square measure (WCSS) affected. Variable produced significantly higher WCSS excluded considered exclusion final analysis, order increase homogeneity clusters.clustering completed, final step geodemographic classification interpretation resulting cluster, commonly done observing average values variables cluster.two examples explore creation simple geodemographic classifications city Leicester, using variables United Kingdom 2011 Census included among 167 initial variables Gale et al., 2016 taken account creating 2011 Output Area Classification.variables going take account five ones listed , plus total count statistical unit, Total_Dwellings.u086: Detachedu087: Semi-detachedu088: Terraced (including end-terrace)u089: Flatsu090: Caravan mobile temporary structureThe code extracts necessary variables original dataset applies normalisation steps across five variables listed . Finally, columns renamed user-friendly names adding perc_ front column name.first step k-means select observations random initial centroids. result, every time run computation, starting point slightly different, might result. particular, likely cluster order might chance (.e., cluster 1 one time might cluster 3 next), although overall result stable. Nevertheless, make document reproducible, can set “seed” generation random numbers. ensure observations selected random , thus results . can done R, using function set.seed providing number (relatively large number ) “seed”.","code":"\nlibrary(tidyverse)## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n## ✔ dplyr     1.1.3     ✔ readr     2.1.4\n## ✔ forcats   1.0.0     ✔ stringr   1.5.0\n## ✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n## ✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n## ✔ purrr     1.0.2     \n## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n## ✖ dplyr::filter() masks stats::filter()\n## ✖ dplyr::lag()    masks stats::lag()\n## ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\nlibrary(magrittr)## \n## Attaching package: 'magrittr'\n## \n## The following object is masked from 'package:purrr':\n## \n##     set_names\n## \n## The following object is masked from 'package:tidyr':\n## \n##     extract\nlibrary(cluster)\nleicester_2011OAC <- readr::read_csv(\"2011_OAC_Raw_uVariables_Leicester.csv\")\nleicester_dwellings <-\n  leicester_2011OAC %>%\n  dplyr::select(\n    OA11CD, Total_Dwellings,\n    u086:u090\n  ) %>%\n  # scale across\n  dplyr::mutate(\n    dplyr::across( \n      u086:u090,\n      #scale\n      function(x){ (x / Total_Dwellings) * 100 }\n    )\n  ) %>%\n  dplyr::rename(\n    detached = u086,\n    semidetached = u087,\n    terraced = u088,\n    flats = u089,   \n    carava_tmp = u090\n  ) %>%\n  # rename columns\n  dplyr::rename_with(\n    function(x){ paste0(\"perc_\", x) },\n    detached:carava_tmp\n  )\nset.seed(20201208)"},{"path":"unsupervised-machine-learning.html","id":"terrace-and-semi-detached-houses","chapter":"11 Unsupervised machine learning","heading":"11.3 Terrace and semi-detached houses","text":"first example explores create geodemographic classification using two variables.u087 (now semidetached): Semi-detachedu088(now terraced): Terraced (including end-terrace)first step, can explore relationship two variables. code illustrates use ggpairs GGally library, provides additional complex plots top ggplot2 framework.scatterplot seems indicate least three clusters might exist data. One lot semi-detached terraced house (top-left corner scatterplot); one lot terraced semi-detached houses (bottom-right corner scatterplot); one classes (bottom-left corner scatterplot).However, clearly many OAs populate area three groups. , best approach cluster OAs?code illustrates create three plots. first follows elbow method heuristic. second third take similar approach using silhouette gap statistic. Please see chapter control structures regarding use construct .Based WCSS plot, number clusters k best fitting data range k = 3 k = 6, around inflation point (elbow) line. silhouette plot shows local maximum k = 3 k = 6, indicates observations best fit within clusters 3 6 clusters created. gap statistic steadily increases reaches plateau around k = 5. indicates clustering improves move 2 5 clusters, quality doesn’t increase much afterwards. value k = 6 (value suggested two heuristics) local minimum, still difference neighbouring values relatively small.Overall, heuristics seem indicate k = 3 lead good clustering result. However, gap statistic indicates three clusters compact. probably due fact observations center scatterplot seen rather uniformally distributed space three main clusters. , chosing k = 6 clusters might best fitting approach case. can calculate clusters k = 6 shown .","code":"\n# install.packages(\"GGally\")\nlibrary(GGally)\n\nleicester_dwellings %>%\n  dplyr::select(perc_semidetached, perc_terraced) %>%\n  GGally::ggpairs(\n    upper = list(continuous = wrap(ggally_cor, method = \"kendall\")),\n    lower = list(continuous = wrap(\"points\", alpha = 0.3, size=0.1))\n  )\n# Get only the data necessary for testing\ndata_for_testing <-\n  leicester_dwellings %>%\n  dplyr::select(perc_semidetached, perc_terraced)\n\n# Calculate WCSS and silhouette\n# for k = 2 to 15\n# Set up two vectors where to store\n# the calculated WCSS and silhouette value\ntesting_wcss <- rep(NA, 15)\ntesting_silhouette <- rep(NA, 15)\n\n# for k = 2 to 15\nfor (testing_k in 2:15){\n  # Calculate kmeans\n  kmeans_result <- \n    stats::kmeans(data_for_testing, centers = testing_k, iter.max = 50)\n  \n  # Extract WCSS\n  # and save it in the vector\n  testing_wcss[testing_k] <- kmeans_result %$% tot.withinss\n  \n  # Calculate average silhouette\n  # and save it in the vector\n  testing_silhouette[testing_k] <- \n    kmeans_result %$% cluster %>%\n    cluster::silhouette(\n      data_for_testing %>% dist()\n    ) %>%\n    magrittr::extract(, 3) %>% mean()\n}\n\n# Calculate the gap statistic using bootstrapping\ntesting_gap <- \n  cluster::clusGap(\n    data_for_testing, \n    FUN = kmeans, \n    K.max = 15, # max number of clusters\n    B = 50      # number of samples\n  )\n# Plots\nplot(2:15, testing_wcss[2:15], type=\"b\", xlab=\"Number of Clusters\", \n     ylab=\"WCSS\", xlim=c(1,15)) +\nabline(v = 3, col = \"red\") +\nabline(v = 6, col = \"red\")## integer(0)\nplot(2:15, testing_silhouette[2:15], type=\"b\", xlab=\"Number of Clusters\", \n     ylab=\"Silhouette\", xlim=c(1,15)) +\nabline(v = 3, col = \"red\") +\nabline(v = 6, col = \"red\")## integer(0)\nplot(2:15, testing_gap[[\"Tab\"]][2:15, \"gap\"], type=\"b\", xlab=\"Number of Clusters\", \n     ylab=\"Gap\", xlim=c(1,15)) +\nabline(v = 3, col = \"red\") +\nabline(v = 6, col = \"red\")## integer(0)\nterr_sede_kmeans <- leicester_dwellings %>%\n  dplyr::select(perc_semidetached, perc_terraced) %>%\n  stats::kmeans(centers = 6, iter.max = 50)\n\nleicester_dwellings <- \n  leicester_dwellings %>%\n  tibble::add_column(\n    terr_sede_cluster = terr_sede_kmeans %$% cluster %>% as.character()\n  )"},{"path":"unsupervised-machine-learning.html","id":"interpreting-the-clusters","chapter":"11 Unsupervised machine learning","heading":"11.3.1 Interpreting the clusters","text":"clustering completed, can analyse results visual analysis. instance, can plot two original variables, along computed clusters illustrated .Another common approach interpreting results create heatmaps average values variables used clustering process cluster.plot , clearly illustrates cluster 6 high percentage semi-detached houses low percentages terraced houses. Cluster 2 high percentage terraced houses low percentages semi-detached houses. Cluster 4 low percentage semi-detached terraced houses. three clusters first identified first scatterplot .Moreover, clustering process identifies cluster 1, includes similar percentages semi-detached terraced houses; well cluster 5, including mostly semi-detached also terraced houses, cluster 3, including mostly terraced also semi-detached houses.","code":"\nleicester_dwellings %>%\n  dplyr::select(perc_semidetached, perc_terraced, terr_sede_cluster) %>%\n  GGally::ggpairs(\n    mapping = aes(color = terr_sede_cluster),\n    upper = list(continuous = wrap(ggally_cor, method = \"kendall\")),\n    lower = list(continuous = wrap(\"points\", alpha = 0.3, size=0.1))\n  )\nleicester_dwellings %>%\n  group_by(terr_sede_cluster) %>%\n  dplyr::summarise(\n    avg_perc_semidetached = mean(perc_semidetached), \n    avg_perc_terraced = mean(perc_terraced)\n  ) %>%\n  dplyr::select(terr_sede_cluster, avg_perc_semidetached, avg_perc_terraced) %>%\n  tidyr::pivot_longer(\n    cols = -terr_sede_cluster,\n    names_to = \"clustering_dimension\",\n    values_to = \"value\"\n  ) %>%\n  ggplot2::ggplot(\n    aes(\n      x = clustering_dimension,\n      y = terr_sede_cluster\n    )\n  ) +\n  ggplot2::geom_tile(aes(fill = value)) +\n  ggplot2::xlab(\"Clustering dimension\") + \n  ggplot2::ylab(\"Cluster\") +\n  ggplot2::scale_fill_viridis_c(option = \"inferno\") +\n  ggplot2::theme_bw()"},{"path":"unsupervised-machine-learning.html","id":"a-geodemographic-of-dwelling-types","chapter":"11 Unsupervised machine learning","heading":"11.4 A geodemographic of dwelling types","text":"case study useful simple example create geodemographic classification, possibly interesting analysis due limited number variables used. section, explore creation geodemographic dwelling types city Leicester, using five variables available original dataset.can start visual analysis data. relationship different variables generally resembles one seen semi-detached terraced houses seen example , except caravans mobile temporary structures, seem fairly rare Leicester. variable seems particularly well correlated . Thus variables included classification, potential contribute relevant information.order identify number clusters k best fits data, can use elbow method, along silhouette gap statistic measures, seen previous example. difference case data_for_testing include five variables.previous example, elbow method (.e., WCSS), silhouette gap statistic seem indicate k = 3 k = 6 might best choice. Let’s see result choosing k = 6.","code":"\nleicester_dwellings %>%\n  dplyr::select(perc_detached:perc_carava_tmp) %>%\n  GGally::ggpairs(\n    upper = list(continuous = wrap(ggally_cor, method = \"kendall\")),\n    lower = list(continuous = wrap(\"points\", alpha = 0.3, size=0.1))\n  )\n# Data for elbow method\ndata_for_testing <-\n  leicester_dwellings %>%\n  dplyr::select(perc_detached:perc_carava_tmp)\n\n# Calculate WCSS and silhouette\n# for k = 2 to 15\n# Set up two vectors where to store\n# the calculated WCSS and silhouette value\ntesting_wcss <- rep(NA, 15)\ntesting_silhouette <- rep(NA, 15)\n\n# for k = 2 to 15\nfor (testing_k in 2:15){\n  # Calculate kmeans\n  kmeans_result <- \n    stats::kmeans(data_for_testing, centers = testing_k, iter.max = 50)\n  \n  # Extract WCSS\n  # and save it in the vector\n  testing_wcss[testing_k] <- kmeans_result %$% tot.withinss\n  \n  # Calculate average silhouette\n  # and save it in the vector\n  testing_silhouette[testing_k] <- \n    kmeans_result %$% cluster %>%\n    cluster::silhouette(\n      data_for_testing %>% dist()\n    ) %>%\n    magrittr::extract(, 3) %>% mean()\n}\n\n# Calculate the gap statistic using bootstrapping\ntesting_gap <- \n  cluster::clusGap(data_for_testing, FUN = kmeans, \n    K.max = 15, B = 50\n  )\n# Plots\nplot(2:15, testing_wcss[2:15], type=\"b\", xlab=\"Number of Clusters\", \n     ylab=\"WCSS\", xlim=c(1,15)) +\nabline(v = 3, col = \"red\") +\nabline(v = 6, col = \"red\")## integer(0)\nplot(2:15, testing_silhouette[2:15], type=\"b\", xlab=\"Number of Clusters\", \n     ylab=\"Silhouette\", xlim=c(1,15)) +\nabline(v = 3, col = \"red\") +\nabline(v = 6, col = \"red\")## integer(0)\nplot(2:15, testing_gap[[\"Tab\"]][2:15, \"gap\"], type=\"b\", xlab=\"Number of Clusters\", \n     ylab=\"Gap\", xlim=c(1,15)) +\nabline(v = 3, col = \"red\") +\nabline(v = 6, col = \"red\")## integer(0)\ndwellings_kmeans <- leicester_dwellings %>%\n  dplyr::select(perc_detached:perc_carava_tmp) %>%\n  stats::kmeans(\n    centers = 6, \n    iter.max = 50\n  )\n\nleicester_dwellings <- \n  leicester_dwellings %>%\n  tibble::add_column(\n    dwellings_cluster = \n      dwellings_kmeans %$% \n        cluster %>%\n          as.character()\n  )"},{"path":"unsupervised-machine-learning.html","id":"interpreting-the-clusters-1","chapter":"11 Unsupervised machine learning","heading":"11.4.1 Interpreting the clusters","text":"first exploratory plot clusters seems reveal clusters closely resemble seen first example .previous example, can use “heatmap” plot explore clusters characterised variables used clustering process (see also Exercise 414.1.1 ).Another common approach explore characteristics clusters created k-means geodemongraphic classification use radar charts (also known spider charts, web charts polar charts), can created R using number libraries, including radarchart fmsb library.radar charts effective visualising values multiple varaibles, long variables similar type, value range. case, values percentages, radar chart effective illustrating variables particularly high averages cluster.","code":"\nleicester_dwellings %>%\n  dplyr::select(perc_detached:perc_carava_tmp, dwellings_cluster) %>%\n  GGally::ggpairs(\n    mapping = aes(color = dwellings_cluster),\n    lower = list(continuous = wrap(\"points\", alpha = 0.3, size=0.1))\n  )\ndwellings_cluster_avgs <-\n  leicester_dwellings %>%\n  group_by(dwellings_cluster) %>%\n  dplyr::summarise(\n    dplyr::across(\n      perc_detached:perc_carava_tmp,\n      mean\n    ) \n  ) %>%\n  # rename columns\n  dplyr::rename_with(\n    function(x){ paste0(\"avg_\", x) },\n    perc_detached:perc_carava_tmp\n  )\n  \ndwellings_cluster_avgs %>%\n  tidyr::pivot_longer(\n    cols = -dwellings_cluster,\n    names_to = \"clustering_dimension\",\n    values_to = \"value\"\n  )  %>%\n  ggplot2::ggplot(\n    aes(\n      x = clustering_dimension,\n      y = dwellings_cluster\n    )\n  ) +\n  ggplot2::geom_tile(\n    aes(\n      fill = value\n    )\n  ) +\n  ggplot2::xlab(\"Clustering dimension\") + \n  ggplot2::ylab(\"Cluster\") +\n  ggplot2::scale_fill_viridis_c(option = \"inferno\") +\n  ggplot2::theme_bw() +\n  ggplot2::theme(\n    axis.text.x = \n      element_text(\n        angle = 90, \n        vjust = 0.5, \n        hjust=1\n      )\n    )\n# install.packages(\"fmsb\")\nlibrary(fmsb)\n\npar(mar=rep(3,4))\npar(mfrow=c(3,2))\n\nfor(cluster_number in 1:6){\n  rbind (\n    # The radar chart requires a maximum and a minimum row \n    # before the actual data\n    rep(100, 5), # max 100% for 5 variables\n    rep(0, 5),   # min 0% for 5 variables\n    dwellings_cluster_avgs %>%\n      dplyr::filter(dwellings_cluster == cluster_number) %>%\n      dplyr::select(-dwellings_cluster) %>%\n      as.data.frame()\n    ) %>%\n    fmsb::radarchart(title = paste(\"Cluster\", cluster_number))\n}"},{"path":"unsupervised-machine-learning.html","id":"exercise-414.1","chapter":"11 Unsupervised machine learning","heading":"11.5 Exercise 414.1","text":"Question 414.1.1: Based “heatmap”, radar charts map created example , characterise five clusters? name ?.Question 414.1.2: Create geodemographic classification using data seen second example , creating k = 9 clusters.Question 414.1.3: Create geodemographic classification city Leicester based presence peoples different age groups included 2011_OAC_Raw_uVariables_Leicester.csv dataset (u007 u019).Stefano De Sabbata – text licensed CC -SA 4.0, contains public sector information licensed Open Government Licence v3.0, code licensed GNU GPL v3.0.","code":""},{"path":"r-scripting.html","id":"r-scripting","chapter":"12 R scripting","heading":"12 R scripting","text":"Print chapter","code":""},{"path":"r-scripting.html","id":"conditional-statements","chapter":"12 R scripting","heading":"12.1 Conditional statements","text":"Conditional statements fundamental (procedural) programming, allow execute execute part procedure depending whether certain condition true. condition tested, part procedure execute case condition true included code block.simple conditional statement can created using example . complex structure can created using else provide procedure execute case condition true also alternative procedure executed condition false.Finally, conditional statements can nested. , conditional statement can included part code block executed condition tested. instance, example , second conditional statement included code block executed case condition false.Similarly, one examples seen lecture coded follows.Alternatively, set conditions can rewritten follows.choice structure conditions depends logic behind code aiming execute nature order operations need executed.","code":"\ntemperature <- 25\n\nif (temperature > 25) {\n  cat(\"It really warm today!\")\n}\ntemperature <- 12\n\nif (temperature > 25) {\n  cat(\"It really warm today!\")\n} else {\n  cat(\"Today is not warm\")\n}## Today is not warm\ntemperature <- -5\n\nif (temperature > 25) {\n  cat(\"It really warm today!\")\n} else {\n  if (temperature > 0) {\n    cat(\"There is a nice temperature today\")\n  } else {\n    cat(\"This is really cold!\")\n  }\n}## This is really cold!\na_value <- -7\n\nif (a_value == 0) {\n  cat(\"Zero\")\n} else {\n  if (a_value < 0) {\n    cat(\"Negative\") \n  } else {\n    cat(\"Positive\")\n  }\n}## Negative\na_value <- -7\n\nif (a_value < 0) {\n    cat(\"Negative\") \n} else if (a_value == 0) {\n  cat(\"Zero\")\n} else {\n  cat(\"Positive\")\n}## Negative"},{"path":"r-scripting.html","id":"loops","chapter":"12 R scripting","heading":"12.2 Loops","text":"Loops another core component (procedural) programming implement idea solving problem executing task performing set steps number times. two main kinds loops R - deterministic conditional loops. former executed fixed number times, specified beginning loop. latter executed specific condition met. deterministic conditional loops extremely important working vectors.","code":""},{"path":"r-scripting.html","id":"conditional-loops","chapter":"12 R scripting","heading":"12.2.1 Conditional Loops","text":"R, conditional loops can implemented using repeat. difference two mostly syntactical: first tests condition first execute related code block condition true; second executes code block break command given (usually conditional statement).Conditional loops can source issues within script. accurately designed, can enter infinite loop – instance condition always TRUE break condition within repeat always FALSE.","code":"\na_value <- 0\n# Keep printing as long as x is smaller than 2\nwhile (a_value < 2) {\n  cat(a_value, \"\\n\")\n  a_value <- a_value + 1\n}## 0 \n## 1\na_value <- 0\n# Keep printing, if x is greater or equal than 2 than stop\nrepeat {\n  cat(a_value, \"\\n\")\n  a_value <- a_value + 1\n  if (a_value >= 2) break\n}## 0 \n## 1"},{"path":"r-scripting.html","id":"deterministic-loops","chapter":"12 R scripting","heading":"12.2.2 Deterministic Loops","text":"deterministic loop executes subsequent code block iterating elements provided vector. iteration (.e., execution code block), current element vector assigned variable statement, can used code block., instance, possible iterate vector print elements.common practice create vector integers spot (e.g., using : operator) execute certain sequence steps pre-defined number times.","code":"\neast_midlands_cities <- c(\"Derby\", \"Leicester\", \"Lincoln\", \"Nottingham\")\n\nfor (city in east_midlands_cities){\n  cat(city, \"\\n\")\n}## Derby \n## Leicester \n## Lincoln \n## Nottingham\nfor (iterator in 1:3) {\n  cat(\"Exectuion number\", iterator, \":\\n\")\n  cat(\"    Step1: Hi!\\n\")\n  cat(\"    Step2: How is it going?\\n\")\n}## Exectuion number 1 :\n##     Step1: Hi!\n##     Step2: How is it going?\n## Exectuion number 2 :\n##     Step1: Hi!\n##     Step2: How is it going?\n## Exectuion number 3 :\n##     Step1: Hi!\n##     Step2: How is it going?"},{"path":"r-scripting.html","id":"example-multiple-shapirowilk-tests","chapter":"12 R scripting","heading":"12.3 Example: multiple Shapiro–Wilk tests","text":"seen lecture, possible use control structures scale analysis design code way analysis step executed different data. code variation seen class, illustrates conduct simple analysis normal distribution different sections input data.chunck ises option results = 'asis', allows interpret output Markdown code. allows translate presence ### near supergroup name third-level heading output.","code":"\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(knitr)\nleicester_2011OAC <- read_csv(\"2011_OAC_Raw_uVariables_Leicester.csv\")\nfor (\n  \n  current_supgrp in\n    # Extract the list of unique supergroup names\n    leicester_2011OAC %>% \n      pull(supgrpname) %>% \n      unique()\n  \n  ) {\n  \n  # Print supergroup name\n  cat(\"\\n\\n\")\n  cat(\n    # Use a markdown heading\n    # including some space above and blow it\n    \"###\",\n    current_supgrp,\n    \"\\n\\n\"\n  )\n  \n  # Run a Shapiro–Wilk test\n  current_shapiro_test <-\n    leicester_2011OAC %>% \n    filter(supgrpname == current_supgrp) %>% \n    pull(Total_Population) %>%\n    shapiro.test()\n  \n  # Extract p value\n  current_p_value <-\n    current_shapiro_test %$% \n    p.value\n  \n  # Print whether the distribution\n  # is significant or not\n  if (current_p_value > 0.01){\n    cat(\n      \"\\n\\n\",\n      \"The total population\",\n      \"of OA in Leicester\",\n      \"in areas classified as\",\n      current_supgrp,\n      \"is normally distributed\",\n      \"\\n\\n\"\n    )\n  } else {\n    cat(\n      \"\\n\\n\",\n      \"The total population\",\n      \"of OA in Leicester\",\n      \"in areas classified as\",\n      current_supgrp,\n      \"is not normally distributed\",\n      \"\\n\\n\"\n    )\n  }\n  \n  # Create an histogram\n  current_hist <-\n    leicester_2011OAC %>% \n    filter(supgrpname == current_supgrp) %>% \n    ggplot(aes(\n      x = Total_Population\n    )) +\n    geom_histogram() +\n    ggtitle(current_supgrp) +\n    theme_bw() \n  \n  # Print the histogram\n  print(current_hist)\n  \n  cat(\"\\n\\n\")\n  \n}"},{"path":"r-scripting.html","id":"suburbanites","chapter":"12 R scripting","heading":"12.3.1 Suburbanites","text":"total population OA Leicester areas classified Suburbanites normally distributed","code":""},{"path":"r-scripting.html","id":"cosmopolitans","chapter":"12 R scripting","heading":"12.3.2 Cosmopolitans","text":"total population OA Leicester areas classified Cosmopolitans normally distributed","code":""},{"path":"r-scripting.html","id":"multicultural-metropolitans","chapter":"12 R scripting","heading":"12.3.3 Multicultural Metropolitans","text":"total population OA Leicester areas classified Multicultural Metropolitans normally distributed","code":""},{"path":"r-scripting.html","id":"ethnicity-central","chapter":"12 R scripting","heading":"12.3.4 Ethnicity Central","text":"total population OA Leicester areas classified Ethnicity Central normally distributed","code":""},{"path":"r-scripting.html","id":"constrained-city-dwellers","chapter":"12 R scripting","heading":"12.3.5 Constrained City Dwellers","text":"total population OA Leicester areas classified Constrained City Dwellers normally distributed","code":""},{"path":"r-scripting.html","id":"hard-pressed-living","chapter":"12 R scripting","heading":"12.3.6 Hard-Pressed Living","text":"total population OA Leicester areas classified Hard-Pressed Living normally distributed","code":""},{"path":"r-scripting.html","id":"urbanites","chapter":"12 R scripting","heading":"12.3.7 Urbanites","text":"total population OA Leicester areas classified Urbanites normally distributed","code":""},{"path":"r-scripting.html","id":"function-definition","chapter":"12 R scripting","heading":"12.4 Function definition","text":"Recall first chapter algorithm effective procedure mechanical rule, automatic method, programme performing mathematical operation (Cutland, 1980, p. 721). program specific set instructions implement abstract algorithm. definition algorithm (thus program) can consist one functions, sets instructions perform task, possibly using input, possibly returning output value.code presents two versions simple function one parameter. function simply calculates square root number returns value two digits.definition function executed, function becomes part environment, visible Environment panel subsection titled Functions. Thereafter, function can called Console, portions script, well scripts.","code":"\nlibrary(tidyverse)\n\nsqrt_2digits <- \n  function (input_value) {\n    sqrt_value <- sqrt(input_value)\n    rounded_value <- round(sqrt_value, digits = 2)\n    rounded_value\n  }\n\nsqrt_2digits_pipe <- \n  function (input_value) {\n    input_value %>% \n      sqrt() %>% \n      round(digits = 2)\n  }\nsqrt_2digits(2)## [1] 1.41\nsqrt_2digits_pipe(2)## [1] 1.41"},{"path":"r-scripting.html","id":"functions-and-control-structures","chapter":"12 R scripting","heading":"12.4.1 Functions and control structures","text":"One issue writing functions making sure data given data right kind. example, happens try compute cube root negative number using function defined ?probably wasn’t answer wanted, NaN (Number) value returned mathematical expression numerically indeterminate. case, actually due shortcoming ^ operator R, works positive base values. fact \\(-7\\) perfectly valid cube root \\(-343\\), since \\((-7)\\times(-7)\\times(-7) = -343\\).work around limitation, can state conditional rule:x < 0\ncalculate cube root x ‘normally’.\ncalculate cube root x ‘normally’.Otherwise:\nwork cube root positive number, change negative.\nwork cube root positive number, change negative.kinds situations can dealt R function using statement, shown . Note operator - (.e., symbol minus) used obtain inverse number, way -1 inverse number 1.However, things can go wrong. example, cube_root(\"Leicester\") cause error occur, Error x^(1 / 3) : non-numeric argument binary operator. shouldn’t surprising cube roots make sense numbers, character variables. Thus, might helpful cube root function spot print warning explaining problem, rather just crashing fairly cryptic error message one , moment.function re-written making use .numeric second conditional statement. input value numeric, function returns value NA (Available) instead number. Note statement inside another statement, always possible nest code blocks – within within within within … etc.Remember, cat printing function instructs R display provided argument (case, phrase within quotes) output Console. \\n cat tells R add newline printing warning.","code":"\ncube_root <- function (input_value) {\n   result <- input_value ^ (1 / 3)\n   result\n}\n\ncube_root(-343)## [1] NaN\ncube_root <- function (input_value) {\n    if (input_value >= 0){\n        result <- input_value^(1 / 3) \n    }else{\n        result <- -( (-input_value)^(1/3) )\n    }\n    result\n}\n\ncube_root(343)\ncube_root(-343)\ncube_root <- function (input_value) { \n    if (is.numeric(input_value)) {\n        if (input_value >= 0){\n            result <- input_value^(1/3) \n        }else{\n            result <- -(-input_value)^(1/3)\n        }\n        result\n    }else{\n        cat(\"WARNING: Input variable must be numeric\\n\") \n        NA\n    }\n}"},{"path":"r-scripting.html","id":"debugging","chapter":"12 R scripting","heading":"12.5 Debugging","text":"term bug commonly used computer science refer error, failure, fault system. term `debugging'' programming thus refers procedure searching mistakes code. procedure can simply done re-reading revising written code, R provides useful command calleddebug`, allows follow execution code, easily identify mistakes.Try debugging function - since working properly, won’t (hopefully!) find errors, demonstrate debug facility, typing debug(cube_root) RStudio Console. tells R want run cube_root debug mode. Next enter cube_root(-50) RStudio Console see repeatedly pressing return key steps function. Note particularly happens statement.stage process, can type R expression check value. instance, get statement, can enter input_value > 0 RStudio Console see whether true false. Checking value expressions various points stepping code good way identifying potential bugs glitches code. Try running code cube root calculations, replacing -50 different numbers, get used using debugging facility.done, enter undebug(cube_root). tells R ready return cube_root running normal mode. details debugger, enter help(debug) RStudio Console.","code":""},{"path":"r-scripting.html","id":"writing-functions-for-dplyr","chapter":"12 R scripting","heading":"12.6 Writing functions for dplyr","text":"functions defined base R function, designed work within Tidyverse. one input expected number, although can applied vector. functions designed expect dataframe tibble first argument (Tidyverse functions select mutate) designed use non-standard evaluation typical Tidyverse – .e., column names provided variable names, strings quotes.Programming dplyr article dplyr pages explains process full detail source information section. greatly simplify, design functions Tidyverse, functions expect first argument tibble (dataframe). parameters expect column names argument using non-standard evaluation need used embracing doubled braces {{. Furthermore, order create output column names using input column names operator := must used illustrated . additional paramenter can provided normal.example defines function named percentage_of, calculates new column percentage column based another column total. first parameter data expects dataframe tibbe argument, two subsequent parameters expect column names used non-standard evaluation. function uses mutate add new column. new column defined using := operator name os perc_ followed column names, followed _over_ name column used total. first total columns used embraced {{, dividing one finally multipled hundred.function can used calculated percentage people Leicester commuting work using private transport (u121 2011 Output Area Classification dataset seen previous chapters), using Total_Pop_No_NI_Students_16_to_74 total. first example shows procedure without function. second example shows procedure using function.","code":"\npercentage_of <- \n  function (data, var_col, total_col) {\n    data %>%\n      mutate(\n        \"perc_{{var_col}}\" :=\n          ({{ var_col }} / {{ total_col }}) * 100\n      )\n  }\nleicester_2011OAC <- read_csv(\"2011_OAC_Raw_uVariables_Leicester.csv\")\nlibrary(knitr)\n\nleicester_2011OAC %>%\n  select(\n    OA11CD, u121, Total_Pop_No_NI_Students_16_to_74\n  ) %>%\n  mutate(\n    perc_u121 = \n      (u121 / Total_Pop_No_NI_Students_16_to_74) * 100\n  ) %>%\n  slice_head(n = 5) %>% \n  kable()\nleicester_2011OAC %>%\n  select(\n    OA11CD, u121, Total_Pop_No_NI_Students_16_to_74\n  ) %>%\n  percentage_of(\n    u121, Total_Pop_No_NI_Students_16_to_74\n  ) %>%\n  slice_head(n = 5) %>% \n  kable()"},{"path":"r-scripting.html","id":"loading-r-scripts","chapter":"12 R scripting","heading":"12.7 Loading R scripts","text":"key reason writing functions can write use time need . principle behind creation libraries, (mostly) collections functions. purpose, possible write functions R script load script (functions defined) fromanother script RMarkdown document – fashion similar library loaded. Create new R script named my_useful_functions.R (seen section), copy code R script save file.Create new RMarkdown document copy-paste code new document.knitted, new document uses source instruct interpreter load run code my_useful_functions.R script, thus loading definition percentage_of function. rest code load 2011 OAC data invokes function within pipe. simple example, can extremely powerful tool create library functions used different scripts.Stefano De Sabbata – text licensed CC -SA 4.0, contains public sector information licensed Open Government Licence v3.0, code licensed GNU GPL v3.0.","code":"\nlibrary(tidyverse)\n\npercentage_of <- \n  function (data, var_col, total_col) {\n    data %>%\n      mutate(\n        \"perc_{{var_col}}\" :=\n          ({{ var_col }} / {{ total_col }}) * 100\n      )\n  }\nlibrary(tidyverse)\n\nsource(\"my_useful_functions.R\")\n\nleicester_2011OAC <- read_csv(\"2011_OAC_Raw_uVariables_Leicester.csv\")\n\nleicester_2011OAC %>%\n  select(\n    OA11CD, u121, Total_Pop_No_NI_Students_16_to_74\n  ) %>%\n  percentage_of(\n    u121, Total_Pop_No_NI_Students_16_to_74\n  ) %>%\n  slice_head(n = 5) %>% \n  kable()"},{"path":"appendix-1.html","id":"appendix-1","chapter":"Appendix 1","heading":"Appendix 1","text":"","code":""},{"path":"appendix-1.html","id":"basic-types","chapter":"Appendix 1","heading":"Basic types","text":"value R instance one three basic types, encoding fundamentally different type information: numeric encoding numbers; logical encoding truth values (also known Boolean values); character encoding text. type characteristics related operations, discussed .","code":""},{"path":"appendix-1.html","id":"numeric","chapter":"Appendix 1","heading":"Numeric","text":"numeric type represents numbers (integers reals).Base numeric operators.pre-defined functions R:Use simple brackets specify order execution. specified default order : rise power first, multiplication division, sum subtraction last.object NaN (Number) returned R result operation number.confused object NA (Available), returned missing data.","code":"\na_number <- 1.41\nis.numeric(a_number)## [1] TRUE\nis.integer(a_number)## [1] FALSE\nis.double(a_number) # i.e., is real## [1] TRUE\nabs(-2) # Absolute value## [1] 2\nceiling(3.475) # Upper round## [1] 4\nfloor(3.475) # Lower round## [1] 3\ntrunc(5.99) # Truncate## [1] 5\nlog10(100) # Logarithm 10## [1] 2\nlog(exp(2)) # Natural logarithm and e## [1] 2\na_number <- 1\n(a_number + 2) * 3## [1] 9\na_number + (2 * 3)## [1] 7\na_number + 2 * 3## [1] 7\n0 / 0## [1] NaN\nis.nan(0 / 0)## [1] TRUE"},{"path":"appendix-1.html","id":"logical","chapter":"Appendix 1","heading":"Logical","text":"logical type encodes two truth values: True False.R provides series basic logic operators can use evaluate conditions. instance, can use logic operator == evaluate condition 5==2, tests whether value 5 equal value 2. Conditions can tested values well variables.","code":"\nlogical_var <- TRUE\nis.logical(logical_var)## [1] TRUE\nisTRUE(logical_var)## [1] TRUE\nas.logical(0) # TRUE if not zero## [1] FALSE\n5==2## [1] FALSE\nfirst_value <- 5\nsecond_value <- 2\nfirst_value == 5## [1] TRUE\nfirst_value == 2## [1] FALSE\nsecond_value == 5## [1] FALSE\nsecond_value == 2## [1] TRUE\nfirst_value == second_value## [1] FALSE"},{"path":"appendix-1.html","id":"character","chapter":"Appendix 1","heading":"Character","text":"character type represents text objects, including single characters character strings (text objects longer one character, commonly referred simply strings computer science).","code":"\na_string <- \"Hello world!\"\nis.character(a_string)## [1] TRUE\nis.numeric(a_string)## [1] FALSE\nas.character(2) # type conversion  (a.k.a. casting)## [1] \"2\"\nas.numeric(\"2\")## [1] 2\nas.numeric(\"Ciao\")## Warning: NAs introduced by coercion## [1] NA"},{"path":"appendix-1.html","id":"types-and-variables","chapter":"Appendix 1","heading":"Types and variables","text":"variable storing value given type said type. However, variables R don’t assigned type . means variable can assigned numeric value first changed character value.precise, many programming languages require declare variable. , state type variable can used. Variable declaration particularly common older programming languages C Java. R require declare variables types.","code":"\na_variable <- 1.41\na_variable## [1] 1.41\nis.numeric(a_variable)## [1] TRUE\na_variable <- \"Hello world!\"\na_variable## [1] \"Hello world!\"\nis.character(a_variable)## [1] TRUE\nis.numeric(a_variable)## [1] FALSE"},{"path":"appendix-1.html","id":"more-on-vectors-and-factors","chapter":"Appendix 1","heading":"More on vectors and factors","text":"","code":""},{"path":"appendix-1.html","id":"vectors-1","chapter":"Appendix 1","heading":"Vectors","text":"operator : can used create integer vectors, starting number specified operator number specified operator.functions seq rep can also used create vectors, illustrated .logical operators can used test conditions vector. former returns TRUE least one element satisfies statement second returns TRUE elements satisfy condition","code":"\n# Create a vector containing integers between 2 and 4\ntwo_to_four <- 2:4\ntwo_to_four## [1] 2 3 4\n# Retrieve cities between the second and the fourth\neast_midlands_cities <- c(\"Derby\", \"Leicester\", \"Lincoln\", \"Nottingham\")\neast_midlands_cities[two_to_four]## [1] \"Leicester\"  \"Lincoln\"    \"Nottingham\"\n# As the second element of two_to_four is 3...\ntwo_to_four[2]## [1] 3\n# the following command will retrieve the third city\neast_midlands_cities[two_to_four[2]]## [1] \"Lincoln\"\n# Create a vector with cities from the previous vector\nselected_cities <- c(east_midlands_cities[1], east_midlands_cities[3:4])\nseq(1, 10, by = 0.5)##  [1]  1.0  1.5  2.0  2.5  3.0  3.5  4.0  4.5  5.0  5.5  6.0  6.5  7.0  7.5  8.0\n## [16]  8.5  9.0  9.5 10.0\nseq(1, 10, length.out = 6)## [1]  1.0  2.8  4.6  6.4  8.2 10.0\nrep(\"Ciao\", 4)## [1] \"Ciao\" \"Ciao\" \"Ciao\" \"Ciao\"\nany(east_midlands_cities == \"Leicester\")## [1] TRUE\nmy_sequence <- seq(1, 10, length.out = 7)\nmy_sequence## [1]  1.0  2.5  4.0  5.5  7.0  8.5 10.0\nany(my_sequence > 5)## [1] TRUE\nall(my_sequence > 5)## [1] FALSE"},{"path":"appendix-1.html","id":"factors","chapter":"Appendix 1","heading":"Factors","text":"factor data type similar vector. However, values contained factor can selected set levels.function table can used obtain tabulated count level.specific set levels can specified creating factor providing levels argument.statistics terminology, (unordered) factors categorical (.e., binary nominal) variables. Levels ordered.greater operator meaningful income_nominal factor defined .statistics terminology, ordered factors ordinal variables. Levels ordered.Stefano De Sabbata – text licensed CC -SA 4.0, contains public sector information licensed Open Government Licence v3.0, code licensed GNU GPL v3.0.","code":"\nhouses_vector <- c(\"Bungalow\", \"Flat\", \"Flat\",\n  \"Detached\", \"Flat\", \"Terrace\", \"Terrace\")\nhouses_vector## [1] \"Bungalow\" \"Flat\"     \"Flat\"     \"Detached\" \"Flat\"     \"Terrace\"  \"Terrace\"\nhouses_factor <- factor(c(\"Bungalow\", \"Flat\", \"Flat\",\n  \"Detached\", \"Flat\", \"Terrace\", \"Terrace\"))\nhouses_factor## [1] Bungalow Flat     Flat     Detached Flat     Terrace  Terrace \n## Levels: Bungalow Detached Flat Terrace\nhouses_factor <- factor(c(\"Bungalow\", \"Flat\", \"Flat\",\n  \"Detached\", \"Flat\", \"Terrace\", \"Terrace\"))\nhouses_factor## [1] Bungalow Flat     Flat     Detached Flat     Terrace  Terrace \n## Levels: Bungalow Detached Flat Terrace\ntable(houses_factor)## houses_factor\n## Bungalow Detached     Flat  Terrace \n##        1        1        3        2\nhouses_factor_spec <- factor(\n  c(\"People Carrier\", \"Flat\", \"Flat\", \"Hatchback\",\n      \"Flat\", \"Terrace\", \"Terrace\"),\n  levels = c(\"Bungalow\", \"Flat\", \"Detached\",\n       \"Semi\", \"Terrace\"))\n\ntable(houses_factor_spec)## houses_factor_spec\n## Bungalow     Flat Detached     Semi  Terrace \n##        0        3        0        0        2\nincome_nominal <- factor(\n  c(\"High\", \"High\", \"Low\", \"Low\", \"Low\",\n      \"Medium\", \"Low\", \"Medium\"),\n  levels = c(\"Low\", \"Medium\", \"High\"))\nincome_nominal > \"Low\"## Warning in Ops.factor(income_nominal, \"Low\"): '>' not meaningful for factors## [1] NA NA NA NA NA NA NA NA\nincome_ordered <- ordered(\n  c(\"High\", \"High\", \"Low\", \"Low\", \"Low\",\n      \"Medium\", \"Low\", \"Medium\"),\n  levels = c(\"Low\", \"Medium\", \"High\"))\n\nincome_ordered > \"Low\"## [1]  TRUE  TRUE FALSE FALSE FALSE  TRUE FALSE  TRUE\nsort(income_ordered)## [1] Low    Low    Low    Low    Medium Medium High   High  \n## Levels: Low < Medium < High"},{"path":"solutions.html","id":"solutions","chapter":"Solutions","heading":"Solutions","text":"Print chapter","code":""},{"path":"solutions.html","id":"solutions-101","chapter":"Solutions","heading":"Solutions 101","text":"Note can add sections code, “Intro” section selecting Code > Insert Section… top menu RStudio. visible right panel RStudio integrated editor select Code > Show Document Outline top menu. Code sections can help visually organise code Document Outline allows select section jump code.code deletes variable function currently defined environment, generally good practice, avoid script interacting objects left previous sessions.Load necessary libraries.","code":"\nrm(list = ls())\n# Load the tidyverse\nlibrary(tidyverse)\n\n# Load magrittr\n# necessary for options 3 and 4 \n# of my answer to Question 101.2.4\nlibrary(magrittr)"},{"path":"solutions.html","id":"solutions-101.1","chapter":"Solutions","heading":"Solutions 101.1","text":"Question 101.1.1: Write piece code using pipe operator takes input number 1632, calculates logarithm base 10, takes highest integer number lower calculated value (lower round), verifies whether integer.Question 101.1.2: Write piece code using pipe operator takes input number 1632, calculates square root, takes lowest integer number higher calculated value (higher round), verifies whether integer.Question 101.1.3: Write piece code using pipe operator takes input string \"1632\", transforms number, checks whether result Number.Question 101.1.4: Write piece code using pipe operator takes input string \"-16.32\", transforms number, takes absolute value truncates , finally checks whether result Available.Question 101.1.5: Rewrite piece code substituting last line function mean(). kind result obtain? represent?output numeric value representing aritmetic mean (average) petal lengths flowers iris dataset.Question 101.1.6: edit code created Question 101.1.6 substituting Petal.Length Petal.Width first Species ? kind results obtain? mean?","code":"\n1632 %>%\n  # calculate the logarithm to the base 10\n  log10() %>%\n  # highest integer number lower than the value\n  floor() %>%\n  # check whether it is an integer\n  is.integer()\n  # The answer is FALSE\n  # as the value is still of type numeric\n  # rather than type integer\n# The code below replicates the procedure above\n# but checking the value data type\n1632 %>%\n  log10() %>%\n  floor() %>%\n  class()\n# The code below replicates the procedure above\n# but using as.integer to transfor the value\n# to an integer type\n1632 %>%\n  log10() %>%\n  floor() %>%\n  as.integer() %>%\n  class()\n# As above but checking whether the value is an integer\n1632 %>%\n  log10() %>%\n  floor() %>%\n  as.integer() %>%\n  is.integer()\n1632 %>%\n  # calculate the square root\n  sqrt() %>%\n  # lowest integer number higher than the value\n  ceiling() %>%\n  # check whether it is an integer\n  is.integer()\n\"1632\" %>%\n  # transform it into a number\n  as.numeric() %>%\n  # check whether the result is Not a Number\n  is.nan()\n\"-16.32\" %>%\n  # transform it into a number\n  as.numeric() %>%\n  # take the absolute value\n  abs() %>%\n  # truncate\n  trunc() %>%\n  # check whether the result is Not Available\n  is.na()\niris %>% \n  pull(Petal.Length) %>% \n  mean()\niris %>% \n  pull(Petal.Width) %>% \n  mean()\n\niris %>% \n  pull(Species) %>% \n  mean()"},{"path":"solutions.html","id":"solutions-102","chapter":"Solutions","heading":"Solutions 102","text":"","code":""},{"path":"solutions.html","id":"solutions-102.1","chapter":"Solutions","heading":"Solutions 102.1","text":"Extend code script practical-102_my-script-002.R include code necessary solve questions .Question 101.1.1: Write piece code using pipe operator dplyr library generate table showing air time carrier, flights starting JFK airport. examples , use slice_head kable output nicely-formatted table containing first 10 rows.Question 102.1.2: Write piece code using pipe operator dplyr library generate table showing arrival delay overall air time, flights October 12th. examples , use slice_head kable output nicely-formatted table containing first 10 rows.Question 102.1.3: Write piece code using pipe operator dplyr library generate table showing arrival delay, origin destination, flight leaving 11am 2pm. examples , use slice_head kable output nicely-formatted table containing first 10 rows.","code":"\nlibrary(tidyverse)\nlibrary(nycflights13)\nlibrary(knitr)\n# Start from the entire dataset\nflights %>%\n  # Retain only the necessary columns\n  select(origin, carrier, air_time) %>%\n  # Retain only rows representing flights from JFK\n  filter(origin == \"JFK\") %>%\n  slice_head(n = 5) %>%\n  kable()\n# Start from the entire dataset\nflights %>%\n  # Retain only the necessary columns\n  select(year:day, arr_delay, air_time) %>%\n  # Retain only rows where month equals 10 and day equals 12\n  filter(month == 10 & day == 12) %>%\n  slice_head(n = 5) %>%\n  kable()\n# Start from the entire dataset\nflights %>%\n  # Retain only the necessary columns\n  select(origin, dest, dep_time, arr_delay) %>%\n  # Retain only rows where departure time is greater than or equal to 1100\n  # and departure time is also less than or equal to 1100\n  filter(dep_time >= 1100 & dep_time <= 1400) %>%\n  slice_head(n = 5) %>%\n  kable()"},{"path":"solutions.html","id":"solutions-103","chapter":"Solutions","heading":"Solutions 103","text":"","code":""},{"path":"solutions.html","id":"solutions-103.1","chapter":"Solutions","heading":"Solutions 103.1","text":"Create RMarkdown document RStudio, using Exercise 103 title PDF output. Delete contents except first five lines compose heading. Save document practical-103_exercises.Rmd. Add libraries code necessary read data 2011_OAC_Raw_uVariables_Leicester.csv file. Create first section document (e.g., adding second heading Exercise 103.1) add answers questions .order answer questions , inspect look-table 2011_OAC_Raw_uVariables_Lookup.csv (e.g., using Microsoft Excel) identify columns necessary complete task.Question 103.1.1: Identify five variables part variable subdomain Housing Type write code necessary compute total number household spaces Leicester housing type.Question 103.1.2: Write code necessary compute total number household spaces Leicester housing type grouped 2011 OAC supergroup.Question 103.1.3: Write code necessary compute percentage household spaces (.e., total number household spaces) Leicester housing type grouped 2011 OAC supergroup.Question 103.1.4: Modify code written Question 103.1.3, using verb rename change column names columns containing percentages names resemble related housing type (e.g., perc_of_detached).","code":"\nleicester_2011OAC %>% \n  summarise(\n    u086_tot = sum(u086),\n    u087_tot = sum(u087),\n    u088_tot = sum(u088),\n    u089_tot = sum(u089),\n    u090_tot = sum(u090)\n  ) %>% \n  kable()\nleicester_2011OAC %>% \n  group_by(supgrpname) %>% \n  summarise(\n    u086_tot = sum(u086),\n    u087_tot = sum(u087),\n    u088_tot = sum(u088),\n    u089_tot = sum(u089),\n    u090_tot = sum(u090)\n  ) %>% \n  kable()\nleicester_2011OAC %>% \n  group_by(supgrpname) %>% \n  summarise(\n    u086_tot = sum(u086),\n    u087_tot = sum(u087),\n    u088_tot = sum(u088),\n    u089_tot = sum(u089),\n    u090_tot = sum(u090)\n  ) %>% \n  mutate(\n    tot_hspaces = \n      u086_tot + u087_tot +\n      u088_tot + u089_tot +\n      u090_tot\n  ) %>% \n  mutate(\n    u086_perc = (u086_tot / tot_hspaces) * 100,\n    u087_perc = (u087_tot / tot_hspaces) * 100,\n    u088_perc = (u088_tot / tot_hspaces) * 100,\n    u089_perc = (u089_tot / tot_hspaces) * 100,\n    u090_perc = (u090_tot / tot_hspaces) * 100\n  ) %>% \n  select(\n    supgrpname,\n    u086_perc, u087_perc, u088_perc,\n    u089_perc, u090_perc\n  ) %>% \n  kable(digits = c(0, 2, 2, 2, 2, 2))"},{"path":"solutions.html","id":"solutions-104","chapter":"Solutions","heading":"Solutions 104","text":"","code":""},{"path":"solutions.html","id":"solutions-104.1","chapter":"Solutions","heading":"Solutions 104.1","text":"Extend code script Data_Wrangling_Example.R (see code ) include code necessary solve questions . Use full list variable names 2011 UK Census used generate 2011 OAC thatcan found file 2011_OAC_Raw_uVariables_Lookup.csv indetify columns use complete tasks.Question 104.1.1: Write piece code using pipe operator dplyr library generate table showing percentage EU citizens total population, calculated grouping OAs related decile Index Multiple Deprivations, accounting areas classified Cosmopolitans Ethnicity Central Multicultural Metropolitans.Question 104.1.2: Write piece code using pipe operator dplyr library generate table showing percentage EU citizens total population, calculated grouping OAs related supergroup 2011 OAC, accounting areas top 5 deciles Index Multiple Deprivations.Question 104.1.3: Write piece code using pipe operator dplyr library generate table showing percentage people aged 65 , calculated grouping OAs related supergroup 2011 OAC decile Index Multiple Deprivations, ordering table calculated value descending order.Extend code script Data_Wrangling_Example.R include code necessary solve questions .Question 104.1.4: Write piece code using pipe operator dplyr tidyr libraries generate long format leicester_2011OAC_IMD2015 table including values (census variables) used Question 104.1.3.Question 104.1.5: Write piece code using pipe operator dplyr tidyr libraries generate table similar one generated Question 104.1.4, showing values percentages total population.","code":"\n# Data_Wrangling_Example.R \n\n# Load the tidyverse\nlibrary(tidyverse)\n\n# Load 2011 OAC data\nleicester_2011OAC <- \n  read_csv(\"data/2011_OAC_Raw_uVariables_Leicester.csv\")\n\n# Load Indexes of Multiple deprivation data\nleicester_IMD2015 <- \n  read_csv(\"data/IndexesMultipleDeprivation2015_Leicester.csv\")\n\nleicester_IMD2015_decile_wide <- leicester_IMD2015 %>%\n  # Select only Socres\n  filter(Measurement == \"Decile\") %>%\n  # Trim names of IndicesOfDeprivation\n  mutate(\n    IndicesOfDeprivation = str_replace_all(IndicesOfDeprivation, \"\\\\s\", \"\")\n  ) %>%\n  mutate(\n    IndicesOfDeprivation = str_replace_all(IndicesOfDeprivation, \"[:punct:]\", \"\")\n  ) %>%\n  mutate(\n    IndicesOfDeprivation = str_replace_all(IndicesOfDeprivation, \"\\\\(\", \"\")\n  ) %>%\n  mutate(\n    IndicesOfDeprivation = str_replace_all(IndicesOfDeprivation, \"\\\\)\", \"\")\n  ) %>%\n  # Spread\n  pivot_wider(\n    names_from = IndicesOfDeprivation,\n    values_from = Value\n  ) %>%\n  # Drop columns\n  select(-DateCode, -Measurement, -Units)\n\n# Join\nleicester_2011OAC_IMD2015 <- \n  leicester_2011OAC %>%\n  inner_join(\n    leicester_IMD2015_decile_wide, \n    by = c(\"LSOA11CD\" = \"FeatureCode\")\n  )\nleicester_2011OAC_IMD2015 %>%\n  filter(supgrpname %in% c(\"Cosmopolitans\", \"Ethnicity Central\", \"Multicultural Metropolitans\")) %>%\n  group_by(IndexofMultipleDeprivationIMD) %>%\n  summarise(\n    adults_not_empl_perc = (sum(u044 + u045) / sum(Total_Population)) * 100\n  ) %>%\n  kable()\nleicester_2011OAC_IMD2015 %>%\n  filter(IndexofMultipleDeprivationIMD <= 5) %>%\n  group_by(supgrpname) %>%\n  summarise(\n    eu_perc = (sum(u043 + u044 + u045) / sum(Total_Population)) * 100\n  ) %>%\n  kable()\nleicester_2011OAC_IMD2015 %>%\n  filter(IndexofMultipleDeprivationIMD <= 5) %>%\n  group_by(supgrpname, IndexofMultipleDeprivationIMD) %>%\n  summarise(\n    aged_65_above = (sum(u016 + u017 + u018 + u019) / sum(Total_Population)) * 100\n  ) %>%\n  arrange(-aged_65_above) %>%\n  kable()\nlong_table <- leicester_2011OAC_IMD2015 %>%\n  select(OA11CD, supgrpname, IndexofMultipleDeprivationIMD, u016, u017, u018, u019, Total_Population) %>%\n  pivot_longer(\n    # Can't combine character values (e.g. supgrpname)\n    # with numeric value (e.g, Total_Population) thus\n    # pivot only numeric columns\n    cols = u016:Total_Population,\n    names_to = \"attribute\",\n    values_to = \"value\"\n  ) \n\nlong_table %>%\n  slice_head(n = 5) %>%\n  kable()\n\nlong_table_alt <- leicester_2011OAC_IMD2015 %>%\n  select(OA11CD, supgrpcode, IndexofMultipleDeprivationIMD, u016, u017, u018, u019, Total_Population) %>%\n  pivot_longer(\n    # Otherwise, use supgrpcode instead of supgrpname\n    cols = -OA11CD,\n    names_to = \"attribute\",\n    values_to = \"value\"\n  ) \n\nlong_table_alt %>%\n  slice_head(n = 7) %>%\n  kable()\nperc_long_table <- leicester_2011OAC_IMD2015 %>%\n  select(OA11CD, supgrpname, IndexofMultipleDeprivationIMD, u016, u017, u018, u019, Total_Population) %>%\n  mutate(\n    perc_u016 = (u016 / Total_Population) * 100, \n    perc_u017 = (u017 / Total_Population) * 100, \n    perc_u018 = (u018 / Total_Population) * 100, \n    perc_u019 = (u019 / Total_Population) * 100\n  ) %>%\n  select(OA11CD, supgrpname, IndexofMultipleDeprivationIMD, perc_u016, perc_u017, perc_u018, perc_u019) %>%\n  pivot_longer(\n    # Can't combine character values (e.g. supgrpname)\n    # with numeric value (e.g, Total_Population) thus\n    # pivot only numeric columns\n    cols = perc_u016:perc_u019,\n    names_to = \"attribute\",\n    values_to = \"value\"\n  ) \n\nperc_long_table %>%\n  slice_head(n = 5) %>%\n  kable()"},{"path":"solutions.html","id":"solutions-201","chapter":"Solutions","heading":"Solutions 201","text":"","code":""},{"path":"solutions.html","id":"solutions-201.1","chapter":"Solutions","heading":"Solutions 201.1","text":"Open Leicester_population project used previous chapters extend “Exploring deprivation indices Leicester” document include code necessary solve questions . Use full list variable names 2011 UK Census used generate 2011 OAC can found file 2011_OAC_Raw_uVariables_Lookup.csv indetify columns use complete tasks.Question 201.1.1: Write piece code create chart showing percentage EU citizens total population decile Index Multiple DeprivationsAlternatively.Question 201.1.2: Write piece code create chart showing relationship percentage EU citizens total population related score Index Multiple Deprivations, illustrating also 2011 OAC class OA.Question 201.1.3: Write piece code create chart showing relationship percentage people aged 65 related score Income Deprivation, illustrating also 2011 OAC class OA.Question 201.1.4: graph produced Question 201.1.3 mean? Write 100 words explaining conclusions can drawn graph – remember “larger score, deprived area”.Question 201.1.5: Identify index multiple deprivation closely relate percentage people per OA whose “day--day activities limited lot little” based “Standardised Illness Ratio”.","code":"\nleicester_2011OAC_IMD2015 %>% \n  group_by(IndexofMultipleDeprivationIMD) %>%\n  summarise(\n    eu_perc = (sum(u043 + u044 + u045) / sum(Total_Population)) * 100\n  ) %>%\n  ggplot(\n    aes(\n      x = eu_perc,\n      # Note that it is necessary to convert the index\n      # to a factor, otherwise it is interpreted as a number\n      y = as_factor(IndexofMultipleDeprivationIMD)\n    )\n  ) +\n  geom_col() +\n  theme_bw()\nleicester_2011OAC_IMD2015 %>% \n  group_by(IndexofMultipleDeprivationIMD) %>%\n  summarise(\n    eu_perc = (sum(u043 + u044 + u045) / sum(Total_Population)) * 100\n  ) %>%\n  mutate(\n    reminder = 100 - eu_perc\n  ) %>%\n  pivot_longer(\n    cols = -IndexofMultipleDeprivationIMD,\n    names_to = \"country_of_origin\",\n    values_to = \"percentage\"\n  ) %>% \n  ggplot(\n    aes(\n      x = percentage,\n      # Note that it is necessary to convert the index\n      # to a factor, otherwise it is interpreted as a number\n      y = as_factor(IndexofMultipleDeprivationIMD),\n      fill = country_of_origin\n    )\n  ) +\n  geom_col() +\n  theme_bw()\n# Use a similar code as used in Exercise 104\n# but filtering in the Scores rather than the deciles\nleicester_IMD2015_score_wide <- leicester_IMD2015 %>%\n  # Select only Socres\n  filter(Measurement == \"Score\") %>%\n  # Trim names of IndicesOfDeprivation\n  mutate(\n    IndicesOfDeprivation = str_replace_all(IndicesOfDeprivation, \"\\\\s\", \"\")\n  ) %>%\n  mutate(\n    IndicesOfDeprivation = str_replace_all(IndicesOfDeprivation, \"[:punct:]\", \"\")\n  ) %>%\n  mutate(\n    IndicesOfDeprivation = str_replace_all(IndicesOfDeprivation, \"\\\\(\", \"\")\n  ) %>%\n  mutate(\n    IndicesOfDeprivation = str_replace_all(IndicesOfDeprivation, \"\\\\)\", \"\")\n  ) %>%\n  # Spread\n  pivot_wider(\n    names_from = IndicesOfDeprivation,\n    values_from = Value\n  ) %>%\n  # Drop columns\n  select(-DateCode, -Measurement, -Units)\n\n# Join\nleicester_2011OAC_IMD2015_score <- \n  leicester_2011OAC %>%\n  inner_join(\n    leicester_IMD2015_score_wide, \n    by = c(\"LSOA11CD\" = \"FeatureCode\")\n  )\n\nleicester_2011OAC_IMD2015_score %>% \n  mutate(\n    eu_perc = ((u043 + u044 + u045) / Total_Population) * 100\n  ) %>%\n  ggplot(\n    aes(\n      x = eu_perc,\n      y = IndexofMultipleDeprivationIMD,\n      colour = fct_reorder(supgrpname, supgrpcode)\n    )\n  ) +\n  geom_point() +\n  scale_fill_manual(\n    values = c(\"#e41a1c\", \"#f781bf\", \"#ff7f00\", \"#a65628\", \"#984ea3\", \"#377eb8\", \"#ffff33\")\n  ) +\n  theme_bw()\nleicester_2011OAC_IMD2015_score %>% \n  mutate(\n    aged_65_above = ((u016 + u017 + u018 + u019) / Total_Population) * 100\n  ) %>%\n  ggplot(\n    aes(\n      x = aged_65_above,\n      y = LivingEnvironmentDeprivationDomain,\n      colour = fct_reorder(supgrpname, supgrpcode)\n    )\n  ) +\n  geom_point() +\n  scale_fill_manual(\n    values = c(\"#e41a1c\", \"#f781bf\", \"#ff7f00\", \"#a65628\", \"#984ea3\", \"#377eb8\", \"#ffff33\")\n  ) +\n  theme_bw()"},{"path":"solutions.html","id":"solutions-202","chapter":"Solutions","heading":"Solutions 202","text":"","code":""},{"path":"solutions.html","id":"solutions-202.1","chapter":"Solutions","heading":"Solutions 202.1","text":"Create new RMarkdown document, add code necessary recreate table leic_2011OAC_20to24 used example . Use code re-shape table leic_2011OAC_20to24 pivoting perc_age_20_to_24 column wider multiple columns using supgrpname new column names.manipulation creates one column per supergroup, containing perc_age_20_to_24 OA part supergroup, NA value OA part supergroup. transformation illustrated two tables . first shows extract original leic_2011OAC_20to24 dataset, followed wide version leic_2011OAC_20to24_supgrp.Question 202.1.1: code uses newly created leic_2011OAC_20to24_supgrp table calculate descriptive statistics calculated variable leic_2011OAC_20to24 supergroup. leic_2011OAC_20to24 normally distributed subgroups? yes, supergroups based values justify claim? (Write 200 words)can set p < .01 threshold, reasonable number cases dataset (hundreds, least 2011OAC supergroups). can claim leic_2011OAC_20to24 normally distributed supergroups Suburbanites (SU), Cosmopolitans (CP), Constrained City Dwellers (CD) Hard-Pressed Living (HP), normtest.p value 0.01, allows us reject null hypothesis. variable leic_2011OAC_20to24 seems instead normally distributed Multicultural Metropolitans (MM), Ethnicity Central (EC) Urbanites (UR). also illustrated graphs .Question 202.1.2: Write code necessary test normality leic_2011OAC_20to24 supergroups analysis conducted Question 202.1.1 indicated normal, using function shapiro.test, draw respective Q-Q plot.Example Hard-Pressed Living (HP).Question 202.1.3: Observe output Levene’s test executed . result tell variance perc_age_20_to_24 supergroups?Note leveneTest designed work Tidyverse approach. , code uses . argument placeholder specify input table leic_2011OAC_20to24 coming pipe used argument data parameter.significant, Levene’s test indicates variance different different levels. , can set p < .01 threshold. output , p value (Pr(>F)) much lower threshold, indicating test significance. Thus, perc_age_20_to_24 different variance different 2011OAc supergroups.","code":"\nleic_2011OAC_20to24_supgrp <- leic_2011OAC_20to24 %>%\n  pivot_wider(\n    names_from = supgrpname,\n    values_from = perc_age_20_to_24\n  )\nleic_2011OAC_20to24 %>%\n  slice_min(OA11CD, n = 10) %>%\n  kable(digits = 3)\nleic_2011OAC_20to24_supgrp %>%\n  slice_min(OA11CD, n = 10) %>%\n  kable(digits = 3)\nleic_2011OAC_20to24_supgrp %>%\n  select(-OA11CD) %>%\n  stat.desc(norm = TRUE) %>%\n  kable(digits = 3)\nleic_2011OAC_20to24 %>% \n  filter(supgrpname %in% c(\"SU\", \"CP\", \"CD\", \"HP\")) %>% \n  ggplot(\n    aes(\n      x = perc_age_20_to_24\n    )\n  ) + \n  geom_histogram(\n    aes(\n      y = ..density..\n    )\n  ) +\n  facet_wrap(\n    vars(supgrpname),\n    ncol = 2,\n    scales = \"free\"\n  ) +\n  ggtitle(\"Normally distributed\") +\n  theme_bw()\nleic_2011OAC_20to24 %>% \n  filter(supgrpname %in% c(\"MM\", \"EC\", \"UR\")) %>% \n  ggplot(\n    aes(\n      x = perc_age_20_to_24\n    )\n  ) + \n  geom_histogram(\n    aes(\n      y = ..density..\n    )\n  ) +\n  facet_wrap(\n    vars(supgrpname),\n    ncol = 2,\n    scales = \"free\"\n  ) +\n  ggtitle(\"Not normally distributed\") +\n  theme_bw()\nleic_2011OAC_20to24 %>%\n  filter(supgrpname == \"HP\") %>% \n  pull(perc_age_20_to_24) %>%\n  shapiro.test()## \n##  Shapiro-Wilk normality test\n## \n## data:  .\n## W = 0.99303, p-value = 0.8863\nleic_2011OAC_20to24 %>%\n  filter(supgrpname == \"HP\") %>% \n  ggplot(\n    aes(\n      sample = perc_age_20_to_24\n    )\n  ) +\n  stat_qq() +\n  stat_qq_line()\nleic_2011OAC_20to24 %>% \n  leveneTest(\n    perc_age_20_to_24 ~ supgrpname, \n    data = .\n  )## Levene's Test for Homogeneity of Variance (center = median)\n##        Df F value    Pr(>F)    \n## group   6  62.011 < 2.2e-16 ***\n##       962                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"solutions.html","id":"solutions-203","chapter":"Solutions","heading":"Solutions 203","text":"","code":""},{"path":"solutions.html","id":"solutions-203.1","chapter":"Solutions","heading":"Solutions 203.1","text":"Create new RMarkdown document, add code necessary oad 2011_OAC_Raw_uVariables_Leicester.csv dataset.Question 203.1.1: Check whether values mean age (u020) normally distributed, whether can transformed normally distributed set using logarithmic inverse hyperbolic sine functions.Start exploring variable distribution.variable u020 clearly normally distributed, probably mostly due long tail right side.Thus, can try transform variable using logarithmic transformation inverse hyperbolic sine transformation. Let’s try .Even using logarithmic inverse hyperbolic sine transformation, variable u020 still normally distributed. p-values higher compared original variable, still well \\(0.01\\) threshold. visible charts, transformed variables still show level long tails sides.Question 203.1.2: Check whether values mean age (u020) normally distributed looking different 2011OAC supergroups separately. Check whether can transformed normally distributed set using logarithmic inverse hyperbolic sine functions.Note code pivotes table obtain one column per 2011OAC supergroup.original u020 variable normally distributed (using \\(0.01\\) threshold) Suburbanites supergroup, transformed variables normally distributed supergroups except Hard-Pressed Living.Question 203.1.3: distribution mean age (u020) different different 2011OAC supergroups Leicester?use variable transformed using logarithmic transformation (easier interpret inverse hyperbolic sine) excluding Hard-Pressed Living supergroup, ensure groups taken account normally distributed. can test using ANOVA.Let’s start boxplot, can also horizontal, nominal variable (categories) provides y aesthetic continuous variable provides x aesthetic. example , choice makes 2011OAC supergroup names easier read.test significant F(5, 862) = 51.91 p < .01, thus can say distribution u020 across 2011OAC supergroups, excluding Hard-Pressed Living, different.","code":"\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(knitr)\nlibrary(pastecs)\n\nleicester_2011OAC <- read_csv(\"2011_OAC_Raw_uVariables_Leicester.csv\")\nleicester_2011OAC %>% \n  ggplot(aes(\n    x = u020\n  )) +\n  geom_histogram() +\n  theme_bw()## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\nleicester_2011OAC %>% \n  ggplot(aes(\n    sample = u020\n  )) +\n  stat_qq() +\n  stat_qq_line() +\n  theme_bw()\nleicester_2011OAC %>% \n  pull(u020) %>% \n  shapiro.test()## \n##  Shapiro-Wilk normality test\n## \n## data:  .\n## W = 0.9663, p-value = 3.476e-14\nleicester_2011OAC_trans <-\n  leicester_2011OAC %>% \n  mutate(\n    log10_u020 = log10(u020),\n    ihs_u020 = asinh(u020)\n  )\nleicester_2011OAC_trans %>% \n  ggplot(aes(\n    sample = log10_u020\n  )) +\n  stat_qq() +\n  stat_qq_line() +\n  theme_bw()\nleicester_2011OAC_trans %>% \n  ggplot(aes(\n    x = log10_u020\n  )) +\n  geom_histogram() +\n  theme_bw()## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\nleicester_2011OAC_trans %>% \n  pull(log10_u020) %>% \n  shapiro.test()## \n##  Shapiro-Wilk normality test\n## \n## data:  .\n## W = 0.99474, p-value = 0.001888\nleicester_2011OAC_trans %>% \n  ggplot(aes(\n    x = ihs_u020\n  )) +\n  geom_histogram() +\n  theme_bw()## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\nleicester_2011OAC_trans %>% \n  ggplot(aes(\n    sample = ihs_u020\n  )) +\n  stat_qq() +\n  stat_qq_line() +\n  theme_bw()\nleicester_2011OAC_trans %>% \n  pull(ihs_u020) %>% \n  shapiro.test()## \n##  Shapiro-Wilk normality test\n## \n## data:  .\n## W = 0.99473, p-value = 0.001874\nleicester_2011OAC %>% \n  select(OA11CD, supgrpname, u020) %>% \n  pivot_wider(\n    id_cols = OA11CD,\n    names_from = supgrpname,\n    names_prefix = \"u020 \",\n    values_from = u020\n  ) %>% \n  select(-OA11CD) %>% \n  stat.desc(\n    basic = FALSE,\n    desc = FALSE,\n    norm = TRUE\n  ) %>% \n  kable(digits = 2)\nleicester_2011OAC_trans %>% \n  select(OA11CD, supgrpname, log10_u020) %>% \n  pivot_wider(\n    id_cols = OA11CD,\n    names_from = supgrpname,\n    names_prefix = \"log10 u020 \",\n    values_from = log10_u020\n  ) %>% \n  select(-OA11CD) %>% \n  stat.desc(\n    basic = FALSE,\n    desc = FALSE,\n    norm = TRUE\n  ) %>% \n  kable(digits = 2)\nleicester_2011OAC_trans %>% \n  select(OA11CD, supgrpname, ihs_u020) %>% \n  pivot_wider(\n    id_cols = OA11CD,\n    names_from = supgrpname,\n    names_prefix = \"ihs u020 \",\n    values_from = ihs_u020\n  ) %>% \n  select(-OA11CD) %>% \n  stat.desc(\n    basic = FALSE,\n    desc = FALSE,\n    norm = TRUE\n  ) %>% \n  kable(digits = 2)\nleicester_2011OAC_trans %>% \n  select(OA11CD, supgrpname, log10_u020) %>% \n  filter(supgrpname != \"Hard-Pressed Living\") %>% \n  ggplot(\n    aes(\n      x = log10_u020, \n      y = supgrpname\n    )\n  ) +\n  geom_boxplot() +\n  theme_bw()\nlog10_u020_supgrpname_aov <-\n  leicester_2011OAC_trans %>% \n  select(OA11CD, supgrpname, log10_u020) %>% \n  filter(supgrpname != \"Hard-Pressed Living\") %$% \n  aov(log10_u020 ~ supgrpname) %>%\n  summary()\n\nlog10_u020_supgrpname_aov##              Df Sum Sq Mean Sq F value Pr(>F)    \n## supgrpname    5  1.049 0.20988   51.91 <2e-16 ***\n## Residuals   862  3.485 0.00404                   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"solutions.html","id":"solutions-203.2","chapter":"Solutions","heading":"Solutions 203.2","text":"Question 203.2.1: mentioned , discussing movement cities, assumption people living city centre live flats work cycle work, whereas people living suburbs live whole houses commute via car. Study correlation presence flats (u089) people commuting work foot, bicycle similar means (u122) OAs. Consider whether values might need normalised otherwised transformed starting testing procedure.first necessary step normalise data using variable statistical unit Total_Household_Spaces u089 Total_Pop_No_NI_Students_16_to_74 u122 suggested lookup table.two plots table indicate two variables normally distributed left skewed. , can try apply logarithmic transformation try “un-skew” . Note rename_with uses purrr-style lamba shorthand paste log10_ front column names.presence NaN values u089 column indicates variable includes zeros, logarithm defined. , can try using inverse iperbolic sine.values now NaN, still, variables normally distributed. , can use Pearson’s r correlation, one assumptions met. first instance, can use Spearman’s rho assess correlation.output includes warning Warning: compute exact p-value ties. , need check number ties dataset.number ties sizable, use Kendall’s tau advisable.test significant (p < .01), thus can say relationship, however extremely weak. value tau 0.27 indicating two variables share 7.3% variance.Question 203.2.2: Another interesting issue explore relationship car ownership use public transport. Study correlation presence households owning 2 cars vans (u118) people commuting work via public transport (u120) foot, bicycle similar means (u122) OAs. Consider whether values might need normalised otherwised transformed starting testing procedure.procedure similar one seen . variable Total_Households used normalise u118. Importantly, question specifies people commuting work via public transport (u120) foot, bicycle similar means (u122) also need sum u120 u122 account one (answers census questions mutually exclusive, one mode chosen), variables normally distributed. Thus, can try use logarithmic transformation try “un-skew” ., u118n includes zero value, thus producing NaN values table . , can try using inverse hyperbolic sine.Note ihs_u120u122n normally distributed, ihs_u118n normally distributed. , can’t use Pearson’s r correlation, resort use Spearman’s rho.Note Spearman’s rho statistic negative, indicating inverse relationship variables. first indication households 2 cars vans, fewer people commute work via public transport foot, bicycle similar means. However, , functions return warning Warning: compute exact p-value ties., dataset contains sizable number ties, thus use Kendall’s tau advisable.test significant (p < .01), thus can say inverse relationship, however weak. value tau -0.307 indicating two variables share 9.4% variance.","code":"\nflats_commuting <-\n  leicester_2011OAC %>% \n  mutate(\n    u089n = (u089 / Total_Household_Spaces) * 100,\n    u122n = (u122 / Total_Pop_No_NI_Students_16_to_74) * 100\n  ) %>% \n  select(OA11CD, u089n, u122n)\nflats_commuting %>% \n  ggplot(aes(\n    x = u089n\n  )) +\n  geom_histogram() +\n  theme_bw()## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\nflats_commuting %>% \n  ggplot(aes(\n    x = u122n\n  )) +\n  geom_histogram() +\n  theme_bw()## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\nflats_commuting %>% \n  select(u089n, u122n) %>%\n  stat.desc(\n    basic = FALSE,\n    desc = FALSE,\n    norm = TRUE\n  ) %>% \n  kable(digits = 2)\nflats_commuting %>% \n  select(u089n, u122n) %>%\n  mutate(\n    across(\n      everything(),\n      log10\n    )\n  ) %>% \n  rename_with(\n    # this is a shorthand\n    # to paste log10_ in front\n    # of all column names\n    ~ paste0(\"log10_\", .x)\n  ) %>% \n  stat.desc(\n    basic = FALSE,\n    desc = FALSE,\n    norm = TRUE\n  ) %>% \n  kable(digits = 2)\nflats_commuting %>% \n  select(u089n, u122n) %>%\n  mutate(\n    across(\n      everything(),\n      asinh\n    )\n  ) %>% \n  rename_with(\n    # this is a shorthand\n    # to paste ihs_ in front\n    # of all column names\n    ~ paste0(\"ihs_\", .x)\n  ) %>% \n  stat.desc(\n    basic = FALSE,\n    desc = FALSE,\n    norm = TRUE\n  ) %>% \n  kable(digits = 2)\nflats_commuting %>% \n  ggplot(aes(\n    x = u089n,\n    y = u122n\n  )) +\n  geom_point() +\n  theme_bw()\nu089n_u122n_spearman <-\n  flats_commuting %$%\n  cor.test(\n    u089n, u122n, \n    method = \"spearman\"\n  )## Warning in cor.test.default(u089n, u122n, method = \"spearman\"): Cannot compute\n## exact p-value with ties\nu089n_u122n_spearman## \n##  Spearman's rank correlation rho\n## \n## data:  u089n and u122n\n## S = 92011434, p-value < 2.2e-16\n## alternative hypothesis: true rho is not equal to 0\n## sample estimates:\n##       rho \n## 0.3932327\nflats_commuting %>%\n  count(u089n) %>%\n  filter(n > 1) %>%\n  count(wt = n()) %>%\n  pull(n)## Warning: `wt = n()` is deprecated\n## ℹ You can now omit the `wt` argument## [1] 127\nflats_commuting %>%\n  count(u122n) %>%\n  filter(n > 1) %>%\n  count(wt = n()) %>%\n  pull(n)## Warning: `wt = n()` is deprecated\n## ℹ You can now omit the `wt` argument## [1] 85\nu089n_u122n_kendall <-\n  flats_commuting %$%\n  cor.test(\n    u089n, u122n, \n    method = \"kendall\"\n  )\n\nu089n_u122n_kendall## \n##  Kendall's rank correlation tau\n## \n## data:  u089n and u122n\n## z = 12.534, p-value < 2.2e-16\n## alternative hypothesis: true tau is not equal to 0\n## sample estimates:\n##       tau \n## 0.2696906\ncas_vs_not_cars <-\n  leicester_2011OAC %>% \n  mutate(\n    u118n = (u118 / Total_Households) * 100,\n    u120u122n = ((u120 + u122) / Total_Pop_No_NI_Students_16_to_74) * 100\n  ) %>% \n  select(OA11CD, u118n, u120u122n)\ncas_vs_not_cars %>% \n  ggplot(aes(\n    x = u118n\n  )) +\n  geom_histogram() +\n  theme_bw()## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\ncas_vs_not_cars %>% \n  ggplot(aes(\n    x = u120u122n\n  )) +\n  geom_histogram() +\n  theme_bw()## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\ncas_vs_not_cars %>% \n  select(u118n, u120u122n) %>%\n  stat.desc(\n    basic = FALSE,\n    desc = FALSE,\n    norm = TRUE\n  ) %>% \n  kable(digits = 2)\ncas_vs_not_cars %>% \n  select(u118n, u120u122n) %>%\n  mutate(\n    across(\n      everything(),\n      log10\n    )\n  ) %>% \n  rename_with(\n    # this is a shorthand\n    # to paste log10_ in front\n    # of all column names\n    ~ paste0(\"log10_\", .x)\n  ) %>% \n  stat.desc(\n    basic = FALSE,\n    desc = FALSE,\n    norm = TRUE\n  ) %>% \n  kable(digits = 2)\ncas_vs_not_cars %>% \n  select(u118n, u120u122n) %>%\n  mutate(\n    across(\n      everything(),\n      asinh\n    )\n  ) %>% \n  rename_with(\n    # this is a shorthand\n    # to paste ihs_ in front\n    # of all column names\n    ~ paste0(\"ihs_\", .x)\n  ) %>% \n  stat.desc(\n    basic = FALSE,\n    desc = FALSE,\n    norm = TRUE\n  ) %>% \n  kable(digits = 2)\ncas_vs_not_cars %>% \n  ggplot(aes(\n    x = u118n,\n    y = u120u122n\n  )) +\n  geom_point() +\n  theme_bw()\nu118n_u120u122n_spearman <-\n  cas_vs_not_cars %$%\n  cor.test(\n    u118n, u120u122n, \n    method = \"spearman\"\n  )## Warning in cor.test.default(u118n, u120u122n, method = \"spearman\"): Cannot\n## compute exact p-value with ties\nu118n_u120u122n_spearman## \n##  Spearman's rank correlation rho\n## \n## data:  u118n and u120u122n\n## S = 217161157, p-value < 2.2e-16\n## alternative hypothesis: true rho is not equal to 0\n## sample estimates:\n##        rho \n## -0.4320643\ncas_vs_not_cars %>%\n  count(u118n) %>%\n  filter(n > 1) %>%\n  count(wt = n()) %>%\n  pull(n)## Warning: `wt = n()` is deprecated\n## ℹ You can now omit the `wt` argument## [1] 115\ncas_vs_not_cars %>%\n  count(u120u122n) %>%\n  filter(n > 1) %>%\n  count(wt = n()) %>%\n  pull(n)## Warning: `wt = n()` is deprecated\n## ℹ You can now omit the `wt` argument## [1] 83\nu118n_u120u122n_kendall <-\n  cas_vs_not_cars %$%\n  cor.test(\n    u118n, u120u122n, \n    method = \"kendall\"\n  )\n\nu118n_u120u122n_kendall## \n##  Kendall's rank correlation tau\n## \n## data:  u118n and u120u122n\n## z = -14.278, p-value < 2.2e-16\n## alternative hypothesis: true tau is not equal to 0\n## sample estimates:\n##        tau \n## -0.3065051"},{"path":"solutions.html","id":"solutions-204","chapter":"Solutions","heading":"Solutions 204","text":"","code":""},{"path":"solutions.html","id":"solutions-204.1","chapter":"Solutions","heading":"Solutions 204.1","text":"Question 204.1.1: linear regression modelling require variables normally distributed, skewed variables dep_delay arr_delay might significant obstacle robust model. possible build robust model using “un-skewed” variables?variables, can use inverse hyperbolic sine (asinh) “un-skew” variables. However, illustrated plot , “expands” area around origin two axis point linear relationship might lost.can still try build model. However, illustrated , relaionship become weak, model still robust.alternative approach might use double square root function – .e., square root positive values square root opposite value negative values (if_else coming weeks). However, seems fully resolve issue.","code":"\nflights_nov_20 %>%\n  mutate(\n    ihs_dep_delay = asinh(-dep_delay),\n    ihs_arr_delay = asinh(-arr_delay)\n  ) %>%\n  ggplot(\n    aes(\n      x = ihs_dep_delay, \n      y = ihs_arr_delay\n    )\n  ) +\n  geom_point() + \n  coord_fixed(ratio = 1) +\n  theme_bw()\ndelay_model_2 <- flights_nov_20 %>% \n  mutate(\n    ihs_dep_delay = asinh(-dep_delay),\n    ihs_arr_delay = asinh(-arr_delay)\n  ) %$%\n  lm(ihs_dep_delay ~ ihs_arr_delay) \n\ndelay_model_summary_2 <- delay_model_2 %>%\n  summary()\n\ndelay_model_summary_2## \n## Call:\n## lm(formula = ihs_dep_delay ~ ihs_arr_delay)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.4610 -1.0106  0.2507  0.9211  4.1417 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)    0.39881    0.06441   6.192 8.75e-10 ***\n## ihs_arr_delay  0.48274    0.01930  25.006  < 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.885 on 972 degrees of freedom\n## Multiple R-squared:  0.3915, Adjusted R-squared:  0.3909 \n## F-statistic: 625.3 on 1 and 972 DF,  p-value: < 2.2e-16\ndelay_model_2 %>% \n  rstandard() %>% \n  shapiro.test()## \n##  Shapiro-Wilk normality test\n## \n## data:  .\n## W = 0.96798, p-value = 8.19e-14\ndelay_model_2 %>% \n  bptest()## \n##  studentized Breusch-Pagan test\n## \n## data:  .\n## BP = 230.08, df = 1, p-value < 2.2e-16\ndelay_model_2 %>%\n  dwtest()## \n##  Durbin-Watson test\n## \n## data:  .\n## DW = 1.7535, p-value = 5.175e-05\n## alternative hypothesis: true autocorrelation is greater than 0\ndelay_model_2 %>% \n  plot()\nflights_nov_20 %>%\n  mutate(\n    sqrt_dep_delay = \n      if_else(\n        dep_delay >= 0, \n        sqrt(dep_delay), \n        (-1 * sqrt(-dep_delay)\n      )\n    ),\n    sqrt_arr_delay = \n      if_else(\n        arr_delay >= 0, \n        sqrt(arr_delay), \n        (-1 * sqrt(-arr_delay)\n      )\n    )\n  ) %>%\n  ggplot(\n    aes(\n      x = sqrt_dep_delay, \n      y = sqrt_arr_delay\n    )\n  ) +\n  geom_point() + \n  coord_fixed(ratio = 1) +\n  theme_bw()"},{"path":"solutions.html","id":"solutions-204.2","chapter":"Solutions","heading":"Solutions 204.2","text":"Question 204.2.1: possible create model linking housing type (u086 u090) number people commuting work via public transport (u120) foot, bicycle similar means (u122) OAs?five variables 2011OAC dataset related housing type two related type commuting mentioned question (listed ). , first step extract normalise also summing two variables related commuting, done previous exercises.u086: Whole house bungalow: Detachedu087: Whole house bungalow: Semi-detachedu088: Whole house bungalow: Terraced (including end-terrace)u089: Flatsu090: Caravan mobile temporary structureu120: Public Transportu122: foot, Bicycle OtherWe can explore relationships variables using pairs panel.plot illustrates variable related commuting via foot, bicycle public transport Leicester skewed. applies variables related housing type. time, presence Caravan mobile temporary structure seems minimal. , can attempt remove latter, “un-skew” rest using logarithmic function. cases least one variables minimum zero, can take example explore use function \\(log_{10}(x+1)\\) – shift logaritmic function left, result zero \\(x=0\\).plot indicates somewhat weak linear relationship commuting via foot, bicycle public transport housing types. time, seems clear relationship semidetached houses flats, indicates possible case multicolinearity. Let’s try include variables model double-check multicolinearity.afterwards.Interestingly, results don’t indicate sigificant level multicolinearity. However, model robust. Also, can see variable log_perc_flats significant model. Thus, can try remove variable see whether improvement.variables within models now significant. However, \\(R^2\\) low, indicating weak model. Moreover, tests indicate model robust.Thus, can conclude , , seem like possible create model linking population density housing type Leicester.Stefano De Sabbata – text licensed CC -SA 4.0, contains public sector information licensed Open Government Licence v3.0, code licensed GNU GPL v3.0.","code":"\nleicester_dwellings <-\n  leicester_2011OAC %>%\n  dplyr::select(\n    OA11CD, \n    Total_Dwellings, \n    Total_Pop_No_NI_Students_16_to_74,\n    u120, u122,\n    u086:u090\n  ) %>%\n  mutate(\n    # On foot, Bicycle or Other\n    # + Public Transport\n    perc_fbpt = \n      ((u120 + u122) / \n         Total_Pop_No_NI_Students_16_to_74) * 100\n  ) %>% \n  dplyr::mutate(\n    dplyr::across( \n      u086:u090,\n      #scale\n      function(x){ (x / Total_Dwellings) * 100 }\n    )\n  ) %>%\n  dplyr::rename(\n    perc_detached = u086,\n    perc_semidetached = u087,\n    perc_terraced = u088,\n    perc_flats = u089,   \n    perc_carava_tmp = u090\n  ) %>%\n  # Remove columns not needed anymore\n  select(\n    -Total_Dwellings, \n    -Total_Pop_No_NI_Students_16_to_74,\n    -u120, -u122\n  ) %>% \n  arrange(perc_fbpt)\nlibrary(GGally)\n\nleicester_dwellings %>%\n  dplyr::select(perc_fbpt,perc_detached:perc_carava_tmp) %>%\n  GGally::ggpairs(\n    upper = list(continuous = wrap(ggally_cor, method = \"kendall\")),\n    lower = list(continuous = wrap(\"points\", alpha = 0.3, size=0.1))\n  )\nleicester_dwellings %<>%\n  dplyr::mutate(\n    dplyr::across(\n      c(perc_fbpt,perc_detached:perc_flats),\n      ~ log10(.x + 1)\n    )\n  ) %>%\n  dplyr::rename_with(\n    ~ paste0(\"log_\", .x),\n    c(perc_fbpt,perc_detached:perc_flats)\n  )\nleicester_dwellings %>%\n  dplyr::select(log_perc_fbpt,log_perc_detached:log_perc_flats) %>%\n  GGally::ggpairs(\n    upper = list(continuous = wrap(ggally_cor, method = \"kendall\")),\n    lower = list(continuous = wrap(\"points\", alpha = 0.3, size=0.1))\n  )\n# Create model\ndwellings_model1 <- \n  leicester_dwellings %$%\n  lm(\n    log_perc_fbpt ~ \n      log_perc_detached + log_perc_semidetached + \n      log_perc_terraced + log_perc_flats\n  )\n\n# Print summary\ndwellings_model1 %>%\n  summary()## \n## Call:\n## lm(formula = log_perc_fbpt ~ log_perc_detached + log_perc_semidetached + \n##     log_perc_terraced + log_perc_flats)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.42849 -0.06348  0.01066  0.06716  0.33209 \n## \n## Coefficients:\n##                        Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)            1.441188   0.023965  60.136  < 2e-16 ***\n## log_perc_detached     -0.097106   0.008704 -11.156  < 2e-16 ***\n## log_perc_semidetached -0.071241   0.009176  -7.763 2.10e-14 ***\n## log_perc_terraced      0.039661   0.006777   5.852 6.63e-09 ***\n## log_perc_flats         0.009618   0.007510   1.281    0.201    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.09989 on 964 degrees of freedom\n## Multiple R-squared:  0.3602, Adjusted R-squared:  0.3576 \n## F-statistic: 135.7 on 4 and 964 DF,  p-value: < 2.2e-16\ndwellings_model1 %>%\n  plot()\ndwellings_model1 %>%\n  rstandard() %>% \n  shapiro.test()## \n##  Shapiro-Wilk normality test\n## \n## data:  .\n## W = 0.98718, p-value = 1.722e-07\ndwellings_model1 %>% \n  bptest()## \n##  studentized Breusch-Pagan test\n## \n## data:  .\n## BP = 83.074, df = 4, p-value < 2.2e-16\ndwellings_model1 %>%\n  dwtest()## \n##  Durbin-Watson test\n## \n## data:  .\n## DW = 0.71231, p-value < 2.2e-16\n## alternative hypothesis: true autocorrelation is greater than 0\ndwellings_model1 %>%\n  vif()##     log_perc_detached log_perc_semidetached     log_perc_terraced \n##              1.313314              1.948536              1.129558 \n##        log_perc_flats \n##              1.943342\n# Create model\ndwellings_model2 <- \n  leicester_dwellings %$%\n  lm(\n    log_perc_fbpt ~ \n      log_perc_detached + log_perc_semidetached + \n      log_perc_terraced\n  )\n\n# Print summary\ndwellings_model2 %>%\n  summary()## \n## Call:\n## lm(formula = log_perc_fbpt ~ log_perc_detached + log_perc_semidetached + \n##     log_perc_terraced)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.42751 -0.06284  0.01012  0.06590  0.33884 \n## \n## Coefficients:\n##                        Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)            1.465339   0.014794  99.047  < 2e-16 ***\n## log_perc_detached     -0.099686   0.008471 -11.768  < 2e-16 ***\n## log_perc_semidetached -0.078462   0.007243 -10.833  < 2e-16 ***\n## log_perc_terraced      0.037686   0.006601   5.709 1.51e-08 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.09992 on 965 degrees of freedom\n## Multiple R-squared:  0.3591, Adjusted R-squared:  0.3571 \n## F-statistic: 180.3 on 3 and 965 DF,  p-value: < 2.2e-16\ndwellings_model2 %>%\n  plot()\ndwellings_model2 %>%\n  rstandard() %>% \n  shapiro.test()## \n##  Shapiro-Wilk normality test\n## \n## data:  .\n## W = 0.98751, p-value = 2.413e-07\ndwellings_model2 %>% \n  bptest()## \n##  studentized Breusch-Pagan test\n## \n## data:  .\n## BP = 80.115, df = 3, p-value < 2.2e-16\ndwellings_model2 %>%\n  dwtest()## \n##  Durbin-Watson test\n## \n## data:  .\n## DW = 0.71123, p-value < 2.2e-16\n## alternative hypothesis: true autocorrelation is greater than 0\ndwellings_model2 %>%\n  vif()##     log_perc_detached log_perc_semidetached     log_perc_terraced \n##              1.242958              1.212989              1.071107"},{"path":"groupwork-exercise.html","id":"groupwork-exercise","chapter":"Groupwork exercise","heading":"Groupwork exercise","text":"","code":""},{"path":"groupwork-exercise.html","id":"introduction","chapter":"Groupwork exercise","heading":"Introduction","text":"One key pledges current government United Kingdom made 2019 commitment levelling – , address local regional inequalities. idea levelling country centre political debate last two years, involving wide range socio-economic topics education22 rail investment23.Centre Cities (independent charity research centre) identified24 health education two key areas core levelling agenda. conducting groupwork, focus relationship health, education occupation within Leicester. aim understand better health linked education occupation local level, perspective advising local authority key areas might prioritised support.","code":""},{"path":"groupwork-exercise.html","id":"data","chapter":"Groupwork exercise","heading":"Data","text":"explore issues outlined , groupwork uses data 2011 Output Area Classification (2011 OAC) introduced Chapter 3. dataset includes public sector information licensed Open Government Licence v3.0 Office National Statistics.2011 OAC geodemographic classification census Output Areas (OA) UK, created Gale et al. (2016) starting initial set 167 prospective variables United Kingdom Census 2011: 86 removed, 41 retained , 40 combined, leading final set 60 variables. Gale et al. (2016) finally used k-means clustering approach create 8 clusters supergroups (see map datashine.org.uk), well 26 groups 76 subgroups. dataset file 2011_OAC_Raw_uVariables_Leicester.csv contains original 167 variables, well resulting groups, city Leicester.","code":""},{"path":"groupwork-exercise.html","id":"instructions","chapter":"Groupwork exercise","heading":"Instructions","text":"continuing remainder groupwork, create new project named Leicester_health_education make sure activated.Download Blackboard (see data folder repository) 2011_OAC_Raw_uVariables_Leicester.csv file computer (upload RStudio Server necessary, done already). full variable names can found file 2011_OAC_Raw_uVariables_Lookup.csv. Write RMarkdown document compiled PDF HTML file presenting answers questions listed . present answers order listed, separate section document, including code, output textual component required.","code":""},{"path":"groupwork-exercise.html","id":"part-1","chapter":"Groupwork exercise","heading":"Part 1","text":"Conduct exploratory comparative analysis variables listed Table 1. Include code, output (can include graphics) description findings. latter 500 words can written final discussion analysis, description step analysis, combination two.\nTable 12.1: Variables used Part 1 2 groupwork exercise\n","code":""},{"path":"groupwork-exercise.html","id":"part-2","chapter":"Groupwork exercise","heading":"Part 2","text":"Select two among variables explored Part 1 (see Table 1) create robust (possible), simple linear regression model. model outcome (dependent) variable indicator health population predictors (independent) variables variables related occupation. variables selected based outcome analysis done Part 1, order ensure model strong robust possible.\\[health = occupation + error \\]Remember “correlation imply causation”. 😊","code":""},{"path":"groupwork-exercise.html","id":"part-3","chapter":"Groupwork exercise","heading":"Part 3","text":"Use variables explored Part 1 (see Table 1) create robust (possible), multiple linear regression model. model outcome (dependent) variable indicator health population. indicator can one variables explored Part 1 (see Table 1) combination thereof. model predictors (independent) variables relevant set variables related education occupation.\\[health = (education + occupation) + error \\]Present model achieves best fit process identified. Include code, output (can include graphics), discussion process interpretation final model. latter two 500 words can written final discussion analysis, description step analysis, combination two.Alternatively, robust model significant model can created Leicester, include code output (can include graphics) illustrate finding, related discussion (still, 500 words). latter written final discussion analysis, description step analysis, combination two.…, remember “correlation imply causation”. 😊Stefano De Sabbata – text licensed CC -SA 4.0, contains public sector information licensed Open Government Licence v3.0, code licensed GNU GPL v3.0.","code":""},{"path":"lecture-slides.html","id":"lecture-slides","chapter":"Lecture slides","heading":"Lecture slides","text":"Print chapterThis book companion module GY7702 R Data Science MSc Geographic Information Science School Geography, Geology, Environment University Leicester. module composed four units, one two three weeks long. Monday, publish three 20-minute lecture videos, along assigned reading, introduce topics week. Please watch lecture complete reading practical sessions Thursday. practical session provide opportunity apply seen videos read materials. materials include exercises complete time, practical session. questions, please contact via email come along office hours.slides used lectures available web-based slides pages linked .Introduction Reproducible Data Science\n101 Introduction R\n102 Reproducible data science\n103 Data manipulation\n104 Table operations\n101 Introduction R102 Reproducible data science103 Data manipulation104 Table operationsData analysis\n201 Exploratory visualisation\n202 Exploratory statistics\n203 Comparing data\n204 Regression models\n205 Multiple regression models\n201 Exploratory visualisation202 Exploratory statistics203 Comparing data204 Regression models205 Multiple regression modelsMachine learning\n301 Supervised\n302 Unsupervised\n301 Supervised302 UnsupervisedR Scripting\n401 R scripting\n401 R scriptingby Stefano De Sabbata – text licensed CC -SA 4.0, contains public sector information licensed Open Government Licence v3.0, code licensed GNU GPL v3.0.","code":""},{"path":"reading-list.html","id":"reading-list","chapter":"Reading list","heading":"Reading list","text":"week assigned reading books listed . particular, using R Data Science Garrett Grolemund Hadley Wickham Programming Skills Data Science Michael Freeman Joel Ross first two units, Discovering Statistics Using R Andy Field, Jeremy Miles Zoë Field, Machine Learning R: Expert techniques predictive modeling Brett Lantz third fourth unit. sneak peek use R geographic information analysis, can suggest Introduction R Spatial Analysis Mapping Chris Brunsdon Lex Comber Geocomputation R Robin Lovelace.","code":""},{"path":"reading-list.html","id":"suggested-reading","chapter":"Reading list","heading":"Suggested reading","text":"R Data Science Garrett Grolemund Hadley Wickham, O’Reilly Media, 2016. See online book.Machine Learning R: Expert techniques predictive modeling Brett Lantz, Packt Publishing, 2019. See book webpage.","code":""},{"path":"reading-list.html","id":"further-reading","chapter":"Reading list","heading":"Further reading","text":"Programming Skills Data Science: Start Writing Code Wrangle, Analyze, Visualize Data R Michael Freeman Joel Ross, Addison-Wesley, 2019. See book webpage repository.Art R Programming: Tour Statistical Software Design Norman Matloff, Starch Press, 2011. See book webpage.Discovering Statistics Using R Andy Field, Jeremy Miles Zoë Field, SAGE Publications Ltd, 2012. See book webpage.Introduction Statistical Learning Applications R Gareth James, Daniela Witten, Trevor Hastie Robert Tibshirani, Springer, 2013. See book webpage.Introduction Machine Learning R Scott V. Burger, O’Reilly Media, 2018. See book webpage.Machine Learning R, tidyverse, mlr Hefin . Rhys, Manning Publications, 2020. See book webpage.Deep Learning R François Chollet J. J. Allaire, Manning Publications, 2018. See book webpage.Introduction R Spatial Analysis Mapping Chris Brunsdon Lex Comber, Sage, 2015. See book webpage.Geocomputation R Robin Lovelace, Jakub Nowosad Jannes Muenchow. See, CRC Press, 2019. See online book.Stefano De Sabbata – text licensed CC -SA 4.0, contains public sector information licensed Open Government Licence v3.0, code licensed GNU GPL v3.0.","code":""},{"path":"references-1.html","id":"references-1","chapter":"References","heading":"References","text":"","code":""}]
